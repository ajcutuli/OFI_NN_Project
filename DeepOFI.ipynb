{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Deep Learning Approach to Limit Order Book Forecasting\n",
    "By Aric Cutuli<br>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening Remarks\n",
    "The enigma concerning the predictability of markets has always been the principal driver of my interest in finance, and it inpires my ongoing exploration of machine learning's applications within the analysis and forecasting of financial time series. Today, we compare the performance of a studied deep learning model for limit order book forecasting on two stationary representations of the limit order book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "In this notebook, we implement an artificial neural network originally employed by Zhang et al[<sub>[1]</sub>](#ref1) that combines convolutional neural networks (CNNs) and a long short-term memory (LSTM) neural network in order to classify future directions of an order book at a high frequency. Specifically, given Coinbase order book data for Bitcoin, we seek to predict whether the mid price increases, decreases, or does not change in the next observation of the time series. Unlike Zhang et al's papers we reference[<sub>[1]</sub>](#ref1)[<sub>[2]</sub>](#ref2), which use non-stationary order book states as inputs to the network, our instantiation of the architecture is trained on order flow and order flow imbalance, which are stationary quantities derived from the limit order book[<sub>[3]</sub>](#ref3). Hence, this discussion also draws heavy inspiration from a 2021 article by Kolm et al[<sub>[4]</sub>](#ref4), which demonstrated that forecasting using order flow significantly outperforms raw order book inputs. Today, we further this discussion by doing an analysis of the impact that differencing order flow into order flow imbalance has on the forecasting performance of the model. We also approach the problem from a time series modeling perspective via the Box-Jenkins method, a procedure that was not explicitly documented in those papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order Books, Flow, and Imbalance\n",
    "Today's trading of equities and other securities is often facilitated by a [*limit order book*](https://en.wikipedia.org/wiki/Order_book), also known as an *order book*. The order book collects bids and offers made by prospective buyers and sellers and determines which incoming orders get executed and which are added to the book. The *bid price* is the highest price buyers are prepared to buy at, and the *ask price* is the lowest price sellers are willing to sell at. The *mid price*, which our model seeks to predict moves in, is the midpoint of the bid price and the ask price.\n",
    "\n",
    "An order is defined by its side, quantity demanded, price to trade at, and time of submission. As one enters the system, the matching engine of the exchange tries to match the order with existing orders in the book. Orders that match are executed and called *market orders*, and orders that do not match or only partially match are added to the book and called *limit orders*.\n",
    "\n",
    "[<img src='Images/Limit-order-book-diagram-A-new-buy-limit-order-arrives-at-price-bt-increasing-the.png' style='width:425px;height:312px'/>](https://www.researchgate.net/figure/Limit-order-book-diagram-A-new-buy-limit-order-arrives-at-price-bt-increasing-the_fig1_297725489)\n",
    "\n",
    "Our model takes as inputs representations of the first ten levels of the order book. A level is denoted by its price and volume that is bid or asked. So, as we progress down levels on the bid side of the order book, the price decreases, and as we progress down levels of the ask side, the price increases. Each observation in our dataset will be a 40-variable vector displaying the price and volume for each of the top ten bid and ask levels, giving us a truncated screenshot of the *state of the limit order book* at each timestep. \n",
    "\n",
    "$$ \\text{s}_t^{LOB} := (a_t^1, v_t^{1,a}, b_t^1, v_t^{1,b}, ..., a_t^{10}, v_t^{10,a}, b_t^{10}, v_t^{10,b})^T \\in \\mathbb{R}^{40} $$\n",
    "\n",
    "We define the *bid order flows* (bOF) and *ask order flows* (aOF) at a timestamp to be 10-variable vectors computed using two consecutive order book states, where each element is given by\n",
    "\n",
    "$$ \\text{bOF}_{t,i} :=   \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      v_t^{i,b}, & b_t^i > b_{t-1}^i \\\\\n",
    "      v_t^{i,b} - v_{t-1}^{i,b}, & b_t^i = b_{t-1}^i \\\\\n",
    "      -v_t^{i,b}, & b_t^i < b_{t-1}^i \\\\\n",
    "\\end{array} \n",
    "\\right. $$\n",
    "\n",
    "$$ \\text{aOF}_{t,i} :=   \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      -v_t^{i,a}, & a_t^i > a_{t-1}^i \\\\\n",
    "      v_t^{i,a} - v_{t-1}^{i,a}, & a_t^i = a_{t-1}^i \\\\\n",
    "      v_t^{i,a}, & a_t^i < a_{t-1}^i \\\\\n",
    "\\end{array} \n",
    "\\right. $$\n",
    "\n",
    "for $i = 1, ..., 10$. With this, we define *order flow* (OF)\n",
    "\n",
    "$$ \\text{OF}_t :=  (\\text{bOF}_{t,1}, \\text{aOF}_{t,1}, ..., \\text{bOF}_{t,10}, \\text{aOF}_{t,10})^T \\in \\mathbb{R}^{20} $$\n",
    "\n",
    "and *order flow imbalance* (OFI)\n",
    "\n",
    "$$ \\text{OFI}_t := \\text{bOF}_t - \\text{aOF}_t \\in \\mathbb{R}^{10}. $$\n",
    "\n",
    "While a sequence of limit order book states is a complex non-stationary process, the above formulas for order flow and order flow imbalance transform consecutive order book states into a [stationary process](https://en.wikipedia.org/wiki/Stationary_process). This property allows for our eventual test test of the deep learning model to be reasonably similar to the training set and thus appropriate to predict off of using the model. It also allows for more ease in the learning of long-term dependencies by our LSTM layer, which Kolm et al see as a reason behind their finding that sequence length only marginally impacted model performance[<sub>[4]</sub>](#ref4). On a separate note, when trained on order flow, which keeps the bid and ask sides separate, the CNN layers of our model will be given the added flexibility of being able to combine bid and ask order flows asymmetrically, so we hypothesize that our forecasting model will perform better on order flow than on order flow imbalance. This theory is expressed by Kolm et al[<sub>[4]</sub>](#ref4) and shared by time series analysis theory[<sub>[5]</sub>](#ref5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the CNN-LSTM Model\n",
    "While the [universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem) states that a feedforward neural network with a single hidden layer can approximate any continuous function on any compact set, different neural network architectures are capable of exploiting unique structures in data, a quality that is particularly useful for the learning of complex financial time series. These different architectures, which include CNNs and LSTMs among others, can compress data and change their behavior over time in a way that supports their efficacy in difficult modeling situations. On the other hand, simple feedforward neural networks can suffer from instability and less interpretability when seeking to learn complex relationships in data[<sub>[6]</sub>](#ref6).\n",
    "\n",
    "Although artificial neural networks can be used individually, they are often complementary in their modeling capabilities and, when used together, can learn unique structures in data and improve a model's ability to execute a desired task. For instance, our CNN-LSTM model architecture we adopt consists of CNN layers and an Inception Module that compress and wrap the order book data in a manner that allows a LSTM module to learn temporal dependencies from a smaller parameter space, leading to a more parsimonious model[<sub>[1]</sub>](#ref1).\n",
    "\n",
    "[*Convolutional neural networks*](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNNs) are feedforward neural networks that can exploit data locality in an input, so in the CNN-LSTM model, CNN layers reduce the dimension of the multivariate input by aggregating bid and ask sides and levels in the order book. The output of these convolutional layers serve as an input to the [*Inception Module*](https://arxiv.org/pdf/1409.4842v1.pdf), which wraps convolutions together to capture behavior over multiple timescales, acting as a sort of moving average indicator whose decay weights are learned via [backpropagation](https://en.wikipedia.org/wiki/Backpropagation)[<sub>[1]</sub>](#ref1). Then, the outputs of the Inception Module are concatenated and reshaped into an input to the [*long short-term memory*](https://en.wikipedia.org/wiki/Long_short-term_memory) (LSTM) layer. LSTMs are a class of [*recurrent neural networks*](https://en.wikipedia.org/wiki/Recurrent_neural_network) (RNNs) that are designed to handle temporal dependencies in sequential data and alleviate the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) faced by generic RNNs[<sub>[7]</sub>](#ref7). The LSTM unit consists of a memory cell and three gates that determine what information should be remembered by the memory cell. For an $ n $-dimensional input vector $\\text{x}_t$, the LSTM unit is defined by\n",
    "\n",
    "$$ \\text{f}_t = \\sigma (\\text{U}^f \\text{x}_t + \\text{W}^f \\text{h}_{t-1} + \\text{b}^f) $$\n",
    "$$ \\text{i}_t = \\sigma (\\text{U}^i \\text{x}_t + \\text{W}^i \\text{h}_{t-1} + \\text{b}^i) $$\n",
    "$$ \\text{o}_t = \\sigma (\\text{U}^o \\text{x}_t + \\text{W}^o \\text{h}_{t-1} + \\text{b}^o) $$\n",
    "$$ \\text{c}_t = \\text{f}_t \\circ \\text{c}_{t-1} + \\text{i}_t \\circ \\text{tanh} (\\text{U}^c \\text{x}_t + \\text{W}^c \\text{h}_{t-1} + \\text{b}^c) $$\n",
    "$$ \\text{h}_t = \\text{o}_t \\circ \\text{tanh} (\\text{c}_t) $$\n",
    "\n",
    "where $ m $ is the number of LSTM units in the module, $ \\sigma := (1+e^{-x})^{-1} $ is the sigmoid activation function, $ \\text{f}_t \\in \\mathbb{R}^m $ is the forget gate's activation vector, $ \\text{i}_t \\in \\mathbb{R}^m $ is the input gate's activation vector, $ \\text{o}_t \\in \\mathbb{R}^m $ is the output gate's activation vector, $ \\text{c}_t \\in \\mathbb{R}^m $ is the LSTM unit's hidden state vector, and $ \\text{h}_t \\in \\mathbb{R}^m $ is the unit's output vector. $ \\text{U} \\in \\mathbb{R}^{m \\times n} $, $ \\text{W} \\in \\mathbb{R}^{m \\times m} $, and $ \\text{b} \\in \\mathbb{R}^m $ are learned during training and represent the weight matrices in connection to the input vector, the weight matrices in connection to the previous output state, and the bias vectors, respectively. \n",
    "\n",
    "[<img src='Images/lstm.png' style='width:500px;height:390px'/>](https://blog.mlreview.com/understanding-lstm-and-its-diagrams-37e2f46f1714)\n",
    "\n",
    "Moreover, Zhang et al[<sub>[2]</sub>](#ref2) showcase the performance benefit of applying [variational dropout](https://arxiv.org/pdf/1512.05287v5.pdf) to the model by serving as a stochastic [regularizer](https://en.wikipedia.org/wiki/Regularization_(mathematics)) to reduce [overfitting](https://en.wikipedia.org/wiki/Overfitting) and make decisions with some understanding of the predictive variation produced by our model parameters. That is, with [*Monte-Carlo (MC) dropout*](https://docs.aws.amazon.com/prescriptive-guidance/latest/ml-quantifying-uncertainty/mc-dropout.html), we can add [epistemic uncertainty](https://en.wikipedia.org/wiki/Uncertainty_quantification#Aleatoric_and_epistemic)[<sup>1</sup>](#fn1) to our neural network architecture by making multiple predictions and dropping a different random sample of neurons with every forward pass. This random sampling leads to different predictions on each evaluation iteration, so we can average the results to—in theory—improve out-of-sample predictions. The dropout layer is inserted after the Inception Module, and we determine its rate with [cross-validated grid-search](https://scikit-learn.org/stable/modules/grid_search.html).\n",
    "\n",
    "Lastly, since we formulate this forecasting problem as one of classification, we add an output layer with a [softmax activation function](https://en.wikipedia.org/wiki/Softmax_function), resulting in a final output whose elements represent the probability of observing each price movement in the next timestamp. We train the models by minimizing [categorical cross-entropy loss](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression) with [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) using the [Adam optimization algorithm](https://arxiv.org/pdf/1412.6980.pdf), whose parameters we set to those of the DeepLOB implementation of the model[<sub>[1]</sub>](#ref1).\n",
    "\n",
    "<sup>1. </sup><span id=\"fn1\"><sup>Here, epistemic uncertainty refers to the fact that we don't have an analytical understanding of the posterior distribution of the model parameters. Because of our lack of data, we are uncertain of the integrity of the model parameters we estimate via backpropagation.</sup></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input, layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def CNN_LSTM(time_series, lag_param, dropout):\n",
    "    \n",
    "    # Convolutions across LOB levels\n",
    "    if time_series == 'OF':\n",
    "        input_tensor = Input(shape=(lag_param,20,1))\n",
    "        # Combine imbalance information across sides for each level of the order book\n",
    "        layer_x = layers.Conv2D(filters=16, kernel_size=(1,2), strides=(1,2))(input_tensor)\n",
    "        layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "        # Combine imbalance information across time for each side and level of the order book\n",
    "        layer_x = layers.Conv2D(filters=16, kernel_size=(4,1), padding='same')(layer_x)\n",
    "        layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "        layer_x = layers.Conv2D(filters=16, kernel_size=(4,1), padding='same')(layer_x)\n",
    "        layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "    elif time_series == 'OFI':\n",
    "        input_tensor = Input(shape=(lag_param,10,1))\n",
    "        # Combine imbalance information across time for each side and level of the order book\n",
    "        layer_x = layers.Conv2D(filters=16, kernel_size=(4,1), padding='same')(input_tensor)\n",
    "        layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "        layer_x = layers.Conv2D(filters=16, kernel_size=(4,1), padding='same')(layer_x)\n",
    "        layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "    else:\n",
    "        raise Exception(\"'time_series' should be 'OF' or 'OFI'\")\n",
    "\n",
    "    # Combine imbalance information across all levels of the book\n",
    "    layer_x = layers.Conv2D(filters=16, kernel_size=(1,10))(layer_x)\n",
    "    layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "\n",
    "    # Inception Module\n",
    "    # Tower 1\n",
    "    tower_1 = layers.Conv2D(filters=32, kernel_size=(1,1), padding='same')(layer_x)\n",
    "    tower_1 = layers.LeakyReLU(alpha=0.01)(tower_1)\n",
    "    tower_1 = layers.Conv2D(filters=32, kernel_size=(3,1), padding='same')(tower_1)\n",
    "    tower_1 = layers.LeakyReLU(alpha=0.01)(tower_1)\n",
    "    # Tower 2\n",
    "    tower_2 = layers.Conv2D(filters=32, kernel_size=(1,1), padding='same')(layer_x)\n",
    "    tower_2 = layers.LeakyReLU(alpha=0.01)(tower_2)\n",
    "    tower_2 = layers.Conv2D(filters=32, kernel_size=(5,1), padding='same')(tower_2)\n",
    "    tower_2 = layers.LeakyReLU(alpha=0.01)(tower_2)  \n",
    "    # Tower 3\n",
    "    tower_3 = layers.MaxPooling2D(pool_size=(3,1), padding='same', strides=(1,1))(layer_x)\n",
    "    tower_3 = layers.Conv2D(filters=32, kernel_size=(1,1), padding='same')(tower_3)\n",
    "    tower_3 = layers.LeakyReLU(alpha=0.01)(tower_3)\n",
    "\n",
    "    # Concatenation and reshaping\n",
    "    layer_x = layers.concatenate([tower_1, tower_2, tower_3], axis=-1)\n",
    "    layer_x = layers.Reshape(target_shape=(lag_param, 96))(layer_x)\n",
    "    \n",
    "    # Insert variational dropout layer\n",
    "    # By setting training to true, we enable dropout during evaluation passes\n",
    "    layer_x = layers.Dropout(dropout)(layer_x, training=True)\n",
    "    \n",
    "    # LSTM with 64 hidden units\n",
    "    layer_x = layers.LSTM(units=64)(layer_x)\n",
    "    \n",
    "    # Final output layer\n",
    "    output = layers.Dense(units=3, activation='softmax')(layer_x)\n",
    "    \n",
    "    model = Model(input_tensor, output)\n",
    "    \n",
    "    opt = Adam(learning_rate=0.01, epsilon=1)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_details = {\n",
    "    'OF': {\n",
    "        'model': None, 'function': CNN_LSTM, 'data': None\n",
    "    },\n",
    "    'OFI': {\n",
    "        'model': None, 'function': CNN_LSTM, 'data': None\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "We scrape our data using [Coinbase's public API](https://github.com/danpaquin/coinbasepro-python). Our method pulls live order book state information for Bitcoin traded on Coinbase. We essentially have the ability to choose however many observations we desire. But to avoid processing the data pull for an obscene length of time, we unfortunately decide not to get as many observations as the datasets used in the aforementioned related papers, since the purpose of this notebook is not to submit a solution to the model risk management team but instead to showcase what I've taught myself in the past few months. That being said, we extract 100,000 observations over the course of about a day, and we add a couple extra to account for the transformations we make later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7618932478344387\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from cbpro import PublicClient\n",
    "from time import time, strftime, gmtime\n",
    "\n",
    "public_client = PublicClient()\n",
    "lob_data = pd.DataFrame()\n",
    "start = time()\n",
    "while len(lob_data) < 100002:\n",
    "    raw_data = pd.concat((pd.DataFrame.from_dict(public_client.get_product_order_book('BTC-USD',level=2)['asks'])[:10],\n",
    "                     pd.DataFrame.from_dict(public_client.get_product_order_book('BTC-USD',level=2)['bids'])[:10]),axis=1)\n",
    "    lob_data = pd.concat((lob_data, pd.concat((pd.DataFrame(raw_data.drop(2,axis=1).iloc[i]).T for i in range(10)), axis=1).apply(lambda x: pd.Series(x.dropna().values))))\n",
    "end = time()\n",
    "\n",
    "print((end-start)/len(lob_data))\n",
    "\n",
    "lob_data.columns = ['PRICE_ASK_1','VOLUME_ASK_1','PRICE_BID_1','VOLUME_BID_1',\n",
    "           'PRICE_ASK_2','VOLUME_ASK_2','PRICE_BID_2','VOLUME_BID_2',\n",
    "           'PRICE_ASK_3','VOLUME_ASK_3','PRICE_BID_3','VOLUME_BID_3',\n",
    "           'PRICE_ASK_4','VOLUME_ASK_4','PRICE_BID_4','VOLUME_BID_4',\n",
    "           'PRICE_ASK_5','VOLUME_ASK_5','PRICE_BID_5','VOLUME_BID_5',\n",
    "           'PRICE_ASK_6','VOLUME_ASK_6','PRICE_BID_6','VOLUME_BID_6',\n",
    "           'PRICE_ASK_7','VOLUME_ASK_7','PRICE_BID_7','VOLUME_BID_7',\n",
    "           'PRICE_ASK_8','VOLUME_ASK_8','PRICE_BID_8','VOLUME_BID_8',\n",
    "           'PRICE_ASK_9','VOLUME_ASK_9','PRICE_BID_9','VOLUME_BID_9',\n",
    "           'PRICE_ASK_10','VOLUME_ASK_10','PRICE_BID_10','VOLUME_BID_10']\n",
    "lob_data.index = range(len(lob_data))\n",
    "lob_data = lob_data.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it can vary between any two events in the dataset, the time interval between two observations is on average 0.7619 seconds. So, in lieu of the granularity that Zhang et al and Ntakaris et al boast in their respective datasets[<sub>[1]</sub>](#ref1)[<sub>[3]</sub>](#ref3)[<sub>[8]</sub>](#ref9), we simply extract labels for relative changes in only the next event. Just as in those datasets, our labels describe the percentage change of the mid price between events. For percentage changes greater than 0.002, we use label 1, for percentages change between -0.002 and 0.002, we use label 0, and for percentage changes smaller than -0.002, we use label -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRICE_ASK_0</th>\n",
       "      <th>VOLUME_ASK_0</th>\n",
       "      <th>PRICE_BID_0</th>\n",
       "      <th>VOLUME_BID_0</th>\n",
       "      <th>PRICE_ASK_1</th>\n",
       "      <th>VOLUME_ASK_1</th>\n",
       "      <th>PRICE_BID_1</th>\n",
       "      <th>VOLUME_BID_1</th>\n",
       "      <th>PRICE_ASK_2</th>\n",
       "      <th>VOLUME_ASK_2</th>\n",
       "      <th>...</th>\n",
       "      <th>VOLUME_BID_7</th>\n",
       "      <th>PRICE_ASK_8</th>\n",
       "      <th>VOLUME_ASK_8</th>\n",
       "      <th>PRICE_BID_8</th>\n",
       "      <th>VOLUME_BID_8</th>\n",
       "      <th>PRICE_ASK_9</th>\n",
       "      <th>VOLUME_ASK_9</th>\n",
       "      <th>PRICE_BID_9</th>\n",
       "      <th>VOLUME_BID_9</th>\n",
       "      <th>LABEL_1TICK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20690.00</td>\n",
       "      <td>0.084309</td>\n",
       "      <td>20689.99</td>\n",
       "      <td>0.011289</td>\n",
       "      <td>20690.02</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>20689.98</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>20691.38</td>\n",
       "      <td>0.981250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>20695.72</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>20685.97</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>20696.33</td>\n",
       "      <td>0.358238</td>\n",
       "      <td>20685.65</td>\n",
       "      <td>0.002158</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20690.00</td>\n",
       "      <td>0.084309</td>\n",
       "      <td>20689.99</td>\n",
       "      <td>0.011289</td>\n",
       "      <td>20690.02</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>20689.98</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>20691.38</td>\n",
       "      <td>0.981250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>20695.72</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>20685.97</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>20696.33</td>\n",
       "      <td>0.358238</td>\n",
       "      <td>20685.65</td>\n",
       "      <td>0.002158</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20690.00</td>\n",
       "      <td>0.084309</td>\n",
       "      <td>20689.99</td>\n",
       "      <td>0.011289</td>\n",
       "      <td>20690.02</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>20689.98</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>20691.38</td>\n",
       "      <td>0.981250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>20695.72</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>20685.97</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>20696.33</td>\n",
       "      <td>0.358238</td>\n",
       "      <td>20685.65</td>\n",
       "      <td>0.002158</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20690.00</td>\n",
       "      <td>0.084309</td>\n",
       "      <td>20689.65</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>20690.02</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>20689.64</td>\n",
       "      <td>0.006213</td>\n",
       "      <td>20691.38</td>\n",
       "      <td>0.981250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>20695.72</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>20685.57</td>\n",
       "      <td>2.043200</td>\n",
       "      <td>20696.33</td>\n",
       "      <td>0.358238</td>\n",
       "      <td>20685.32</td>\n",
       "      <td>0.018317</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20689.88</td>\n",
       "      <td>0.017278</td>\n",
       "      <td>20689.83</td>\n",
       "      <td>0.011799</td>\n",
       "      <td>20689.89</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>20686.69</td>\n",
       "      <td>0.003841</td>\n",
       "      <td>20689.90</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024724</td>\n",
       "      <td>20692.00</td>\n",
       "      <td>0.231488</td>\n",
       "      <td>20682.78</td>\n",
       "      <td>0.025542</td>\n",
       "      <td>20693.47</td>\n",
       "      <td>0.016210</td>\n",
       "      <td>20682.10</td>\n",
       "      <td>0.241633</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>21076.00</td>\n",
       "      <td>0.020025</td>\n",
       "      <td>21072.72</td>\n",
       "      <td>0.002722</td>\n",
       "      <td>21076.08</td>\n",
       "      <td>0.025441</td>\n",
       "      <td>21072.71</td>\n",
       "      <td>0.091130</td>\n",
       "      <td>21076.14</td>\n",
       "      <td>0.020467</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018979</td>\n",
       "      <td>21078.66</td>\n",
       "      <td>0.008612</td>\n",
       "      <td>21071.74</td>\n",
       "      <td>0.227235</td>\n",
       "      <td>21078.72</td>\n",
       "      <td>0.227227</td>\n",
       "      <td>21071.32</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>21071.31</td>\n",
       "      <td>0.005324</td>\n",
       "      <td>21068.78</td>\n",
       "      <td>0.094586</td>\n",
       "      <td>21071.35</td>\n",
       "      <td>0.013313</td>\n",
       "      <td>21068.65</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>21071.41</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.237270</td>\n",
       "      <td>21073.46</td>\n",
       "      <td>0.013086</td>\n",
       "      <td>21066.24</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>21073.48</td>\n",
       "      <td>0.021072</td>\n",
       "      <td>21065.65</td>\n",
       "      <td>0.355873</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>21071.31</td>\n",
       "      <td>0.005324</td>\n",
       "      <td>21068.78</td>\n",
       "      <td>0.094586</td>\n",
       "      <td>21071.35</td>\n",
       "      <td>0.013313</td>\n",
       "      <td>21068.65</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>21071.41</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.237270</td>\n",
       "      <td>21073.46</td>\n",
       "      <td>0.013086</td>\n",
       "      <td>21066.24</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>21073.48</td>\n",
       "      <td>0.021072</td>\n",
       "      <td>21065.65</td>\n",
       "      <td>0.355873</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>21064.51</td>\n",
       "      <td>0.111293</td>\n",
       "      <td>21064.50</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>21064.60</td>\n",
       "      <td>0.041741</td>\n",
       "      <td>21061.10</td>\n",
       "      <td>0.744450</td>\n",
       "      <td>21065.25</td>\n",
       "      <td>0.008726</td>\n",
       "      <td>...</td>\n",
       "      <td>0.355963</td>\n",
       "      <td>21066.80</td>\n",
       "      <td>0.009655</td>\n",
       "      <td>21058.89</td>\n",
       "      <td>0.004937</td>\n",
       "      <td>21067.00</td>\n",
       "      <td>0.227349</td>\n",
       "      <td>21057.91</td>\n",
       "      <td>0.098579</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>21064.19</td>\n",
       "      <td>0.094998</td>\n",
       "      <td>21061.10</td>\n",
       "      <td>0.744450</td>\n",
       "      <td>21064.25</td>\n",
       "      <td>0.012357</td>\n",
       "      <td>21059.36</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>21064.31</td>\n",
       "      <td>0.227376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195993</td>\n",
       "      <td>21065.25</td>\n",
       "      <td>0.009204</td>\n",
       "      <td>21056.68</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>21065.26</td>\n",
       "      <td>0.686197</td>\n",
       "      <td>21056.15</td>\n",
       "      <td>0.058778</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100001 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        PRICE_ASK_0  VOLUME_ASK_0  PRICE_BID_0  VOLUME_BID_0  PRICE_ASK_1  \\\n",
       "0          20690.00      0.084309     20689.99      0.011289     20690.02   \n",
       "1          20690.00      0.084309     20689.99      0.011289     20690.02   \n",
       "2          20690.00      0.084309     20689.99      0.011289     20690.02   \n",
       "3          20690.00      0.084309     20689.65      0.005000     20690.02   \n",
       "4          20689.88      0.017278     20689.83      0.011799     20689.89   \n",
       "...             ...           ...          ...           ...          ...   \n",
       "99996      21076.00      0.020025     21072.72      0.002722     21076.08   \n",
       "99997      21071.31      0.005324     21068.78      0.094586     21071.35   \n",
       "99998      21071.31      0.005324     21068.78      0.094586     21071.35   \n",
       "99999      21064.51      0.111293     21064.50      0.005000     21064.60   \n",
       "100000     21064.19      0.094998     21061.10      0.744450     21064.25   \n",
       "\n",
       "        VOLUME_ASK_1  PRICE_BID_1  VOLUME_BID_1  PRICE_ASK_2  VOLUME_ASK_2  \\\n",
       "0           0.000143     20689.98      0.000348     20691.38      0.981250   \n",
       "1           0.000143     20689.98      0.000348     20691.38      0.981250   \n",
       "2           0.000143     20689.98      0.000348     20691.38      0.981250   \n",
       "3           0.000143     20689.64      0.006213     20691.38      0.981250   \n",
       "4           0.000100     20686.69      0.003841     20689.90      0.000100   \n",
       "...              ...          ...           ...          ...           ...   \n",
       "99996       0.025441     21072.71      0.091130     21076.14      0.020467   \n",
       "99997       0.013313     21068.65      0.080000     21071.41      0.005000   \n",
       "99998       0.013313     21068.65      0.080000     21071.41      0.005000   \n",
       "99999       0.041741     21061.10      0.744450     21065.25      0.008726   \n",
       "100000      0.012357     21059.36      0.002087     21064.31      0.227376   \n",
       "\n",
       "        ...  VOLUME_BID_7  PRICE_ASK_8  VOLUME_ASK_8  PRICE_BID_8  \\\n",
       "0       ...      0.000300     20695.72      0.100000     20685.97   \n",
       "1       ...      0.000300     20695.72      0.100000     20685.97   \n",
       "2       ...      0.000300     20695.72      0.100000     20685.97   \n",
       "3       ...      0.150000     20695.72      0.100000     20685.57   \n",
       "4       ...      0.024724     20692.00      0.231488     20682.78   \n",
       "...     ...           ...          ...           ...          ...   \n",
       "99996   ...      0.018979     21078.66      0.008612     21071.74   \n",
       "99997   ...      0.237270     21073.46      0.013086     21066.24   \n",
       "99998   ...      0.237270     21073.46      0.013086     21066.24   \n",
       "99999   ...      0.355963     21066.80      0.009655     21058.89   \n",
       "100000  ...      0.195993     21065.25      0.009204     21056.68   \n",
       "\n",
       "        VOLUME_BID_8  PRICE_ASK_9  VOLUME_ASK_9  PRICE_BID_9  VOLUME_BID_9  \\\n",
       "0           0.003401     20696.33      0.358238     20685.65      0.002158   \n",
       "1           0.003401     20696.33      0.358238     20685.65      0.002158   \n",
       "2           0.003401     20696.33      0.358238     20685.65      0.002158   \n",
       "3           2.043200     20696.33      0.358238     20685.32      0.018317   \n",
       "4           0.025542     20693.47      0.016210     20682.10      0.241633   \n",
       "...              ...          ...           ...          ...           ...   \n",
       "99996       0.227235     21078.72      0.227227     21071.32      0.100000   \n",
       "99997       0.120000     21073.48      0.021072     21065.65      0.355873   \n",
       "99998       0.120000     21073.48      0.021072     21065.65      0.355873   \n",
       "99999       0.004937     21067.00      0.227349     21057.91      0.098579   \n",
       "100000      0.000969     21065.26      0.686197     21056.15      0.058778   \n",
       "\n",
       "        LABEL_1TICK  \n",
       "0               0.0  \n",
       "1               0.0  \n",
       "2               0.0  \n",
       "3               0.0  \n",
       "4               0.0  \n",
       "...             ...  \n",
       "99996          -1.0  \n",
       "99997           0.0  \n",
       "99998          -1.0  \n",
       "99999          -1.0  \n",
       "100000          0.0  \n",
       "\n",
       "[100001 rows x 41 columns]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lob_data['LABEL_1'] = np.zeros(len(lob_data))\n",
    "for i in range(len(lob_data)-1):\n",
    "    if (lob_data.loc[i+1,'PRICE_ASK_0'] + lob_data.loc[i+1,'PRICE_ASK_0']) > 1.00002*(lob_data.loc[i,'PRICE_ASK_0'] + lob_data.loc[i,'PRICE_ASK_0']):\n",
    "        lob_data['LABEL_1'][i] = 1\n",
    "    elif (lob_data.loc[i+1,'PRICE_BID_0'] + lob_data.loc[i+1,'PRICE_BID_0']) < 0.99998*(lob_data.loc[i,'PRICE_BID_0'] + lob_data.loc[i,'PRICE_BID_0']):\n",
    "        lob_data['LABEL_1'][i] = -1\n",
    "lob_data = lob_data.head(len(lob_data)-1)\n",
    "\n",
    "# Save the LOB data\n",
    "lob_data.to_csv('BTC-USD-LOB-{}'.format(strftime('%d-%b-%Y', gmtime())), index=False)\n",
    "\n",
    "# Load the saved data\n",
    "# lob_data = pd.read_csv('BTC-USD-LOB- ...')\n",
    "\n",
    "lob_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we now have our sequence of 40-variable vectors of order book states, we can obtain the order flow data as well as the order flow imbalance data for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bOF_0</th>\n",
       "      <th>aOF_0</th>\n",
       "      <th>bOF_1</th>\n",
       "      <th>aOF_1</th>\n",
       "      <th>bOF_2</th>\n",
       "      <th>aOF_2</th>\n",
       "      <th>bOF_3</th>\n",
       "      <th>aOF_3</th>\n",
       "      <th>bOF_4</th>\n",
       "      <th>aOF_4</th>\n",
       "      <th>...</th>\n",
       "      <th>aOF_5</th>\n",
       "      <th>bOF_6</th>\n",
       "      <th>aOF_6</th>\n",
       "      <th>bOF_7</th>\n",
       "      <th>aOF_7</th>\n",
       "      <th>bOF_8</th>\n",
       "      <th>aOF_8</th>\n",
       "      <th>bOF_9</th>\n",
       "      <th>aOF_9</th>\n",
       "      <th>LABEL_1TICK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.006213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.003067</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.009612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0432</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.018317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.011799</td>\n",
       "      <td>0.017278</td>\n",
       "      <td>-0.003841</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-1.513803</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.018317</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.006624</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057827</td>\n",
       "      <td>-0.01607</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>-0.024724</td>\n",
       "      <td>0.98125</td>\n",
       "      <td>-0.025542</td>\n",
       "      <td>0.231488</td>\n",
       "      <td>-0.241633</td>\n",
       "      <td>0.01621</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>-0.094586</td>\n",
       "      <td>0.005324</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.013313</td>\n",
       "      <td>-0.227265</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.339581</td>\n",
       "      <td>0.006926</td>\n",
       "      <td>-0.000495</td>\n",
       "      <td>0.016311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01686</td>\n",
       "      <td>-0.010967</td>\n",
       "      <td>0.238264</td>\n",
       "      <td>-0.23727</td>\n",
       "      <td>0.061199</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.013086</td>\n",
       "      <td>-0.355873</td>\n",
       "      <td>0.021072</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.111293</td>\n",
       "      <td>-0.74445</td>\n",
       "      <td>0.041741</td>\n",
       "      <td>-0.007445</td>\n",
       "      <td>0.008726</td>\n",
       "      <td>-1.085</td>\n",
       "      <td>0.044998</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.227351</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052319</td>\n",
       "      <td>-0.006386</td>\n",
       "      <td>0.007704</td>\n",
       "      <td>-0.355963</td>\n",
       "      <td>0.637379</td>\n",
       "      <td>-0.004937</td>\n",
       "      <td>0.009655</td>\n",
       "      <td>-0.098579</td>\n",
       "      <td>0.227349</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>-0.74445</td>\n",
       "      <td>0.094998</td>\n",
       "      <td>-0.002087</td>\n",
       "      <td>0.012357</td>\n",
       "      <td>-0.227364</td>\n",
       "      <td>0.227376</td>\n",
       "      <td>-0.237347</td>\n",
       "      <td>0.016477</td>\n",
       "      <td>-0.000697</td>\n",
       "      <td>0.034887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01841</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>0.009204</td>\n",
       "      <td>-0.195993</td>\n",
       "      <td>0.009204</td>\n",
       "      <td>-0.000969</td>\n",
       "      <td>0.009204</td>\n",
       "      <td>-0.058778</td>\n",
       "      <td>0.686197</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          bOF_0     aOF_0     bOF_1     aOF_1     bOF_2     aOF_2     bOF_3  \\\n",
       "0           0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "1           0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "2        -0.005       0.0 -0.006213       0.0 -0.003067       0.0 -0.009612   \n",
       "3      0.011799  0.017278 -0.003841    0.0001 -1.513803    0.0001 -0.018317   \n",
       "4           0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "99995       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "99996 -0.094586  0.005324     -0.08  0.013313 -0.227265     0.005 -0.339581   \n",
       "99997       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "99998    -0.005  0.111293  -0.74445  0.041741 -0.007445  0.008726    -1.085   \n",
       "99999  -0.74445  0.094998 -0.002087  0.012357 -0.227364  0.227376 -0.237347   \n",
       "\n",
       "          aOF_3     bOF_4     aOF_4  ...     aOF_5     bOF_6     aOF_6  \\\n",
       "0           0.0       0.0       0.0  ...       0.0       0.0       0.0   \n",
       "1           0.0       0.0       0.0  ...       0.0       0.0       0.0   \n",
       "2           0.0   -0.0003       0.0  ...       0.0 -0.002158       0.0   \n",
       "3        0.0001      -0.1  0.006624  ...  0.057827  -0.01607  0.000143   \n",
       "4           0.0       0.0       0.0  ...       0.0       0.0       0.0   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "99995       0.0       0.0       0.0  ...       0.0       0.0       0.0   \n",
       "99996  0.006926 -0.000495  0.016311  ...   0.01686 -0.010967  0.238264   \n",
       "99997       0.0       0.0       0.0  ...       0.0       0.0       0.0   \n",
       "99998  0.044998     -0.05  0.227351  ...  0.052319 -0.006386  0.007704   \n",
       "99999  0.016477 -0.000697  0.034887  ...   0.01841     -0.75  0.009204   \n",
       "\n",
       "          bOF_7     aOF_7     bOF_8     aOF_8     bOF_9     aOF_9 LABEL_1TICK  \n",
       "0           0.0       0.0       0.0       0.0       0.0       0.0         0.0  \n",
       "1           0.0       0.0       0.0       0.0       0.0       0.0         0.0  \n",
       "2         -0.15       0.0   -2.0432       0.0 -0.018317       0.0         0.0  \n",
       "3     -0.024724   0.98125 -0.025542  0.231488 -0.241633   0.01621         0.0  \n",
       "4           0.0       0.0       0.0       0.0       0.0       0.0         0.0  \n",
       "...         ...       ...       ...       ...       ...       ...         ...  \n",
       "99995       0.0       0.0       0.0       0.0       0.0       0.0        -1.0  \n",
       "99996  -0.23727  0.061199     -0.12  0.013086 -0.355873  0.021072         0.0  \n",
       "99997       0.0       0.0       0.0       0.0       0.0       0.0        -1.0  \n",
       "99998 -0.355963  0.637379 -0.004937  0.009655 -0.098579  0.227349        -1.0  \n",
       "99999 -0.195993  0.009204 -0.000969  0.009204 -0.058778  0.686197         0.0  \n",
       "\n",
       "[100000 rows x 21 columns]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "of_data = pd.DataFrame()\n",
    "for i in range(10):\n",
    "    \n",
    "    of_data['bOF_{}'.format(i)] = [None] * len(lob_data)\n",
    "    of_data['aOF_{}'.format(i)] = [None] * len(lob_data)\n",
    "\n",
    "    for j in range(1,len(lob_data)):\n",
    "            \n",
    "        # Bid Order Flow\n",
    "        if lob_data.loc[j,'PRICE_BID_{}'.format(i)] > lob_data.loc[j-1,'PRICE_BID_{}'.format(i)]:\n",
    "            of_data['bOF_{}'.format(i)][j] = lob_data.loc[j,'VOLUME_BID_{}'.format(i)]\n",
    "        elif lob_data.loc[j,'PRICE_BID_{}'.format(i)] < lob_data.loc[j-1,'PRICE_BID_{}'.format(i)]:\n",
    "            of_data['bOF_{}'.format(i)][j] = -1*lob_data.loc[j,'VOLUME_BID_{}'.format(i)]\n",
    "        else:\n",
    "            of_data['bOF_{}'.format(i)][j] = lob_data.loc[j,'VOLUME_BID_{}'.format(i)] - lob_data.loc[j-1,'VOLUME_BID_{}'.format(i)]\n",
    "            \n",
    "        # Ask Order Flow\n",
    "        if lob_data.loc[j,'PRICE_ASK_{}'.format(i)] > lob_data.loc[j-1,'PRICE_ASK_{}'.format(i)]:\n",
    "            of_data['aOF_{}'.format(i)][j] = -1*lob_data.loc[j,'VOLUME_ASK_{}'.format(i)]\n",
    "        elif lob_data.loc[j,'PRICE_ASK_{}'.format(i)] < lob_data.loc[j-1,'PRICE_ASK_{}'.format(i)]:\n",
    "            of_data['aOF_{}'.format(i)][j] = lob_data.loc[j,'VOLUME_ASK_{}'.format(i)]\n",
    "        else:\n",
    "            of_data['aOF_{}'.format(i)][j] = lob_data.loc[j,'VOLUME_ASK_{}'.format(i)] - lob_data.loc[j-1,'VOLUME_ASK_{}'.format(i)]\n",
    "            \n",
    "# Add output column to of_data\n",
    "of_data = pd.concat([of_data,lob_data.iloc[:,-1:]],axis=1)\n",
    "\n",
    "# Drop first row, which is null\n",
    "of_data = of_data.iloc[1:,:].reset_index(drop=True)\n",
    "\n",
    "model_details['OF']['data'] = of_data\n",
    "of_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OFI_0</th>\n",
       "      <th>OFI_1</th>\n",
       "      <th>OFI_2</th>\n",
       "      <th>OFI_3</th>\n",
       "      <th>OFI_4</th>\n",
       "      <th>OFI_5</th>\n",
       "      <th>OFI_6</th>\n",
       "      <th>OFI_7</th>\n",
       "      <th>OFI_8</th>\n",
       "      <th>OFI_9</th>\n",
       "      <th>LABEL_1TICK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.006213</td>\n",
       "      <td>-0.003067</td>\n",
       "      <td>-0.009612</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>-0.003401</td>\n",
       "      <td>-0.002158</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-2.0432</td>\n",
       "      <td>-0.018317</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.005478</td>\n",
       "      <td>-0.003941</td>\n",
       "      <td>-1.513903</td>\n",
       "      <td>-0.018417</td>\n",
       "      <td>-0.106624</td>\n",
       "      <td>-0.807827</td>\n",
       "      <td>-0.016213</td>\n",
       "      <td>-1.005974</td>\n",
       "      <td>-0.25703</td>\n",
       "      <td>-0.257842</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>-0.099911</td>\n",
       "      <td>-0.093313</td>\n",
       "      <td>-0.232265</td>\n",
       "      <td>-0.346507</td>\n",
       "      <td>-0.016806</td>\n",
       "      <td>-0.76686</td>\n",
       "      <td>-0.249231</td>\n",
       "      <td>-0.298469</td>\n",
       "      <td>-0.133086</td>\n",
       "      <td>-0.376945</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>-0.116293</td>\n",
       "      <td>-0.786191</td>\n",
       "      <td>-0.016171</td>\n",
       "      <td>-1.129998</td>\n",
       "      <td>-0.277351</td>\n",
       "      <td>-0.289645</td>\n",
       "      <td>-0.01409</td>\n",
       "      <td>-0.993342</td>\n",
       "      <td>-0.014593</td>\n",
       "      <td>-0.325928</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>-0.839448</td>\n",
       "      <td>-0.014444</td>\n",
       "      <td>-0.45474</td>\n",
       "      <td>-0.253823</td>\n",
       "      <td>-0.035584</td>\n",
       "      <td>-0.067699</td>\n",
       "      <td>-0.759204</td>\n",
       "      <td>-0.205197</td>\n",
       "      <td>-0.010173</td>\n",
       "      <td>-0.744974</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          OFI_0     OFI_1     OFI_2     OFI_3     OFI_4     OFI_5     OFI_6  \\\n",
       "0           0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "1           0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "2        -0.005 -0.006213 -0.003067 -0.009612   -0.0003 -0.003401 -0.002158   \n",
       "3     -0.005478 -0.003941 -1.513903 -0.018417 -0.106624 -0.807827 -0.016213   \n",
       "4           0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "99995       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "99996 -0.099911 -0.093313 -0.232265 -0.346507 -0.016806  -0.76686 -0.249231   \n",
       "99997       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "99998 -0.116293 -0.786191 -0.016171 -1.129998 -0.277351 -0.289645  -0.01409   \n",
       "99999 -0.839448 -0.014444  -0.45474 -0.253823 -0.035584 -0.067699 -0.759204   \n",
       "\n",
       "          OFI_7     OFI_8     OFI_9  LABEL_1TICK  \n",
       "0           0.0       0.0       0.0          0.0  \n",
       "1           0.0       0.0       0.0          0.0  \n",
       "2         -0.15   -2.0432 -0.018317          0.0  \n",
       "3     -1.005974  -0.25703 -0.257842          0.0  \n",
       "4           0.0       0.0       0.0          0.0  \n",
       "...         ...       ...       ...          ...  \n",
       "99995       0.0       0.0       0.0         -1.0  \n",
       "99996 -0.298469 -0.133086 -0.376945          0.0  \n",
       "99997       0.0       0.0       0.0         -1.0  \n",
       "99998 -0.993342 -0.014593 -0.325928         -1.0  \n",
       "99999 -0.205197 -0.010173 -0.744974          0.0  \n",
       "\n",
       "[100000 rows x 11 columns]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ofi_data = pd.DataFrame()\n",
    "for i in range(10):\n",
    "    ofi_data['OFI_{}'.format(i)] = of_data['bOF_{}'.format(i)] - of_data['aOF_{}'.format(i)]\n",
    "\n",
    "ofi_data = pd.concat([ofi_data,of_data.iloc[:,-1:]],axis=1).apply(lambda x: pd.Series(x.dropna().values))\n",
    "\n",
    "model_details['OFI']['data'] = ofi_data\n",
    "ofi_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology and Experimentation\n",
    "Now that we have our data, we seek to train the CNN-LSTM to accomplish the forecasting task of classifying future mid prices by their directional moves. In this section, we adopt the [Box-Jenkins approach](https://en.wikipedia.org/wiki/Box%E2%80%93Jenkins_method) to time series modeling by first taking a comically long-winded aside to recognize the time series as vector autoregressive (VAR) processes and then by tuning necessary hyperparameters of the deep learning model and evaluating the trained deep learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Autoregressive Processes\n",
    "[*Vector autoregression*](https://en.wikipedia.org/wiki/Vector_autoregression) (VAR) is the multivariate extension of [*autoregression*](https://en.wikipedia.org/wiki/Autoregressive_model). That is, a VAR model is a statistical representation of a collection of time-varying [stochastic processes](https://en.wikipedia.org/wiki/Stochastic_process) that often serves a role in the modeling of multivariate financial time series and other complex randomly-evolving processes. The term *autoregressive* indicates that each realization of the process is a linear function of previous values in the sequence plus a stochastic error term that is uncorrelated with those of other periods. That is, for a $ K $-variate time series $ \\left\\{y_i\\right\\}_{i=1}^{t-1} $ in which we assume only $ p $ past values are necessary to forecast the next observation $ y_{t} $, we have the VAR$(p)$ representation\n",
    "\n",
    "$$ y_{t}= c + A_1 y_t + A_2 y_{t-1} + ... + A_p y_{t-p} + u_{t}, $$\n",
    "\n",
    "where $ c = (c_1, ..., c_K)' $, $ A_i =  \\begin{bmatrix}\n",
    "\\alpha_{11,i} & \\dots & \\alpha_{1K,i} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\alpha_{K1,i} & \\dots & \\alpha_{KK,i} \\end{bmatrix} $, and $ \\left\\{u_i\\right\\}_{i=1}^{t} = \\left\\{(u_{1i}, ..., u_{Ki})' \\right\\}_{i=1}^{t} \\subset \\mathbb{R}^K$ is independently identically distributed with mean zero. We do not assume that all $ A_i $ are nonzero, so $p$ really represents an upper bound on the order of the process[<sub>[5]</sub>](#ref5).\n",
    "\n",
    "Now, in order to be confident that our lag parameters generalize well to out-of-sample predictions, we want to assert that the VAR$(p)$ processes are *stable*. That is, we want to check that they fluctuate about constant means and their variances do not change with respect to time. If this condition holds, then the processes are stationary[<sub>[5]</sub>](#ref5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationarity and Cointegration\n",
    "A time series is considered weakly [*stationary*](https://en.wikipedia.org/wiki/Stationary_process) if its mean and correlation coefficients remain constant as time progresses. That is, a stationary time series is one that changes over time in a manner that is consistent. This is important because when a model is learned, what the model is really learning are the regression coefficients. So, when a model predicts on data in the time series, it is doing so using its knowledge of the relationship that was learned in previous data points. Because of this, we want to ensure that that relationship remains consistent in unseen future data.\n",
    "\n",
    "As an interesting and relevant aside on the topic of stationarity, we introduce the related concepts of integration and cointegration. A time series $x_t$ is [*integrated*](https://en.wikipedia.org/wiki/Order_of_integration) of order $d$ if $(1-L)^dx_t$ is stationary, where $(1-L)x_t=x_t-x_{t-1}$. And for a collection of individually integrated time series, if a stationary linear combination of them exists, then the combined (multivariate) time series is said to be [*cointegrated*](https://en.wikipedia.org/wiki/Cointegration). We consider a multivariate time series to be stationary if the number of cointegrated relationships is equal to the number of variables in the time series[<sub>[10]</sub>](#ref10).\n",
    "\n",
    "With this understanding of cointegration, it's easy to conclude that our multivariate time series for order flow and order flow imbalance are stationary; since we know order flow and order flow imbalance to be univariately stationary[<sup>2</sup>](#fn2), they are univariately integrated of order 0 and thus a stationary linear combination can be constructed from any collection of variables in their respective time series. Moreover, for a VAR model to be a feasible representation of a time series, all variables must be of the same order of integration, so currently we feel pretty confident thinking that a VAR model is a good way to represent the data since our time series variables are all stationary.  Now, while stability in a VAR$(p)$ process implies stationarity of the process, the converse is not always true[<sub>[5]</sub>](#ref5), so although we accept our time series to be stationary, we still should assert that the VAR processes are stable in order to make appropriate use of their VAR orders.\n",
    "\n",
    "<sup>2. </sup><span id=\"fn2\"><sup>We can easily verify this with a bunch of [Augmented Dickey-Fuller (ADF) tests](https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test) but choose to accept the statement as fact.</sup></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAR Order Selection\n",
    "We turn now to fitting the order flow and order flow imbalance data as VAR models\n",
    "\n",
    "$$ y_{t}= c + A_1 y_t + A_2 y_{t-1} + ... + A_p y_{t-p} + u_{t} $$\n",
    "\n",
    "from which we extract the orders $p$ for use as the lag parameters in the deep learning model, pending that the processes are stable. Now, the articles from which we adopt the CNN-LSTM choose a generic rolling window of 100 timestamps, but squared forecast errors are higher in higher order models than in lower order models, so we want to avoid choosing unnecessarily high VAR orders by statistically estimating $p$ such that $A_p \\neq 0$ and $A_i = 0$ for $i>p$ [<sub>[5]</sub>](#ref5).\n",
    "\n",
    "In practice, estimating the optimal lag is accomplished by iteratively fitting the model with an increasing estimate $m$ for $p$ and selecting the estimate $\\hat{p}$ that minimizes the [Akaike information criterion](https://en.wikipedia.org/wiki/Akaike_information_criterion) (AIC)\n",
    "$$ \\text{AIC}(m)=2\\ln|\\tilde{\\Sigma}_u(m)|+\\frac{2mK^2}{T} $$\n",
    "for the $K$-variate VAR$(m)$ process, where $\\tilde{\\Sigma}_u(m)$ is the [maximum likelihood estimator](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) of the covariance matrix of $u_t$ and $T$ is the number of observations in the process. For a more rigorous exploration of the VAR order selection process and of multivariate time series analysis as a whole, check out Helmut Lütkepohl's book on the topic[<sub>[5]</sub>](#ref5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OF VAR order estimate:  69\n",
      "OFI VAR order estimate:  76\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.api import VAR\n",
    "for time_series in model_details:\n",
    "    model = VAR(model_details[time_series]['data'].iloc[:,:-1])\n",
    "    results = model.fit(maxlags=100, ic='aic')\n",
    "    var_order = results.k_ar\n",
    "    print(time_series + ' VAR order estimate: ', var_order)\n",
    "    model_details[time_series]['var_model'] = results\n",
    "    model_details[time_series]['lag_param'] = var_order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final point to mention here with regards to order selection: when using a lookback period of $m$, our sequential learning model will not be able to learn dependencies over intervals longer than $m$, so while we trust the estimation of VAR order, it does not guarantee that the deep learning model does not [underfit](https://en.wikipedia.org/wiki/Overfitting#Underfitting) the data. In such a circumstance, it would be compulsory to circle back and increase our window length in order to resolve underfitting and be in accordance with the Box-Jenkins approach. More on this in a later section on model diagnostics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAR Stability\n",
    "A VAR$(p)$ process $y_t$ is *stable* if all $Kp$ eigenvalues of the companion matrix $ A = \\begin{bmatrix}\n",
    "A_1 & A_2 & A_3 & \\dots & A_p \\\\\n",
    "I_K & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & I_K & 0 & \\dots & 0 \\\\\n",
    "\\vdots &  & \\ddots &  & \\vdots \\\\\n",
    "0 & \\dots & 0 & I_K & 0 \\end{bmatrix} $[<sup>3</sup>](#fn3)  are inside the [unit circle](https://en.wikipedia.org/wiki/Unit_circle). That is, the process is stable if all $Kp$ solutions to $\\text{det}(A-\\lambda I_{Kp})=0$ satisfy $|\\lambda| < 1$. We can easily check this with [`statsmodels`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.vector_ar.var_model.VAR.html#statsmodels.tsa.vector_ar.var_model.VAR).\n",
    "\n",
    "<sup>3. </sup><span id=\"fn3\"><sup>$I_n$ is the $n\\times n$ [identity matrix](https://en.wikipedia.org/wiki/Identity_matrix).</sup></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OF VAR process is stable.\n",
      "OFI VAR process is stable.\n"
     ]
    }
   ],
   "source": [
    "for time_series in model_details:\n",
    "    if model_details[time_series]['var_model'].is_stable():\n",
    "        print(time_series + ' VAR process is stable.')\n",
    "    else:\n",
    "        print(time_series + ' VAR process is unstable.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepping the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the Time Series\n",
    "For each time series, we split the data by using the first 80% for training and the remaining 20% for out-of-sample testing. Since the ordering of our data matters, we want to avoid look-ahead bias by ensuring that the test set is in the future of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "train_weight = 0.8\n",
    "\n",
    "model_details['OF']['train'] = of_data.iloc[:int(len(of_data)*train_weight)]\n",
    "model_details['OF']['test'] = of_data.iloc[int(len(of_data)*train_weight):]\n",
    "\n",
    "model_details['OFI']['train'] = ofi_data.iloc[:int(len(ofi_data)*train_weight)]\n",
    "model_details['OFI']['test'] = ofi_data.iloc[int(len(ofi_data)*train_weight):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling\n",
    "Data normalization helps avoid potential fitting difficulties that can arise as a result of multiple features assuming different value ranges. Since we are dealing with time series data in which look-ahead bias must be mitigated, we scale the training data without knowledge of the test set and use those same parameters of the training set to scale the test set. This is another reason why stationarity is important. And since our time series are multivariate, we scale each variable with its respective training column's mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b2/s2446cr942bb0jd09xl930y80000gn/T/ipykernel_7953/1394416655.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  time_series['train'][col] = time_series['train'].loc[:,col].apply(stdize_input)\n",
      "/var/folders/b2/s2446cr942bb0jd09xl930y80000gn/T/ipykernel_7953/1394416655.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  time_series['test'][col] = time_series['test'].loc[:,col].apply(stdize_input)\n"
     ]
    }
   ],
   "source": [
    "# Standardize data\n",
    "for time_series in model_details.values():\n",
    "    for col in time_series['train'].columns[:-1]:\n",
    "        mu = np.float(time_series['train'].loc[:,col].mean())\n",
    "        sigma = np.float(time_series['train'].loc[:,col].std())\n",
    "        stdize_input = lambda x: (x - mu) / sigma\n",
    "        time_series['train'][col] = time_series['train'].loc[:,col].apply(stdize_input)\n",
    "        time_series['test'][col] = time_series['test'].loc[:,col].apply(stdize_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Formatting\n",
    "To perform our sequential learning task in TensorFlow, we must transform our data into multi-dimensional arrays, called tensors, to create a time series that will serve as the inputs to the network. From this next block of code, we generate series of over-lapping subsequences to reshape the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# define method to format the data\n",
    "def format_data(data, lag, dimension):\n",
    "    '''\n",
    "    lag: the order of the VAR process, or the sequence length or lookback window\n",
    "    dimension: number of variables in the time series\n",
    "    '''\n",
    "    data = data.values\n",
    "    shape = data.shape\n",
    "    X = np.zeros((shape[0]-lag, lag, dimension))\n",
    "    Y = np.zeros((shape[0]-lag, 1)) # 1 for the number of forecasting horizons\n",
    "    for i in range(shape[0]-lag):\n",
    "        X[i] = data[i:i+lag, :dimension] # take the variables' columns as features\n",
    "        Y[i] = data[i+lag-1, -1:] # take the last column as labels\n",
    "    X = X.reshape(X.shape[0], lag, dimension, 1)\n",
    "    \n",
    "    Y += 1 # relabel as 0, 1, 2\n",
    "    \n",
    "    Y = to_categorical(Y.astype(int)) # format channel as binary vector where 1 is in the position of the true mid price change\n",
    "    \n",
    "    return X,Y\n",
    "\n",
    "# format the data\n",
    "for time_series in model_details.values():\n",
    "    time_series['train_x'], time_series['train_y'] = format_data(time_series['train'], time_series['lag_param'], \n",
    "                                                                 len(time_series['train'].columns)-1)\n",
    "    time_series['test_x'], time_series['test_y'] = format_data(time_series['test'], time_series['lag_param'], \n",
    "                                                               len(time_series['test'].columns)-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79931, 69, 20, 1)\n",
      "(79924, 76, 10, 1)\n",
      "(79931, 3)\n",
      "(79924, 3)\n"
     ]
    }
   ],
   "source": [
    "# tensor shape\n",
    "print(model_details['OF']['train_x'].shape)\n",
    "print(model_details['OFI']['train_x'].shape)\n",
    "print(model_details['OF']['train_y'].shape)\n",
    "print(model_details['OFI']['train_y'].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our input data for the model trained on order flow is a sequence of length 79,931, where each element is a snapshot capturing 69 consecutive observations of order flow, which we recall to be vectors in $\\mathbb{R}^{20}$. On the other hand, the order flow imbalance process had a slightly higher VAR order of 76, so the input sequence has 79,924 observations since each snapshot of consecutive order book representations is not as short. The label data that we use for supervised learning must have the same length as the input data, and the second channel is a binary vector of length 3 to represent the true 1-step mid price change at each observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Tuning with Cross-Validated Grid-Search\n",
    "In order to assess the [bias-variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) of selecting different [hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning), we apply time series [cross-validated grid-search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). As advocated for in BDLOB[<sub>[2]</sub>](#ref2), we employ variational dropout as a stochastic regularizer in the deep neural network and exhaustively compare scoring over several different dropout rates. We apply [early stopping](https://en.wikipedia.org/wiki/Early_stopping) to avoid overfitting by terminating training when validation loss has not inmproved for 5 consecutive epochs[<sub>[4]</sub>](#ref4).\n",
    "\n",
    "In each time series, the ordering of data matters, so we can't apply the typical [$k$-fold cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation) that is applied in cross-sectional models. Instead, a sliding or expanding training window must be used over multiple repetitions of out-of-sample predictions in order to find optimal hyperparameters.\n",
    "[<img src='Images/cross-val1.png' />](https://stackoverflow.com/questions/56601488/is-there-a-way-to-get-a-sliding-nested-cross-validation-using-sklearn)\n",
    "\n",
    "There are pros and cons to choosing either window type. For example, the expanding window includes more observations for training, but parameter confidence can lose interpretability due to the loss of sample size control[<sub>[5]</sub>](#ref5). Data permitting, the sliding window can offer sufficient training data in each repetition, but since we are in scarce supply, we choose to perform cross-validation with an expanding window.\n",
    "\n",
    "Also, although batch size and learning rate are tunable hyperparameters, we choose them to be the same as those of DeepLOB[<sub>[1]</sub>](#ref1).\n",
    "\n",
    "After many unfortunate validation attempts that resulted in no significant improvement in loss or accuracy, I decided to train using CPU instead of GPU and saw much better results[<sup>4</sup>](#fn4).\n",
    "\n",
    "<sup>4. </sup><span id=\"fn3\"><sup>This doesn't seem to be an isolated incident when dealing with Apple Silicon chips. See [this thread](https://developer.apple.com/forums/thread/701056). I updated my tensorflow-metal and tensorflow-macos installations as recommended and still got the same issue.</sup></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=5, min_delta=3e-5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Epoch 1/100\n",
      "417/417 - 13s - loss: 0.8570 - accuracy: 0.7226\n",
      "Epoch 2/100\n",
      "417/417 - 12s - loss: 0.7544 - accuracy: 0.7389\n",
      "Epoch 3/100\n",
      "417/417 - 12s - loss: 0.7538 - accuracy: 0.7389\n",
      "Epoch 4/100\n",
      "417/417 - 12s - loss: 0.7532 - accuracy: 0.7389\n",
      "Epoch 5/100\n",
      "417/417 - 12s - loss: 0.7527 - accuracy: 0.7389\n",
      "Epoch 6/100\n",
      "417/417 - 12s - loss: 0.7516 - accuracy: 0.7389\n",
      "Epoch 7/100\n",
      "417/417 - 12s - loss: 0.7506 - accuracy: 0.7389\n",
      "Epoch 8/100\n",
      "417/417 - 12s - loss: 0.7487 - accuracy: 0.7389\n",
      "Epoch 9/100\n",
      "417/417 - 12s - loss: 0.7466 - accuracy: 0.7389\n",
      "Epoch 10/100\n",
      "417/417 - 12s - loss: 0.7437 - accuracy: 0.7389\n",
      "Epoch 11/100\n",
      "417/417 - 12s - loss: 0.7398 - accuracy: 0.7389\n",
      "Epoch 12/100\n",
      "417/417 - 12s - loss: 0.7345 - accuracy: 0.7389\n",
      "Epoch 13/100\n",
      "417/417 - 12s - loss: 0.7280 - accuracy: 0.7389\n",
      "Epoch 14/100\n",
      "417/417 - 12s - loss: 0.7193 - accuracy: 0.7389\n",
      "Epoch 15/100\n",
      "417/417 - 12s - loss: 0.7071 - accuracy: 0.7389\n",
      "Epoch 16/100\n",
      "417/417 - 12s - loss: 0.6923 - accuracy: 0.7389\n",
      "Epoch 17/100\n",
      "417/417 - 12s - loss: 0.6755 - accuracy: 0.7389\n",
      "Epoch 18/100\n",
      "417/417 - 12s - loss: 0.6588 - accuracy: 0.7389\n",
      "Epoch 19/100\n",
      "417/417 - 12s - loss: 0.6460 - accuracy: 0.7395\n",
      "Epoch 20/100\n",
      "417/417 - 12s - loss: 0.6355 - accuracy: 0.7407\n",
      "Epoch 21/100\n",
      "417/417 - 12s - loss: 0.6274 - accuracy: 0.7435\n",
      "Epoch 22/100\n",
      "417/417 - 12s - loss: 0.6243 - accuracy: 0.7470\n",
      "Epoch 23/100\n",
      "417/417 - 12s - loss: 0.6193 - accuracy: 0.7479\n",
      "Epoch 24/100\n",
      "417/417 - 12s - loss: 0.6160 - accuracy: 0.7503\n",
      "Epoch 25/100\n",
      "417/417 - 12s - loss: 0.6107 - accuracy: 0.7521\n",
      "Epoch 26/100\n",
      "417/417 - 12s - loss: 0.6060 - accuracy: 0.7528\n",
      "Epoch 27/100\n",
      "417/417 - 12s - loss: 0.6023 - accuracy: 0.7543\n",
      "Epoch 28/100\n",
      "417/417 - 12s - loss: 0.6053 - accuracy: 0.7513\n",
      "Epoch 29/100\n",
      "417/417 - 12s - loss: 0.5976 - accuracy: 0.7575\n",
      "Epoch 30/100\n",
      "417/417 - 12s - loss: 0.5959 - accuracy: 0.7536\n",
      "Epoch 31/100\n",
      "417/417 - 12s - loss: 0.5922 - accuracy: 0.7527\n",
      "Epoch 32/100\n",
      "417/417 - 12s - loss: 0.5835 - accuracy: 0.7584\n",
      "Epoch 33/100\n",
      "417/417 - 12s - loss: 0.5816 - accuracy: 0.7560\n",
      "Epoch 34/100\n",
      "417/417 - 12s - loss: 0.5783 - accuracy: 0.7581\n",
      "Epoch 35/100\n",
      "417/417 - 12s - loss: 0.5742 - accuracy: 0.7578\n",
      "Epoch 36/100\n",
      "417/417 - 12s - loss: 0.5699 - accuracy: 0.7620\n",
      "Epoch 37/100\n",
      "417/417 - 12s - loss: 0.5711 - accuracy: 0.7604\n",
      "Epoch 38/100\n",
      "417/417 - 12s - loss: 0.5670 - accuracy: 0.7633\n",
      "Epoch 39/100\n",
      "417/417 - 12s - loss: 0.5647 - accuracy: 0.7628\n",
      "Epoch 40/100\n",
      "417/417 - 12s - loss: 0.5615 - accuracy: 0.7635\n",
      "Epoch 41/100\n",
      "417/417 - 12s - loss: 0.5624 - accuracy: 0.7643\n",
      "Epoch 42/100\n",
      "417/417 - 12s - loss: 0.5589 - accuracy: 0.7617\n",
      "Epoch 43/100\n",
      "417/417 - 12s - loss: 0.5548 - accuracy: 0.7655\n",
      "Epoch 44/100\n",
      "417/417 - 12s - loss: 0.5534 - accuracy: 0.7647\n",
      "Epoch 45/100\n",
      "417/417 - 12s - loss: 0.5510 - accuracy: 0.7672\n",
      "Epoch 46/100\n",
      "417/417 - 12s - loss: 0.5494 - accuracy: 0.7683\n",
      "Epoch 47/100\n",
      "417/417 - 12s - loss: 0.5480 - accuracy: 0.7701\n",
      "Epoch 48/100\n",
      "417/417 - 12s - loss: 0.5458 - accuracy: 0.7678\n",
      "Epoch 49/100\n",
      "417/417 - 12s - loss: 0.5459 - accuracy: 0.7670\n",
      "Epoch 50/100\n",
      "417/417 - 12s - loss: 0.5432 - accuracy: 0.7677\n",
      "Epoch 51/100\n",
      "417/417 - 12s - loss: 0.5422 - accuracy: 0.7720\n",
      "Epoch 52/100\n",
      "417/417 - 12s - loss: 0.5411 - accuracy: 0.7709\n",
      "Epoch 53/100\n",
      "417/417 - 12s - loss: 0.5386 - accuracy: 0.7694\n",
      "Epoch 54/100\n",
      "417/417 - 12s - loss: 0.5389 - accuracy: 0.7689\n",
      "Epoch 55/100\n",
      "417/417 - 12s - loss: 0.5369 - accuracy: 0.7725\n",
      "Epoch 56/100\n",
      "417/417 - 12s - loss: 0.5376 - accuracy: 0.7698\n",
      "Epoch 57/100\n",
      "417/417 - 12s - loss: 0.5353 - accuracy: 0.7736\n",
      "Epoch 58/100\n",
      "417/417 - 12s - loss: 0.5342 - accuracy: 0.7734\n",
      "Epoch 59/100\n",
      "417/417 - 12s - loss: 0.5324 - accuracy: 0.7732\n",
      "Epoch 60/100\n",
      "417/417 - 12s - loss: 0.5326 - accuracy: 0.7724\n",
      "Epoch 61/100\n",
      "417/417 - 12s - loss: 0.5321 - accuracy: 0.7729\n",
      "Epoch 62/100\n",
      "417/417 - 12s - loss: 0.5304 - accuracy: 0.7738\n",
      "Epoch 63/100\n",
      "417/417 - 12s - loss: 0.5284 - accuracy: 0.7759\n",
      "Epoch 64/100\n",
      "417/417 - 12s - loss: 0.5276 - accuracy: 0.7747\n",
      "Epoch 65/100\n",
      "417/417 - 12s - loss: 0.5284 - accuracy: 0.7739\n",
      "Epoch 66/100\n",
      "417/417 - 12s - loss: 0.5272 - accuracy: 0.7786\n",
      "Epoch 67/100\n",
      "417/417 - 12s - loss: 0.5266 - accuracy: 0.7773\n",
      "Epoch 68/100\n",
      "417/417 - 12s - loss: 0.5270 - accuracy: 0.7732\n",
      "Epoch 69/100\n",
      "417/417 - 12s - loss: 0.5257 - accuracy: 0.7733\n",
      "Epoch 70/100\n",
      "417/417 - 12s - loss: 0.5249 - accuracy: 0.7753\n",
      "Epoch 71/100\n",
      "417/417 - 12s - loss: 0.5242 - accuracy: 0.7755\n",
      "Epoch 72/100\n",
      "417/417 - 12s - loss: 0.5225 - accuracy: 0.7766\n",
      "Epoch 73/100\n",
      "417/417 - 12s - loss: 0.5252 - accuracy: 0.7738\n",
      "Epoch 74/100\n",
      "417/417 - 12s - loss: 0.5227 - accuracy: 0.7765\n",
      "Epoch 75/100\n",
      "417/417 - 12s - loss: 0.5223 - accuracy: 0.7755\n",
      "Epoch 76/100\n",
      "417/417 - 12s - loss: 0.5218 - accuracy: 0.7774\n",
      "Epoch 77/100\n",
      "417/417 - 12s - loss: 0.5212 - accuracy: 0.7757\n",
      "Epoch 78/100\n",
      "417/417 - 12s - loss: 0.5214 - accuracy: 0.7745\n",
      "Epoch 79/100\n",
      "417/417 - 12s - loss: 0.5221 - accuracy: 0.7756\n",
      "Epoch 80/100\n",
      "417/417 - 12s - loss: 0.5187 - accuracy: 0.7774\n",
      "Epoch 81/100\n",
      "417/417 - 12s - loss: 0.5172 - accuracy: 0.7790\n",
      "Epoch 82/100\n",
      "417/417 - 12s - loss: 0.5178 - accuracy: 0.7777\n",
      "Epoch 83/100\n",
      "417/417 - 12s - loss: 0.5167 - accuracy: 0.7759\n",
      "Epoch 84/100\n",
      "417/417 - 12s - loss: 0.5156 - accuracy: 0.7776\n",
      "Epoch 85/100\n",
      "417/417 - 12s - loss: 0.5163 - accuracy: 0.7787\n",
      "Epoch 86/100\n",
      "417/417 - 12s - loss: 0.5145 - accuracy: 0.7801\n",
      "Epoch 87/100\n",
      "417/417 - 12s - loss: 0.5139 - accuracy: 0.7786\n",
      "Epoch 88/100\n",
      "417/417 - 12s - loss: 0.5156 - accuracy: 0.7783\n",
      "Epoch 89/100\n",
      "417/417 - 12s - loss: 0.5134 - accuracy: 0.7777\n",
      "Epoch 90/100\n",
      "417/417 - 12s - loss: 0.5147 - accuracy: 0.7802\n",
      "Epoch 91/100\n",
      "417/417 - 12s - loss: 0.5150 - accuracy: 0.7810\n",
      "Epoch 92/100\n",
      "417/417 - 12s - loss: 0.5122 - accuracy: 0.7788\n",
      "Epoch 93/100\n",
      "417/417 - 12s - loss: 0.5117 - accuracy: 0.7798\n",
      "Epoch 94/100\n",
      "417/417 - 12s - loss: 0.5134 - accuracy: 0.7818\n",
      "Epoch 95/100\n",
      "417/417 - 12s - loss: 0.5098 - accuracy: 0.7795\n",
      "Epoch 96/100\n",
      "417/417 - 12s - loss: 0.5096 - accuracy: 0.7795\n",
      "Epoch 97/100\n",
      "417/417 - 12s - loss: 0.5113 - accuracy: 0.7795\n",
      "Epoch 98/100\n",
      "417/417 - 12s - loss: 0.5091 - accuracy: 0.7797\n",
      "Epoch 99/100\n",
      "417/417 - 12s - loss: 0.5112 - accuracy: 0.7797\n",
      "Epoch 100/100\n",
      "417/417 - 12s - loss: 0.5106 - accuracy: 0.7771\n",
      "417/417 - 3s - loss: 0.5450 - accuracy: 0.7736\n",
      "Epoch 1/100\n",
      "833/833 - 25s - loss: 0.7986 - accuracy: 0.7337\n",
      "Epoch 2/100\n",
      "833/833 - 24s - loss: 0.7524 - accuracy: 0.7396\n",
      "Epoch 3/100\n",
      "833/833 - 24s - loss: 0.7509 - accuracy: 0.7396\n",
      "Epoch 4/100\n",
      "833/833 - 24s - loss: 0.7483 - accuracy: 0.7396\n",
      "Epoch 5/100\n",
      "833/833 - 24s - loss: 0.7435 - accuracy: 0.7396\n",
      "Epoch 6/100\n",
      "833/833 - 24s - loss: 0.7329 - accuracy: 0.7396\n",
      "Epoch 7/100\n",
      "833/833 - 24s - loss: 0.7108 - accuracy: 0.7396\n",
      "Epoch 8/100\n",
      "833/833 - 24s - loss: 0.6753 - accuracy: 0.7396\n",
      "Epoch 9/100\n",
      "833/833 - 24s - loss: 0.6378 - accuracy: 0.7405\n",
      "Epoch 10/100\n",
      "833/833 - 24s - loss: 0.6154 - accuracy: 0.7444\n",
      "Epoch 11/100\n",
      "833/833 - 24s - loss: 0.6042 - accuracy: 0.7514\n",
      "Epoch 12/100\n",
      "833/833 - 24s - loss: 0.5973 - accuracy: 0.7534\n",
      "Epoch 13/100\n",
      "833/833 - 24s - loss: 0.5920 - accuracy: 0.7564\n",
      "Epoch 14/100\n",
      "833/833 - 24s - loss: 0.5874 - accuracy: 0.7605\n",
      "Epoch 15/100\n",
      "833/833 - 24s - loss: 0.5849 - accuracy: 0.7630\n",
      "Epoch 16/100\n",
      "833/833 - 24s - loss: 0.5813 - accuracy: 0.7627\n",
      "Epoch 17/100\n",
      "833/833 - 24s - loss: 0.5804 - accuracy: 0.7623\n",
      "Epoch 18/100\n",
      "833/833 - 24s - loss: 0.5771 - accuracy: 0.7657\n",
      "Epoch 19/100\n",
      "833/833 - 24s - loss: 0.5760 - accuracy: 0.7632\n",
      "Epoch 20/100\n",
      "833/833 - 24s - loss: 0.5735 - accuracy: 0.7649\n",
      "Epoch 21/100\n",
      "833/833 - 24s - loss: 0.5733 - accuracy: 0.7640\n",
      "Epoch 22/100\n",
      "833/833 - 24s - loss: 0.5696 - accuracy: 0.7668\n",
      "Epoch 23/100\n",
      "833/833 - 24s - loss: 0.5691 - accuracy: 0.7650\n",
      "Epoch 24/100\n",
      "833/833 - 24s - loss: 0.5677 - accuracy: 0.7652\n",
      "Epoch 25/100\n",
      "833/833 - 24s - loss: 0.5646 - accuracy: 0.7669\n",
      "Epoch 26/100\n",
      "833/833 - 24s - loss: 0.5628 - accuracy: 0.7688\n",
      "Epoch 27/100\n",
      "833/833 - 24s - loss: 0.5620 - accuracy: 0.7648\n",
      "Epoch 28/100\n",
      "833/833 - 24s - loss: 0.5586 - accuracy: 0.7691\n",
      "Epoch 29/100\n",
      "833/833 - 24s - loss: 0.5587 - accuracy: 0.7661\n",
      "Epoch 30/100\n",
      "833/833 - 24s - loss: 0.5559 - accuracy: 0.7665\n",
      "Epoch 31/100\n",
      "833/833 - 24s - loss: 0.5532 - accuracy: 0.7706\n",
      "Epoch 32/100\n",
      "833/833 - 24s - loss: 0.5510 - accuracy: 0.7695\n",
      "Epoch 33/100\n",
      "833/833 - 24s - loss: 0.5489 - accuracy: 0.7701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100\n",
      "833/833 - 24s - loss: 0.5497 - accuracy: 0.7682\n",
      "Epoch 35/100\n",
      "833/833 - 24s - loss: 0.5472 - accuracy: 0.7688\n",
      "Epoch 36/100\n",
      "833/833 - 24s - loss: 0.5452 - accuracy: 0.7694\n",
      "Epoch 37/100\n",
      "833/833 - 24s - loss: 0.5427 - accuracy: 0.7706\n",
      "Epoch 38/100\n",
      "833/833 - 24s - loss: 0.5398 - accuracy: 0.7706\n",
      "Epoch 39/100\n",
      "833/833 - 24s - loss: 0.5378 - accuracy: 0.7715\n",
      "Epoch 40/100\n",
      "833/833 - 24s - loss: 0.5362 - accuracy: 0.7738\n",
      "Epoch 41/100\n",
      "833/833 - 24s - loss: 0.5325 - accuracy: 0.7739\n",
      "Epoch 42/100\n",
      "833/833 - 24s - loss: 0.5314 - accuracy: 0.7737\n",
      "Epoch 43/100\n",
      "833/833 - 24s - loss: 0.5293 - accuracy: 0.7747\n",
      "Epoch 44/100\n",
      "833/833 - 24s - loss: 0.5308 - accuracy: 0.7733\n",
      "Epoch 45/100\n",
      "833/833 - 24s - loss: 0.5291 - accuracy: 0.7735\n",
      "Epoch 46/100\n",
      "833/833 - 24s - loss: 0.5254 - accuracy: 0.7775\n",
      "Epoch 47/100\n",
      "833/833 - 24s - loss: 0.5257 - accuracy: 0.7735\n",
      "Epoch 48/100\n",
      "833/833 - 24s - loss: 0.5238 - accuracy: 0.7754\n",
      "Epoch 49/100\n",
      "833/833 - 24s - loss: 0.5226 - accuracy: 0.7751\n",
      "Epoch 50/100\n",
      "833/833 - 24s - loss: 0.5210 - accuracy: 0.7768\n",
      "Epoch 51/100\n",
      "833/833 - 24s - loss: 0.5212 - accuracy: 0.7766\n",
      "Epoch 52/100\n",
      "833/833 - 24s - loss: 0.5199 - accuracy: 0.7785\n",
      "Epoch 53/100\n",
      "833/833 - 24s - loss: 0.5186 - accuracy: 0.7796\n",
      "Epoch 54/100\n",
      "833/833 - 24s - loss: 0.5174 - accuracy: 0.7783\n",
      "Epoch 55/100\n",
      "833/833 - 24s - loss: 0.5166 - accuracy: 0.7778\n",
      "Epoch 56/100\n",
      "833/833 - 24s - loss: 0.5156 - accuracy: 0.7797\n",
      "Epoch 57/100\n",
      "833/833 - 24s - loss: 0.5155 - accuracy: 0.7796\n",
      "Epoch 58/100\n",
      "833/833 - 24s - loss: 0.5138 - accuracy: 0.7785\n",
      "Epoch 59/100\n",
      "833/833 - 24s - loss: 0.5118 - accuracy: 0.7825\n",
      "Epoch 60/100\n",
      "833/833 - 24s - loss: 0.5115 - accuracy: 0.7811\n",
      "Epoch 61/100\n",
      "833/833 - 24s - loss: 0.5099 - accuracy: 0.7807\n",
      "Epoch 62/100\n",
      "833/833 - 24s - loss: 0.5095 - accuracy: 0.7799\n",
      "Epoch 63/100\n",
      "833/833 - 24s - loss: 0.5082 - accuracy: 0.7807\n",
      "Epoch 64/100\n",
      "833/833 - 24s - loss: 0.5074 - accuracy: 0.7824\n",
      "Epoch 65/100\n",
      "833/833 - 24s - loss: 0.5071 - accuracy: 0.7810\n",
      "Epoch 66/100\n",
      "833/833 - 24s - loss: 0.5063 - accuracy: 0.7815\n",
      "Epoch 67/100\n",
      "833/833 - 24s - loss: 0.5053 - accuracy: 0.7817\n",
      "Epoch 68/100\n",
      "833/833 - 24s - loss: 0.5035 - accuracy: 0.7845\n",
      "Epoch 69/100\n",
      "833/833 - 24s - loss: 0.5032 - accuracy: 0.7843\n",
      "Epoch 70/100\n",
      "833/833 - 24s - loss: 0.5016 - accuracy: 0.7828\n",
      "Epoch 71/100\n",
      "833/833 - 24s - loss: 0.5015 - accuracy: 0.7855\n",
      "Epoch 72/100\n",
      "833/833 - 24s - loss: 0.5009 - accuracy: 0.7821\n",
      "Epoch 73/100\n",
      "833/833 - 24s - loss: 0.4988 - accuracy: 0.7847\n",
      "Epoch 74/100\n",
      "833/833 - 24s - loss: 0.4972 - accuracy: 0.7866\n",
      "Epoch 75/100\n",
      "833/833 - 24s - loss: 0.4967 - accuracy: 0.7864\n",
      "Epoch 76/100\n",
      "833/833 - 24s - loss: 0.4974 - accuracy: 0.7858\n",
      "Epoch 77/100\n",
      "833/833 - 24s - loss: 0.4951 - accuracy: 0.7862\n",
      "Epoch 78/100\n",
      "833/833 - 24s - loss: 0.4943 - accuracy: 0.7867\n",
      "Epoch 79/100\n",
      "833/833 - 24s - loss: 0.4947 - accuracy: 0.7861\n",
      "Epoch 80/100\n",
      "833/833 - 24s - loss: 0.4929 - accuracy: 0.7853\n",
      "Epoch 81/100\n",
      "833/833 - 24s - loss: 0.4922 - accuracy: 0.7872\n",
      "Epoch 82/100\n",
      "833/833 - 24s - loss: 0.4914 - accuracy: 0.7870\n",
      "Epoch 83/100\n",
      "833/833 - 24s - loss: 0.4907 - accuracy: 0.7879\n",
      "Epoch 84/100\n",
      "833/833 - 24s - loss: 0.4903 - accuracy: 0.7882\n",
      "Epoch 85/100\n",
      "833/833 - 24s - loss: 0.4891 - accuracy: 0.7870\n",
      "Epoch 86/100\n",
      "833/833 - 24s - loss: 0.4891 - accuracy: 0.7874\n",
      "Epoch 87/100\n",
      "833/833 - 24s - loss: 0.4870 - accuracy: 0.7883\n",
      "Epoch 88/100\n",
      "833/833 - 24s - loss: 0.4888 - accuracy: 0.7880\n",
      "Epoch 89/100\n",
      "833/833 - 24s - loss: 0.4870 - accuracy: 0.7897\n",
      "Epoch 90/100\n",
      "833/833 - 24s - loss: 0.4863 - accuracy: 0.7906\n",
      "Epoch 91/100\n",
      "833/833 - 24s - loss: 0.4845 - accuracy: 0.7893\n",
      "Epoch 92/100\n",
      "833/833 - 24s - loss: 0.4844 - accuracy: 0.7897\n",
      "Epoch 93/100\n",
      "833/833 - 24s - loss: 0.4844 - accuracy: 0.7893\n",
      "Epoch 94/100\n",
      "833/833 - 24s - loss: 0.4834 - accuracy: 0.7902\n",
      "Epoch 95/100\n",
      "833/833 - 24s - loss: 0.4838 - accuracy: 0.7886\n",
      "Epoch 96/100\n",
      "833/833 - 24s - loss: 0.4824 - accuracy: 0.7902\n",
      "Epoch 97/100\n",
      "833/833 - 24s - loss: 0.4827 - accuracy: 0.7894\n",
      "Epoch 98/100\n",
      "833/833 - 24s - loss: 0.4804 - accuracy: 0.7934\n",
      "Epoch 99/100\n",
      "833/833 - 24s - loss: 0.4795 - accuracy: 0.7917\n",
      "Epoch 100/100\n",
      "833/833 - 24s - loss: 0.4807 - accuracy: 0.7900\n",
      "417/417 - 3s - loss: 0.5064 - accuracy: 0.7731\n",
      "Epoch 1/100\n",
      "1249/1249 - 33s - loss: 0.7762 - accuracy: 0.7424\n",
      "Epoch 2/100\n",
      "1249/1249 - 33s - loss: 0.7410 - accuracy: 0.7440\n",
      "Epoch 3/100\n",
      "1249/1249 - 33s - loss: 0.7315 - accuracy: 0.7440\n",
      "Epoch 4/100\n",
      "1249/1249 - 33s - loss: 0.7011 - accuracy: 0.7440\n",
      "Epoch 5/100\n",
      "1249/1249 - 33s - loss: 0.6460 - accuracy: 0.7441\n",
      "Epoch 6/100\n",
      "1249/1249 - 33s - loss: 0.6069 - accuracy: 0.7476\n",
      "Epoch 7/100\n",
      "1249/1249 - 32s - loss: 0.5927 - accuracy: 0.7539\n",
      "Epoch 8/100\n",
      "1249/1249 - 32s - loss: 0.5842 - accuracy: 0.7594\n",
      "Epoch 9/100\n",
      "1249/1249 - 33s - loss: 0.5781 - accuracy: 0.7618\n",
      "Epoch 10/100\n",
      "1249/1249 - 33s - loss: 0.5743 - accuracy: 0.7633\n",
      "Epoch 11/100\n",
      "1249/1249 - 33s - loss: 0.5687 - accuracy: 0.7649\n",
      "Epoch 12/100\n",
      "1249/1249 - 33s - loss: 0.5696 - accuracy: 0.7655\n",
      "Epoch 13/100\n",
      "1249/1249 - 34s - loss: 0.5663 - accuracy: 0.7648\n",
      "Epoch 14/100\n",
      "1249/1249 - 33s - loss: 0.5642 - accuracy: 0.7661\n",
      "Epoch 15/100\n",
      "1249/1249 - 34s - loss: 0.5612 - accuracy: 0.7680\n",
      "Epoch 16/100\n",
      "1249/1249 - 33s - loss: 0.5578 - accuracy: 0.7659\n",
      "Epoch 17/100\n",
      "1249/1249 - 33s - loss: 0.5530 - accuracy: 0.7682\n",
      "Epoch 18/100\n",
      "1249/1249 - 33s - loss: 0.5486 - accuracy: 0.7693\n",
      "Epoch 19/100\n",
      "1249/1249 - 33s - loss: 0.5474 - accuracy: 0.7717\n",
      "Epoch 20/100\n",
      "1249/1249 - 33s - loss: 0.5446 - accuracy: 0.7708\n",
      "Epoch 21/100\n",
      "1249/1249 - 33s - loss: 0.5487 - accuracy: 0.7699\n",
      "Epoch 22/100\n",
      "1249/1249 - 33s - loss: 0.5469 - accuracy: 0.7709\n",
      "Epoch 23/100\n",
      "1249/1249 - 34s - loss: 0.5444 - accuracy: 0.7706\n",
      "Epoch 24/100\n",
      "1249/1249 - 34s - loss: 0.5403 - accuracy: 0.7719\n",
      "Epoch 25/100\n",
      "1249/1249 - 34s - loss: 0.5475 - accuracy: 0.7703\n",
      "Epoch 26/100\n",
      "1249/1249 - 33s - loss: 0.5495 - accuracy: 0.7702\n",
      "Epoch 27/100\n",
      "1249/1249 - 33s - loss: 0.5486 - accuracy: 0.7701\n",
      "Epoch 28/100\n",
      "1249/1249 - 33s - loss: 0.5465 - accuracy: 0.7696\n",
      "Epoch 29/100\n",
      "1249/1249 - 33s - loss: 0.5446 - accuracy: 0.7712\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00029: early stopping\n",
      "417/417 - 3s - loss: 0.5952 - accuracy: 0.7445\n",
      "Epoch 1/100\n",
      "1666/1666 - 48s - loss: 0.7722 - accuracy: 0.7413\n",
      "Epoch 2/100\n",
      "1666/1666 - 47s - loss: 0.7472 - accuracy: 0.7413\n",
      "Epoch 3/100\n",
      "1666/1666 - 47s - loss: 0.7389 - accuracy: 0.7413\n",
      "Epoch 4/100\n",
      "1666/1666 - 48s - loss: 0.7089 - accuracy: 0.7413\n",
      "Epoch 5/100\n",
      "1666/1666 - 48s - loss: 0.6493 - accuracy: 0.7418\n",
      "Epoch 6/100\n",
      "1666/1666 - 48s - loss: 0.6145 - accuracy: 0.7512\n",
      "Epoch 7/100\n",
      "1666/1666 - 48s - loss: 0.6011 - accuracy: 0.7556\n",
      "Epoch 8/100\n",
      "1666/1666 - 48s - loss: 0.5936 - accuracy: 0.7572\n",
      "Epoch 9/100\n",
      "1666/1666 - 48s - loss: 0.5821 - accuracy: 0.7591\n",
      "Epoch 10/100\n",
      "1666/1666 - 48s - loss: 0.5725 - accuracy: 0.7612\n",
      "Epoch 11/100\n",
      "1666/1666 - 48s - loss: 0.5662 - accuracy: 0.7621\n",
      "Epoch 12/100\n",
      "1666/1666 - 48s - loss: 0.5614 - accuracy: 0.7626\n",
      "Epoch 13/100\n",
      "1666/1666 - 48s - loss: 0.5575 - accuracy: 0.7643\n",
      "Epoch 14/100\n",
      "1666/1666 - 48s - loss: 0.5539 - accuracy: 0.7660\n",
      "Epoch 15/100\n",
      "1666/1666 - 48s - loss: 0.5500 - accuracy: 0.7656\n",
      "Epoch 16/100\n",
      "1666/1666 - 48s - loss: 0.5467 - accuracy: 0.7669\n",
      "Epoch 17/100\n",
      "1666/1666 - 48s - loss: 0.5430 - accuracy: 0.7663\n",
      "Epoch 18/100\n",
      "1666/1666 - 48s - loss: 0.5415 - accuracy: 0.7657\n",
      "Epoch 19/100\n",
      "1666/1666 - 48s - loss: 0.5378 - accuracy: 0.7660\n",
      "Epoch 20/100\n",
      "1666/1666 - 48s - loss: 0.5359 - accuracy: 0.7664\n",
      "Epoch 21/100\n",
      "1666/1666 - 48s - loss: 0.5329 - accuracy: 0.7674\n",
      "Epoch 22/100\n",
      "1666/1666 - 48s - loss: 0.5316 - accuracy: 0.7676\n",
      "Epoch 23/100\n",
      "1666/1666 - 48s - loss: 0.5287 - accuracy: 0.7679\n",
      "Epoch 24/100\n",
      "1666/1666 - 48s - loss: 0.5264 - accuracy: 0.7700\n",
      "Epoch 25/100\n",
      "1666/1666 - 48s - loss: 0.5248 - accuracy: 0.7689\n",
      "Epoch 26/100\n",
      "1666/1666 - 48s - loss: 0.5238 - accuracy: 0.7700\n",
      "Epoch 27/100\n",
      "1666/1666 - 48s - loss: 0.5235 - accuracy: 0.7688\n",
      "Epoch 28/100\n",
      "1666/1666 - 48s - loss: 0.5223 - accuracy: 0.7699\n",
      "Epoch 29/100\n",
      "1666/1666 - 48s - loss: 0.5223 - accuracy: 0.7705\n",
      "Epoch 30/100\n",
      "1666/1666 - 48s - loss: 0.5198 - accuracy: 0.7720\n",
      "Epoch 31/100\n",
      "1666/1666 - 48s - loss: 0.5184 - accuracy: 0.7726\n",
      "Epoch 32/100\n",
      "1666/1666 - 49s - loss: 0.5185 - accuracy: 0.7707\n",
      "Epoch 33/100\n",
      "1666/1666 - 49s - loss: 0.5173 - accuracy: 0.7719\n",
      "Epoch 34/100\n",
      "1666/1666 - 49s - loss: 0.5160 - accuracy: 0.7731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100\n",
      "1666/1666 - 49s - loss: 0.5155 - accuracy: 0.7727\n",
      "Epoch 36/100\n",
      "1666/1666 - 48s - loss: 0.5150 - accuracy: 0.7729\n",
      "Epoch 37/100\n",
      "1666/1666 - 48s - loss: 0.5134 - accuracy: 0.7731\n",
      "Epoch 38/100\n",
      "1666/1666 - 48s - loss: 0.5127 - accuracy: 0.7735\n",
      "Epoch 39/100\n",
      "1666/1666 - 48s - loss: 0.5115 - accuracy: 0.7722\n",
      "Epoch 40/100\n",
      "1666/1666 - 48s - loss: 0.5114 - accuracy: 0.7739\n",
      "Epoch 41/100\n",
      "1666/1666 - 48s - loss: 0.5105 - accuracy: 0.7747\n",
      "Epoch 42/100\n",
      "1666/1666 - 48s - loss: 0.5090 - accuracy: 0.7728\n",
      "Epoch 43/100\n",
      "1666/1666 - 48s - loss: 0.5085 - accuracy: 0.7734\n",
      "Epoch 44/100\n",
      "1666/1666 - 48s - loss: 0.5078 - accuracy: 0.7728\n",
      "Epoch 45/100\n",
      "1666/1666 - 48s - loss: 0.5065 - accuracy: 0.7736\n",
      "Epoch 46/100\n",
      "1666/1666 - 48s - loss: 0.5068 - accuracy: 0.7740\n",
      "Epoch 47/100\n",
      "1666/1666 - 48s - loss: 0.5056 - accuracy: 0.7731\n",
      "Epoch 48/100\n",
      "1666/1666 - 48s - loss: 0.5042 - accuracy: 0.7735\n",
      "Epoch 49/100\n",
      "1666/1666 - 48s - loss: 0.5047 - accuracy: 0.7745\n",
      "Epoch 50/100\n",
      "1666/1666 - 48s - loss: 0.5029 - accuracy: 0.7740\n",
      "Epoch 51/100\n",
      "1666/1666 - 48s - loss: 0.5025 - accuracy: 0.7759\n",
      "Epoch 52/100\n",
      "1666/1666 - 48s - loss: 0.5020 - accuracy: 0.7768\n",
      "Epoch 53/100\n",
      "1666/1666 - 48s - loss: 0.5013 - accuracy: 0.7757\n",
      "Epoch 54/100\n",
      "1666/1666 - 48s - loss: 0.5015 - accuracy: 0.7755\n",
      "Epoch 55/100\n",
      "1666/1666 - 48s - loss: 0.4996 - accuracy: 0.7774\n",
      "Epoch 56/100\n",
      "1666/1666 - 48s - loss: 0.4988 - accuracy: 0.7770\n",
      "Epoch 57/100\n",
      "1666/1666 - 48s - loss: 0.4985 - accuracy: 0.7742\n",
      "Epoch 58/100\n",
      "1666/1666 - 48s - loss: 0.4981 - accuracy: 0.7764\n",
      "Epoch 59/100\n",
      "1666/1666 - 48s - loss: 0.4979 - accuracy: 0.7772\n",
      "Epoch 60/100\n",
      "1666/1666 - 48s - loss: 0.4967 - accuracy: 0.7769\n",
      "Epoch 61/100\n",
      "1666/1666 - 48s - loss: 0.4964 - accuracy: 0.7783\n",
      "Epoch 62/100\n",
      "1666/1666 - 48s - loss: 0.4964 - accuracy: 0.7762\n",
      "Epoch 63/100\n",
      "1666/1666 - 48s - loss: 0.4956 - accuracy: 0.7781\n",
      "Epoch 64/100\n",
      "1666/1666 - 48s - loss: 0.4952 - accuracy: 0.7772\n",
      "Epoch 65/100\n",
      "1666/1666 - 48s - loss: 0.4953 - accuracy: 0.7780\n",
      "Epoch 66/100\n",
      "1666/1666 - 48s - loss: 0.4946 - accuracy: 0.7777\n",
      "Epoch 67/100\n",
      "1666/1666 - 48s - loss: 0.4945 - accuracy: 0.7783\n",
      "Epoch 68/100\n",
      "1666/1666 - 48s - loss: 0.4936 - accuracy: 0.7783\n",
      "Epoch 69/100\n",
      "1666/1666 - 48s - loss: 0.4932 - accuracy: 0.7784\n",
      "Epoch 70/100\n",
      "1666/1666 - 48s - loss: 0.4931 - accuracy: 0.7797\n",
      "Epoch 71/100\n",
      "1666/1666 - 48s - loss: 0.4924 - accuracy: 0.7791\n",
      "Epoch 72/100\n",
      "1666/1666 - 48s - loss: 0.4917 - accuracy: 0.7789\n",
      "Epoch 73/100\n",
      "1666/1666 - 48s - loss: 0.4912 - accuracy: 0.7777\n",
      "Epoch 74/100\n",
      "1666/1666 - 48s - loss: 0.4909 - accuracy: 0.7793\n",
      "Epoch 75/100\n",
      "1666/1666 - 48s - loss: 0.4902 - accuracy: 0.7791\n",
      "Epoch 76/100\n",
      "1666/1666 - 52s - loss: 0.4904 - accuracy: 0.7794\n",
      "Epoch 77/100\n",
      "1666/1666 - 48s - loss: 0.4902 - accuracy: 0.7797\n",
      "Epoch 78/100\n",
      "1666/1666 - 48s - loss: 0.4894 - accuracy: 0.7811\n",
      "Epoch 79/100\n",
      "1666/1666 - 48s - loss: 0.4882 - accuracy: 0.7812\n",
      "Epoch 80/100\n",
      "1666/1666 - 48s - loss: 0.4883 - accuracy: 0.7808\n",
      "Epoch 81/100\n",
      "1666/1666 - 48s - loss: 0.4877 - accuracy: 0.7818\n",
      "Epoch 82/100\n",
      "1666/1666 - 48s - loss: 0.4873 - accuracy: 0.7803\n",
      "Epoch 83/100\n",
      "1666/1666 - 48s - loss: 0.4869 - accuracy: 0.7829\n",
      "Epoch 84/100\n",
      "1666/1666 - 49s - loss: 0.4867 - accuracy: 0.7810\n",
      "Epoch 85/100\n",
      "1666/1666 - 49s - loss: 0.4856 - accuracy: 0.7829\n",
      "Epoch 86/100\n",
      "1666/1666 - 49s - loss: 0.4861 - accuracy: 0.7830\n",
      "Epoch 87/100\n",
      "1666/1666 - 49s - loss: 0.4861 - accuracy: 0.7819\n",
      "Epoch 88/100\n",
      "1666/1666 - 49s - loss: 0.4857 - accuracy: 0.7819\n",
      "Epoch 89/100\n",
      "1666/1666 - 49s - loss: 0.4851 - accuracy: 0.7831\n",
      "Epoch 90/100\n",
      "1666/1666 - 49s - loss: 0.4841 - accuracy: 0.7842\n",
      "Epoch 91/100\n",
      "1666/1666 - 49s - loss: 0.4844 - accuracy: 0.7833\n",
      "Epoch 92/100\n",
      "1666/1666 - 49s - loss: 0.4837 - accuracy: 0.7828\n",
      "Epoch 93/100\n",
      "1666/1666 - 49s - loss: 0.4831 - accuracy: 0.7828\n",
      "Epoch 94/100\n",
      "1666/1666 - 49s - loss: 0.4836 - accuracy: 0.7835\n",
      "Epoch 95/100\n",
      "1666/1666 - 49s - loss: 0.4830 - accuracy: 0.7836\n",
      "Epoch 96/100\n",
      "1666/1666 - 49s - loss: 0.4824 - accuracy: 0.7837\n",
      "Epoch 97/100\n",
      "1666/1666 - 49s - loss: 0.4822 - accuracy: 0.7835\n",
      "Epoch 98/100\n",
      "1666/1666 - 49s - loss: 0.4816 - accuracy: 0.7847\n",
      "Epoch 99/100\n",
      "1666/1666 - 49s - loss: 0.4813 - accuracy: 0.7844\n",
      "Epoch 100/100\n",
      "1666/1666 - 49s - loss: 0.4810 - accuracy: 0.7848\n",
      "417/417 - 3s - loss: 0.4977 - accuracy: 0.7630\n",
      "Epoch 1/100\n",
      "2082/2082 - 61s - loss: 0.7710 - accuracy: 0.7405\n",
      "Epoch 2/100\n",
      "2082/2082 - 60s - loss: 0.7505 - accuracy: 0.7405\n",
      "Epoch 3/100\n",
      "2082/2082 - 60s - loss: 0.7459 - accuracy: 0.7405\n",
      "Epoch 4/100\n",
      "2082/2082 - 60s - loss: 0.7187 - accuracy: 0.7405\n",
      "Epoch 5/100\n",
      "2082/2082 - 60s - loss: 0.6406 - accuracy: 0.7411\n",
      "Epoch 6/100\n",
      "2082/2082 - 60s - loss: 0.6041 - accuracy: 0.7493\n",
      "Epoch 7/100\n",
      "2082/2082 - 61s - loss: 0.5891 - accuracy: 0.7557\n",
      "Epoch 8/100\n",
      "2082/2082 - 61s - loss: 0.5773 - accuracy: 0.7563\n",
      "Epoch 9/100\n",
      "2082/2082 - 61s - loss: 0.5749 - accuracy: 0.7577\n",
      "Epoch 10/100\n",
      "2082/2082 - 61s - loss: 0.5685 - accuracy: 0.7581\n",
      "Epoch 11/100\n",
      "2082/2082 - 61s - loss: 0.5641 - accuracy: 0.7587\n",
      "Epoch 12/100\n",
      "2082/2082 - 61s - loss: 0.5570 - accuracy: 0.7598\n",
      "Epoch 13/100\n",
      "2082/2082 - 61s - loss: 0.5513 - accuracy: 0.7596\n",
      "Epoch 14/100\n",
      "2082/2082 - 61s - loss: 0.5466 - accuracy: 0.7614\n",
      "Epoch 15/100\n",
      "2082/2082 - 61s - loss: 0.5428 - accuracy: 0.7614\n",
      "Epoch 16/100\n",
      "2082/2082 - 61s - loss: 0.5390 - accuracy: 0.7601\n",
      "Epoch 17/100\n",
      "2082/2082 - 61s - loss: 0.5344 - accuracy: 0.7624\n",
      "Epoch 18/100\n",
      "2082/2082 - 61s - loss: 0.5306 - accuracy: 0.7628\n",
      "Epoch 19/100\n",
      "2082/2082 - 63s - loss: 0.5287 - accuracy: 0.7621\n",
      "Epoch 20/100\n",
      "2082/2082 - 61s - loss: 0.5262 - accuracy: 0.7637\n",
      "Epoch 21/100\n",
      "2082/2082 - 62s - loss: 0.5229 - accuracy: 0.7655\n",
      "Epoch 22/100\n",
      "2082/2082 - 62s - loss: 0.5216 - accuracy: 0.7641\n",
      "Epoch 23/100\n",
      "2082/2082 - 61s - loss: 0.5192 - accuracy: 0.7652\n",
      "Epoch 24/100\n",
      "2082/2082 - 61s - loss: 0.5183 - accuracy: 0.7653\n",
      "Epoch 25/100\n",
      "2082/2082 - 61s - loss: 0.5157 - accuracy: 0.7645\n",
      "Epoch 26/100\n",
      "2082/2082 - 61s - loss: 0.5142 - accuracy: 0.7663\n",
      "Epoch 27/100\n",
      "2082/2082 - 61s - loss: 0.5139 - accuracy: 0.7648\n",
      "Epoch 28/100\n",
      "2082/2082 - 62s - loss: 0.5120 - accuracy: 0.7668\n",
      "Epoch 29/100\n",
      "2082/2082 - 61s - loss: 0.5103 - accuracy: 0.7673\n",
      "Epoch 30/100\n",
      "2082/2082 - 61s - loss: 0.5092 - accuracy: 0.7666\n",
      "Epoch 31/100\n",
      "2082/2082 - 62s - loss: 0.5086 - accuracy: 0.7686\n",
      "Epoch 32/100\n",
      "2082/2082 - 61s - loss: 0.5071 - accuracy: 0.7683\n",
      "Epoch 33/100\n",
      "2082/2082 - 62s - loss: 0.5062 - accuracy: 0.7688\n",
      "Epoch 34/100\n",
      "2082/2082 - 61s - loss: 0.5056 - accuracy: 0.7684\n",
      "Epoch 35/100\n",
      "2082/2082 - 61s - loss: 0.5049 - accuracy: 0.7679\n",
      "Epoch 36/100\n",
      "2082/2082 - 61s - loss: 0.5027 - accuracy: 0.7697\n",
      "Epoch 37/100\n",
      "2082/2082 - 61s - loss: 0.5025 - accuracy: 0.7693\n",
      "Epoch 38/100\n",
      "2082/2082 - 61s - loss: 0.5013 - accuracy: 0.7715\n",
      "Epoch 39/100\n",
      "2082/2082 - 61s - loss: 0.5010 - accuracy: 0.7702\n",
      "Epoch 40/100\n",
      "2082/2082 - 61s - loss: 0.5002 - accuracy: 0.7710\n",
      "Epoch 41/100\n",
      "2082/2082 - 61s - loss: 0.5001 - accuracy: 0.7714\n",
      "Epoch 42/100\n",
      "2082/2082 - 61s - loss: 0.4984 - accuracy: 0.7720\n",
      "Epoch 43/100\n",
      "2082/2082 - 61s - loss: 0.4982 - accuracy: 0.7721\n",
      "Epoch 44/100\n",
      "2082/2082 - 61s - loss: 0.4976 - accuracy: 0.7724\n",
      "Epoch 45/100\n",
      "2082/2082 - 61s - loss: 0.4971 - accuracy: 0.7720\n",
      "Epoch 46/100\n",
      "2082/2082 - 61s - loss: 0.4957 - accuracy: 0.7734\n",
      "Epoch 47/100\n",
      "2082/2082 - 61s - loss: 0.4964 - accuracy: 0.7745\n",
      "Epoch 48/100\n",
      "2082/2082 - 61s - loss: 0.4953 - accuracy: 0.7741\n",
      "Epoch 49/100\n",
      "2082/2082 - 61s - loss: 0.4948 - accuracy: 0.7733\n",
      "Epoch 50/100\n",
      "2082/2082 - 61s - loss: 0.4943 - accuracy: 0.7737\n",
      "Epoch 51/100\n",
      "2082/2082 - 61s - loss: 0.4927 - accuracy: 0.7746\n",
      "Epoch 52/100\n",
      "2082/2082 - 61s - loss: 0.4928 - accuracy: 0.7753\n",
      "Epoch 53/100\n",
      "2082/2082 - 61s - loss: 0.4926 - accuracy: 0.7753\n",
      "Epoch 54/100\n",
      "2082/2082 - 61s - loss: 0.4924 - accuracy: 0.7746\n",
      "Epoch 55/100\n",
      "2082/2082 - 61s - loss: 0.4916 - accuracy: 0.7744\n",
      "Epoch 56/100\n",
      "2082/2082 - 61s - loss: 0.4907 - accuracy: 0.7760\n",
      "Epoch 57/100\n",
      "2082/2082 - 61s - loss: 0.4909 - accuracy: 0.7757\n",
      "Epoch 58/100\n",
      "2082/2082 - 61s - loss: 0.4897 - accuracy: 0.7766\n",
      "Epoch 59/100\n",
      "2082/2082 - 61s - loss: 0.4896 - accuracy: 0.7758\n",
      "Epoch 60/100\n",
      "2082/2082 - 61s - loss: 0.4889 - accuracy: 0.7757\n",
      "Epoch 61/100\n",
      "2082/2082 - 61s - loss: 0.4887 - accuracy: 0.7773\n",
      "Epoch 62/100\n",
      "2082/2082 - 61s - loss: 0.4880 - accuracy: 0.7759\n",
      "Epoch 63/100\n",
      "2082/2082 - 61s - loss: 0.4877 - accuracy: 0.7757\n",
      "Epoch 64/100\n",
      "2082/2082 - 61s - loss: 0.4879 - accuracy: 0.7773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/100\n",
      "2082/2082 - 61s - loss: 0.4868 - accuracy: 0.7768\n",
      "Epoch 66/100\n",
      "2082/2082 - 61s - loss: 0.4862 - accuracy: 0.7770\n",
      "Epoch 67/100\n",
      "2082/2082 - 61s - loss: 0.4864 - accuracy: 0.7766\n",
      "Epoch 68/100\n",
      "2082/2082 - 61s - loss: 0.4856 - accuracy: 0.7778\n",
      "Epoch 69/100\n",
      "2082/2082 - 61s - loss: 0.4858 - accuracy: 0.7772\n",
      "Epoch 70/100\n",
      "2082/2082 - 61s - loss: 0.4837 - accuracy: 0.7772\n",
      "Epoch 71/100\n",
      "2082/2082 - 61s - loss: 0.4846 - accuracy: 0.7785\n",
      "Epoch 72/100\n",
      "2082/2082 - 61s - loss: 0.4831 - accuracy: 0.7787\n",
      "Epoch 73/100\n",
      "2082/2082 - 61s - loss: 0.4834 - accuracy: 0.7790\n",
      "Epoch 74/100\n",
      "2082/2082 - 61s - loss: 0.4832 - accuracy: 0.7787\n",
      "Epoch 75/100\n",
      "2082/2082 - 61s - loss: 0.4825 - accuracy: 0.7792\n",
      "Epoch 76/100\n",
      "2082/2082 - 61s - loss: 0.4821 - accuracy: 0.7800\n",
      "Epoch 77/100\n",
      "2082/2082 - 60s - loss: 0.4812 - accuracy: 0.7796\n",
      "Epoch 78/100\n",
      "2082/2082 - 61s - loss: 0.4820 - accuracy: 0.7793\n",
      "Epoch 79/100\n",
      "2082/2082 - 61s - loss: 0.4811 - accuracy: 0.7794\n",
      "Epoch 80/100\n",
      "2082/2082 - 61s - loss: 0.4807 - accuracy: 0.7797\n",
      "Epoch 81/100\n",
      "2082/2082 - 61s - loss: 0.4812 - accuracy: 0.7792\n",
      "Epoch 82/100\n",
      "2082/2082 - 61s - loss: 0.4803 - accuracy: 0.7799\n",
      "Epoch 83/100\n",
      "2082/2082 - 61s - loss: 0.4794 - accuracy: 0.7796\n",
      "Epoch 84/100\n",
      "2082/2082 - 61s - loss: 0.4792 - accuracy: 0.7810\n",
      "Epoch 85/100\n",
      "2082/2082 - 61s - loss: 0.4799 - accuracy: 0.7807\n",
      "Epoch 86/100\n",
      "2082/2082 - 61s - loss: 0.4788 - accuracy: 0.7805\n",
      "Epoch 87/100\n",
      "2082/2082 - 61s - loss: 0.4781 - accuracy: 0.7813\n",
      "Epoch 88/100\n",
      "2082/2082 - 61s - loss: 0.4776 - accuracy: 0.7817\n",
      "Epoch 89/100\n",
      "2082/2082 - 61s - loss: 0.4778 - accuracy: 0.7815\n",
      "Epoch 90/100\n",
      "2082/2082 - 61s - loss: 0.4772 - accuracy: 0.7805\n",
      "Epoch 91/100\n",
      "2082/2082 - 61s - loss: 0.4773 - accuracy: 0.7803\n",
      "Epoch 92/100\n",
      "2082/2082 - 61s - loss: 0.4762 - accuracy: 0.7811\n",
      "Epoch 93/100\n",
      "2082/2082 - 61s - loss: 0.4767 - accuracy: 0.7818\n",
      "Epoch 94/100\n",
      "2082/2082 - 61s - loss: 0.4761 - accuracy: 0.7817\n",
      "Epoch 95/100\n",
      "2082/2082 - 61s - loss: 0.4760 - accuracy: 0.7818\n",
      "Epoch 96/100\n",
      "2082/2082 - 61s - loss: 0.4752 - accuracy: 0.7830\n",
      "Epoch 97/100\n",
      "2082/2082 - 61s - loss: 0.4744 - accuracy: 0.7821\n",
      "Epoch 98/100\n",
      "2082/2082 - 61s - loss: 0.4746 - accuracy: 0.7834\n",
      "Epoch 99/100\n",
      "2082/2082 - 61s - loss: 0.4746 - accuracy: 0.7827\n",
      "Epoch 100/100\n",
      "2082/2082 - 61s - loss: 0.4741 - accuracy: 0.7824\n",
      "417/417 - 3s - loss: 0.6021 - accuracy: 0.7447\n",
      "Epoch 1/100\n",
      "417/417 - 13s - loss: 0.8593 - accuracy: 0.7157\n",
      "Epoch 2/100\n",
      "417/417 - 12s - loss: 0.7549 - accuracy: 0.7389\n",
      "Epoch 3/100\n",
      "417/417 - 12s - loss: 0.7545 - accuracy: 0.7389\n",
      "Epoch 4/100\n",
      "417/417 - 12s - loss: 0.7541 - accuracy: 0.7389\n",
      "Epoch 5/100\n",
      "417/417 - 12s - loss: 0.7534 - accuracy: 0.7389\n",
      "Epoch 6/100\n",
      "417/417 - 12s - loss: 0.7527 - accuracy: 0.7389\n",
      "Epoch 7/100\n",
      "417/417 - 12s - loss: 0.7516 - accuracy: 0.7389\n",
      "Epoch 8/100\n",
      "417/417 - 12s - loss: 0.7498 - accuracy: 0.7389\n",
      "Epoch 9/100\n",
      "417/417 - 12s - loss: 0.7466 - accuracy: 0.7389\n",
      "Epoch 10/100\n",
      "417/417 - 12s - loss: 0.7429 - accuracy: 0.7389\n",
      "Epoch 11/100\n",
      "417/417 - 12s - loss: 0.7371 - accuracy: 0.7389\n",
      "Epoch 12/100\n",
      "417/417 - 12s - loss: 0.7279 - accuracy: 0.7389\n",
      "Epoch 13/100\n",
      "417/417 - 12s - loss: 0.7140 - accuracy: 0.7389\n",
      "Epoch 14/100\n",
      "417/417 - 12s - loss: 0.6940 - accuracy: 0.7389\n",
      "Epoch 15/100\n",
      "417/417 - 12s - loss: 0.6708 - accuracy: 0.7389\n",
      "Epoch 16/100\n",
      "417/417 - 12s - loss: 0.6507 - accuracy: 0.7389\n",
      "Epoch 17/100\n",
      "417/417 - 12s - loss: 0.6390 - accuracy: 0.7394\n",
      "Epoch 18/100\n",
      "417/417 - 12s - loss: 0.6291 - accuracy: 0.7417\n",
      "Epoch 19/100\n",
      "417/417 - 12s - loss: 0.6258 - accuracy: 0.7431\n",
      "Epoch 20/100\n",
      "417/417 - 12s - loss: 0.6183 - accuracy: 0.7443\n",
      "Epoch 21/100\n",
      "417/417 - 12s - loss: 0.6137 - accuracy: 0.7465\n",
      "Epoch 22/100\n",
      "417/417 - 12s - loss: 0.6128 - accuracy: 0.7466\n",
      "Epoch 23/100\n",
      "417/417 - 12s - loss: 0.6067 - accuracy: 0.7494\n",
      "Epoch 24/100\n",
      "417/417 - 12s - loss: 0.6014 - accuracy: 0.7510\n",
      "Epoch 25/100\n",
      "417/417 - 12s - loss: 0.5984 - accuracy: 0.7536\n",
      "Epoch 26/100\n",
      "417/417 - 12s - loss: 0.5938 - accuracy: 0.7536\n",
      "Epoch 27/100\n",
      "417/417 - 12s - loss: 0.5925 - accuracy: 0.7504\n",
      "Epoch 28/100\n",
      "417/417 - 12s - loss: 0.5866 - accuracy: 0.7525\n",
      "Epoch 29/100\n",
      "417/417 - 12s - loss: 0.5829 - accuracy: 0.7553\n",
      "Epoch 30/100\n",
      "417/417 - 12s - loss: 0.5772 - accuracy: 0.7559\n",
      "Epoch 31/100\n",
      "417/417 - 12s - loss: 0.5765 - accuracy: 0.7593\n",
      "Epoch 32/100\n",
      "417/417 - 12s - loss: 0.5726 - accuracy: 0.7618\n",
      "Epoch 33/100\n",
      "417/417 - 12s - loss: 0.5689 - accuracy: 0.7645\n",
      "Epoch 34/100\n",
      "417/417 - 12s - loss: 0.5723 - accuracy: 0.7566\n",
      "Epoch 35/100\n",
      "417/417 - 12s - loss: 0.5655 - accuracy: 0.7651\n",
      "Epoch 36/100\n",
      "417/417 - 12s - loss: 0.5644 - accuracy: 0.7633\n",
      "Epoch 37/100\n",
      "417/417 - 12s - loss: 0.5609 - accuracy: 0.7674\n",
      "Epoch 38/100\n",
      "417/417 - 12s - loss: 0.5604 - accuracy: 0.7659\n",
      "Epoch 39/100\n",
      "417/417 - 12s - loss: 0.5578 - accuracy: 0.7685\n",
      "Epoch 40/100\n",
      "417/417 - 12s - loss: 0.5551 - accuracy: 0.7682\n",
      "Epoch 41/100\n",
      "417/417 - 12s - loss: 0.5534 - accuracy: 0.7689\n",
      "Epoch 42/100\n",
      "417/417 - 12s - loss: 0.5537 - accuracy: 0.7663\n",
      "Epoch 43/100\n",
      "417/417 - 12s - loss: 0.5499 - accuracy: 0.7726\n",
      "Epoch 44/100\n",
      "417/417 - 12s - loss: 0.5474 - accuracy: 0.7725\n",
      "Epoch 45/100\n",
      "417/417 - 12s - loss: 0.5431 - accuracy: 0.7705\n",
      "Epoch 46/100\n",
      "417/417 - 12s - loss: 0.5411 - accuracy: 0.7725\n",
      "Epoch 47/100\n",
      "417/417 - 12s - loss: 0.5394 - accuracy: 0.7742\n",
      "Epoch 48/100\n",
      "417/417 - 12s - loss: 0.5403 - accuracy: 0.7716\n",
      "Epoch 49/100\n",
      "417/417 - 12s - loss: 0.5349 - accuracy: 0.7747\n",
      "Epoch 50/100\n",
      "417/417 - 12s - loss: 0.5351 - accuracy: 0.7731\n",
      "Epoch 51/100\n",
      "417/417 - 12s - loss: 0.5330 - accuracy: 0.7759\n",
      "Epoch 52/100\n",
      "417/417 - 12s - loss: 0.5298 - accuracy: 0.7744\n",
      "Epoch 53/100\n",
      "417/417 - 12s - loss: 0.5297 - accuracy: 0.7742\n",
      "Epoch 54/100\n",
      "417/417 - 12s - loss: 0.5265 - accuracy: 0.7756\n",
      "Epoch 55/100\n",
      "417/417 - 12s - loss: 0.5276 - accuracy: 0.7764\n",
      "Epoch 56/100\n",
      "417/417 - 12s - loss: 0.5231 - accuracy: 0.7792\n",
      "Epoch 57/100\n",
      "417/417 - 12s - loss: 0.5228 - accuracy: 0.7771\n",
      "Epoch 58/100\n",
      "417/417 - 12s - loss: 0.5222 - accuracy: 0.7768\n",
      "Epoch 59/100\n",
      "417/417 - 12s - loss: 0.5216 - accuracy: 0.7795\n",
      "Epoch 60/100\n",
      "417/417 - 12s - loss: 0.5184 - accuracy: 0.7802\n",
      "Epoch 61/100\n",
      "417/417 - 12s - loss: 0.5191 - accuracy: 0.7762\n",
      "Epoch 62/100\n",
      "417/417 - 12s - loss: 0.5166 - accuracy: 0.7806\n",
      "Epoch 63/100\n",
      "417/417 - 12s - loss: 0.5126 - accuracy: 0.7822\n",
      "Epoch 64/100\n",
      "417/417 - 12s - loss: 0.5149 - accuracy: 0.7787\n",
      "Epoch 65/100\n",
      "417/417 - 12s - loss: 0.5141 - accuracy: 0.7822\n",
      "Epoch 66/100\n",
      "417/417 - 12s - loss: 0.5098 - accuracy: 0.7809\n",
      "Epoch 67/100\n",
      "417/417 - 12s - loss: 0.5107 - accuracy: 0.7825\n",
      "Epoch 68/100\n",
      "417/417 - 12s - loss: 0.5111 - accuracy: 0.7793\n",
      "Epoch 69/100\n",
      "417/417 - 12s - loss: 0.5097 - accuracy: 0.7811\n",
      "Epoch 70/100\n",
      "417/417 - 12s - loss: 0.5067 - accuracy: 0.7829\n",
      "Epoch 71/100\n",
      "417/417 - 12s - loss: 0.5053 - accuracy: 0.7836\n",
      "Epoch 72/100\n",
      "417/417 - 12s - loss: 0.5065 - accuracy: 0.7823\n",
      "Epoch 73/100\n",
      "417/417 - 12s - loss: 0.5046 - accuracy: 0.7826\n",
      "Epoch 74/100\n",
      "417/417 - 12s - loss: 0.5041 - accuracy: 0.7821\n",
      "Epoch 75/100\n",
      "417/417 - 12s - loss: 0.5028 - accuracy: 0.7847\n",
      "Epoch 76/100\n",
      "417/417 - 12s - loss: 0.5021 - accuracy: 0.7810\n",
      "Epoch 77/100\n",
      "417/417 - 12s - loss: 0.5000 - accuracy: 0.7846\n",
      "Epoch 78/100\n",
      "417/417 - 12s - loss: 0.5008 - accuracy: 0.7844\n",
      "Epoch 79/100\n",
      "417/417 - 12s - loss: 0.5000 - accuracy: 0.7852\n",
      "Epoch 80/100\n",
      "417/417 - 12s - loss: 0.4986 - accuracy: 0.7862\n",
      "Epoch 81/100\n",
      "417/417 - 12s - loss: 0.4977 - accuracy: 0.7870\n",
      "Epoch 82/100\n",
      "417/417 - 12s - loss: 0.4972 - accuracy: 0.7858\n",
      "Epoch 83/100\n",
      "417/417 - 12s - loss: 0.4977 - accuracy: 0.7843\n",
      "Epoch 84/100\n",
      "417/417 - 12s - loss: 0.4977 - accuracy: 0.7878\n",
      "Epoch 85/100\n",
      "417/417 - 12s - loss: 0.4936 - accuracy: 0.7864\n",
      "Epoch 86/100\n",
      "417/417 - 12s - loss: 0.4955 - accuracy: 0.7853\n",
      "Epoch 87/100\n",
      "417/417 - 12s - loss: 0.4951 - accuracy: 0.7849\n",
      "Epoch 88/100\n",
      "417/417 - 12s - loss: 0.4939 - accuracy: 0.7828\n",
      "Epoch 89/100\n",
      "417/417 - 12s - loss: 0.4931 - accuracy: 0.7873\n",
      "Epoch 90/100\n",
      "417/417 - 12s - loss: 0.4933 - accuracy: 0.7872\n",
      "Epoch 91/100\n",
      "417/417 - 12s - loss: 0.4906 - accuracy: 0.7885\n",
      "Epoch 92/100\n",
      "417/417 - 12s - loss: 0.4896 - accuracy: 0.7882\n",
      "Epoch 93/100\n",
      "417/417 - 12s - loss: 0.4921 - accuracy: 0.7863\n",
      "Epoch 94/100\n",
      "417/417 - 12s - loss: 0.4920 - accuracy: 0.7861\n",
      "Epoch 95/100\n",
      "417/417 - 12s - loss: 0.4879 - accuracy: 0.7907\n",
      "Epoch 96/100\n",
      "417/417 - 12s - loss: 0.4895 - accuracy: 0.7893\n",
      "Epoch 97/100\n",
      "417/417 - 12s - loss: 0.4901 - accuracy: 0.7871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/100\n",
      "417/417 - 12s - loss: 0.4850 - accuracy: 0.7915\n",
      "Epoch 99/100\n",
      "417/417 - 12s - loss: 0.4840 - accuracy: 0.7901\n",
      "Epoch 100/100\n",
      "417/417 - 12s - loss: 0.4854 - accuracy: 0.7903\n",
      "417/417 - 3s - loss: 0.5534 - accuracy: 0.7736\n",
      "Epoch 1/100\n",
      "833/833 - 25s - loss: 0.8139 - accuracy: 0.7218\n",
      "Epoch 2/100\n",
      "833/833 - 27s - loss: 0.7533 - accuracy: 0.7396\n",
      "Epoch 3/100\n",
      "833/833 - 25s - loss: 0.7527 - accuracy: 0.7396\n",
      "Epoch 4/100\n",
      "833/833 - 25s - loss: 0.7520 - accuracy: 0.7396\n",
      "Epoch 5/100\n",
      "833/833 - 24s - loss: 0.7504 - accuracy: 0.7396\n",
      "Epoch 6/100\n",
      "833/833 - 24s - loss: 0.7476 - accuracy: 0.7396\n",
      "Epoch 7/100\n",
      "833/833 - 24s - loss: 0.7414 - accuracy: 0.7396\n",
      "Epoch 8/100\n",
      "833/833 - 24s - loss: 0.7252 - accuracy: 0.7396\n",
      "Epoch 9/100\n",
      "833/833 - 24s - loss: 0.6857 - accuracy: 0.7396\n",
      "Epoch 10/100\n",
      "833/833 - 24s - loss: 0.6348 - accuracy: 0.7401\n",
      "Epoch 11/100\n",
      "833/833 - 24s - loss: 0.6158 - accuracy: 0.7428\n",
      "Epoch 12/100\n",
      "833/833 - 24s - loss: 0.6087 - accuracy: 0.7457\n",
      "Epoch 13/100\n",
      "833/833 - 24s - loss: 0.6031 - accuracy: 0.7475\n",
      "Epoch 14/100\n",
      "833/833 - 24s - loss: 0.5980 - accuracy: 0.7522\n",
      "Epoch 15/100\n",
      "833/833 - 25s - loss: 0.5951 - accuracy: 0.7526\n",
      "Epoch 16/100\n",
      "833/833 - 25s - loss: 0.5917 - accuracy: 0.7562\n",
      "Epoch 17/100\n",
      "833/833 - 25s - loss: 0.5870 - accuracy: 0.7585\n",
      "Epoch 18/100\n",
      "833/833 - 25s - loss: 0.5843 - accuracy: 0.7603\n",
      "Epoch 19/100\n",
      "833/833 - 25s - loss: 0.5813 - accuracy: 0.7597\n",
      "Epoch 20/100\n",
      "833/833 - 25s - loss: 0.5775 - accuracy: 0.7625\n",
      "Epoch 21/100\n",
      "833/833 - 25s - loss: 0.5769 - accuracy: 0.7600\n",
      "Epoch 22/100\n",
      "833/833 - 28s - loss: 0.5726 - accuracy: 0.7629\n",
      "Epoch 23/100\n",
      "833/833 - 25s - loss: 0.5695 - accuracy: 0.7655\n",
      "Epoch 24/100\n",
      "833/833 - 25s - loss: 0.5656 - accuracy: 0.7666\n",
      "Epoch 25/100\n",
      "833/833 - 25s - loss: 0.5626 - accuracy: 0.7677\n",
      "Epoch 26/100\n",
      "833/833 - 25s - loss: 0.5591 - accuracy: 0.7670\n",
      "Epoch 27/100\n",
      "833/833 - 25s - loss: 0.5562 - accuracy: 0.7668\n",
      "Epoch 28/100\n",
      "833/833 - 25s - loss: 0.5509 - accuracy: 0.7668\n",
      "Epoch 29/100\n",
      "833/833 - 25s - loss: 0.5480 - accuracy: 0.7677\n",
      "Epoch 30/100\n",
      "833/833 - 25s - loss: 0.5422 - accuracy: 0.7707\n",
      "Epoch 31/100\n",
      "833/833 - 25s - loss: 0.5426 - accuracy: 0.7679\n",
      "Epoch 32/100\n",
      "833/833 - 25s - loss: 0.5387 - accuracy: 0.7712\n",
      "Epoch 33/100\n",
      "833/833 - 25s - loss: 0.5358 - accuracy: 0.7721\n",
      "Epoch 34/100\n",
      "833/833 - 25s - loss: 0.5335 - accuracy: 0.7755\n",
      "Epoch 35/100\n",
      "833/833 - 25s - loss: 0.5324 - accuracy: 0.7727\n",
      "Epoch 36/100\n",
      "833/833 - 24s - loss: 0.5295 - accuracy: 0.7727\n",
      "Epoch 37/100\n",
      "833/833 - 24s - loss: 0.5277 - accuracy: 0.7746\n",
      "Epoch 38/100\n",
      "833/833 - 24s - loss: 0.5269 - accuracy: 0.7733\n",
      "Epoch 39/100\n",
      "833/833 - 24s - loss: 0.5241 - accuracy: 0.7738\n",
      "Epoch 40/100\n",
      "833/833 - 24s - loss: 0.5217 - accuracy: 0.7756\n",
      "Epoch 41/100\n",
      "833/833 - 24s - loss: 0.5204 - accuracy: 0.7769\n",
      "Epoch 42/100\n",
      "833/833 - 24s - loss: 0.5189 - accuracy: 0.7741\n",
      "Epoch 43/100\n",
      "833/833 - 24s - loss: 0.5171 - accuracy: 0.7767\n",
      "Epoch 44/100\n",
      "833/833 - 24s - loss: 0.5166 - accuracy: 0.7778\n",
      "Epoch 45/100\n",
      "833/833 - 24s - loss: 0.5162 - accuracy: 0.7777\n",
      "Epoch 46/100\n",
      "833/833 - 24s - loss: 0.5135 - accuracy: 0.7773\n",
      "Epoch 47/100\n",
      "833/833 - 24s - loss: 0.5123 - accuracy: 0.7798\n",
      "Epoch 48/100\n",
      "833/833 - 24s - loss: 0.5102 - accuracy: 0.7790\n",
      "Epoch 49/100\n",
      "833/833 - 24s - loss: 0.5105 - accuracy: 0.7787\n",
      "Epoch 50/100\n",
      "833/833 - 24s - loss: 0.5088 - accuracy: 0.7800\n",
      "Epoch 51/100\n",
      "833/833 - 24s - loss: 0.5063 - accuracy: 0.7807\n",
      "Epoch 52/100\n",
      "833/833 - 24s - loss: 0.5065 - accuracy: 0.7812\n",
      "Epoch 53/100\n",
      "833/833 - 24s - loss: 0.5064 - accuracy: 0.7814\n",
      "Epoch 54/100\n",
      "833/833 - 24s - loss: 0.5054 - accuracy: 0.7808\n",
      "Epoch 55/100\n",
      "833/833 - 24s - loss: 0.5043 - accuracy: 0.7807\n",
      "Epoch 56/100\n",
      "833/833 - 24s - loss: 0.5038 - accuracy: 0.7816\n",
      "Epoch 57/100\n",
      "833/833 - 24s - loss: 0.5014 - accuracy: 0.7835\n",
      "Epoch 58/100\n",
      "833/833 - 24s - loss: 0.5016 - accuracy: 0.7812\n",
      "Epoch 59/100\n",
      "833/833 - 24s - loss: 0.5014 - accuracy: 0.7823\n",
      "Epoch 60/100\n",
      "833/833 - 24s - loss: 0.4996 - accuracy: 0.7828\n",
      "Epoch 61/100\n",
      "833/833 - 24s - loss: 0.5009 - accuracy: 0.7836\n",
      "Epoch 62/100\n",
      "833/833 - 24s - loss: 0.4984 - accuracy: 0.7831\n",
      "Epoch 63/100\n",
      "833/833 - 24s - loss: 0.4977 - accuracy: 0.7841\n",
      "Epoch 64/100\n",
      "833/833 - 24s - loss: 0.4981 - accuracy: 0.7838\n",
      "Epoch 65/100\n",
      "833/833 - 24s - loss: 0.4981 - accuracy: 0.7847\n",
      "Epoch 66/100\n",
      "833/833 - 24s - loss: 0.4962 - accuracy: 0.7840\n",
      "Epoch 67/100\n",
      "833/833 - 24s - loss: 0.4952 - accuracy: 0.7866\n",
      "Epoch 68/100\n",
      "833/833 - 24s - loss: 0.4940 - accuracy: 0.7853\n",
      "Epoch 69/100\n",
      "833/833 - 24s - loss: 0.4931 - accuracy: 0.7858\n",
      "Epoch 70/100\n",
      "833/833 - 24s - loss: 0.4945 - accuracy: 0.7842\n",
      "Epoch 71/100\n",
      "833/833 - 24s - loss: 0.4923 - accuracy: 0.7863\n",
      "Epoch 72/100\n",
      "833/833 - 24s - loss: 0.4921 - accuracy: 0.7859\n",
      "Epoch 73/100\n",
      "833/833 - 24s - loss: 0.4912 - accuracy: 0.7858\n",
      "Epoch 74/100\n",
      "833/833 - 24s - loss: 0.4900 - accuracy: 0.7858\n",
      "Epoch 75/100\n",
      "833/833 - 24s - loss: 0.4892 - accuracy: 0.7865\n",
      "Epoch 76/100\n",
      "833/833 - 24s - loss: 0.4898 - accuracy: 0.7867\n",
      "Epoch 77/100\n",
      "833/833 - 24s - loss: 0.4876 - accuracy: 0.7894\n",
      "Epoch 78/100\n",
      "833/833 - 24s - loss: 0.4868 - accuracy: 0.7903\n",
      "Epoch 79/100\n",
      "833/833 - 24s - loss: 0.4868 - accuracy: 0.7910\n",
      "Epoch 80/100\n",
      "833/833 - 24s - loss: 0.4864 - accuracy: 0.7873\n",
      "Epoch 81/100\n",
      "833/833 - 24s - loss: 0.4861 - accuracy: 0.7894\n",
      "Epoch 82/100\n",
      "833/833 - 24s - loss: 0.4859 - accuracy: 0.7888\n",
      "Epoch 83/100\n",
      "833/833 - 24s - loss: 0.4841 - accuracy: 0.7898\n",
      "Epoch 84/100\n",
      "833/833 - 24s - loss: 0.4835 - accuracy: 0.7908\n",
      "Epoch 85/100\n",
      "833/833 - 24s - loss: 0.4829 - accuracy: 0.7898\n",
      "Epoch 86/100\n",
      "833/833 - 24s - loss: 0.4818 - accuracy: 0.7901\n",
      "Epoch 87/100\n",
      "833/833 - 24s - loss: 0.4819 - accuracy: 0.7922\n",
      "Epoch 88/100\n",
      "833/833 - 24s - loss: 0.4819 - accuracy: 0.7905\n",
      "Epoch 89/100\n",
      "833/833 - 24s - loss: 0.4796 - accuracy: 0.7921\n",
      "Epoch 90/100\n",
      "833/833 - 24s - loss: 0.4798 - accuracy: 0.7932\n",
      "Epoch 91/100\n",
      "833/833 - 24s - loss: 0.4800 - accuracy: 0.7905\n",
      "Epoch 92/100\n",
      "833/833 - 24s - loss: 0.4769 - accuracy: 0.7927\n",
      "Epoch 93/100\n",
      "833/833 - 24s - loss: 0.4773 - accuracy: 0.7929\n",
      "Epoch 94/100\n",
      "833/833 - 24s - loss: 0.4766 - accuracy: 0.7928\n",
      "Epoch 95/100\n",
      "833/833 - 24s - loss: 0.4751 - accuracy: 0.7933\n",
      "Epoch 96/100\n",
      "833/833 - 24s - loss: 0.4778 - accuracy: 0.7939\n",
      "Epoch 97/100\n",
      "833/833 - 24s - loss: 0.4765 - accuracy: 0.7932\n",
      "Epoch 98/100\n",
      "833/833 - 24s - loss: 0.4735 - accuracy: 0.7933\n",
      "Epoch 99/100\n",
      "833/833 - 24s - loss: 0.4738 - accuracy: 0.7934\n",
      "Epoch 100/100\n",
      "833/833 - 24s - loss: 0.4724 - accuracy: 0.7961\n",
      "417/417 - 3s - loss: 0.5114 - accuracy: 0.7748\n",
      "Epoch 1/100\n",
      "1249/1249 - 33s - loss: 0.7782 - accuracy: 0.7435\n",
      "Epoch 2/100\n",
      "1249/1249 - 33s - loss: 0.7443 - accuracy: 0.7440\n",
      "Epoch 3/100\n",
      "1249/1249 - 33s - loss: 0.7408 - accuracy: 0.7440\n",
      "Epoch 4/100\n",
      "1249/1249 - 33s - loss: 0.7281 - accuracy: 0.7440\n",
      "Epoch 5/100\n",
      "1249/1249 - 33s - loss: 0.6882 - accuracy: 0.7440\n",
      "Epoch 6/100\n",
      "1249/1249 - 33s - loss: 0.6335 - accuracy: 0.7447\n",
      "Epoch 7/100\n",
      "1249/1249 - 33s - loss: 0.6073 - accuracy: 0.7475\n",
      "Epoch 8/100\n",
      "1249/1249 - 33s - loss: 0.5953 - accuracy: 0.7518\n",
      "Epoch 9/100\n",
      "1249/1249 - 33s - loss: 0.5882 - accuracy: 0.7556\n",
      "Epoch 10/100\n",
      "1249/1249 - 33s - loss: 0.5816 - accuracy: 0.7591\n",
      "Epoch 11/100\n",
      "1249/1249 - 33s - loss: 0.5751 - accuracy: 0.7609\n",
      "Epoch 12/100\n",
      "1249/1249 - 32s - loss: 0.5689 - accuracy: 0.7624\n",
      "Epoch 13/100\n",
      "1249/1249 - 33s - loss: 0.5638 - accuracy: 0.7646\n",
      "Epoch 14/100\n",
      "1249/1249 - 35s - loss: 0.5594 - accuracy: 0.7655\n",
      "Epoch 15/100\n",
      "1249/1249 - 33s - loss: 0.5552 - accuracy: 0.7667\n",
      "Epoch 16/100\n",
      "1249/1249 - 33s - loss: 0.5523 - accuracy: 0.7682\n",
      "Epoch 17/100\n",
      "1249/1249 - 33s - loss: 0.5500 - accuracy: 0.7677\n",
      "Epoch 18/100\n",
      "1249/1249 - 33s - loss: 0.5474 - accuracy: 0.7678\n",
      "Epoch 19/100\n",
      "1249/1249 - 35s - loss: 0.5447 - accuracy: 0.7707\n",
      "Epoch 20/100\n",
      "1249/1249 - 33s - loss: 0.5419 - accuracy: 0.7711\n",
      "Epoch 21/100\n",
      "1249/1249 - 33s - loss: 0.5411 - accuracy: 0.7710\n",
      "Epoch 22/100\n",
      "1249/1249 - 33s - loss: 0.5380 - accuracy: 0.7724\n",
      "Epoch 23/100\n",
      "1249/1249 - 33s - loss: 0.5359 - accuracy: 0.7721\n",
      "Epoch 24/100\n",
      "1249/1249 - 33s - loss: 0.5346 - accuracy: 0.7714\n",
      "Epoch 25/100\n",
      "1249/1249 - 33s - loss: 0.5327 - accuracy: 0.7751\n",
      "Epoch 26/100\n",
      "1249/1249 - 33s - loss: 0.5306 - accuracy: 0.7730\n",
      "Epoch 27/100\n",
      "1249/1249 - 33s - loss: 0.5287 - accuracy: 0.7738\n",
      "Epoch 28/100\n",
      "1249/1249 - 33s - loss: 0.5262 - accuracy: 0.7747\n",
      "Epoch 29/100\n",
      "1249/1249 - 33s - loss: 0.5242 - accuracy: 0.7751\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1249/1249 - 33s - loss: 0.5241 - accuracy: 0.7733\n",
      "Epoch 31/100\n",
      "1249/1249 - 35s - loss: 0.5221 - accuracy: 0.7751\n",
      "Epoch 32/100\n",
      "1249/1249 - 33s - loss: 0.5213 - accuracy: 0.7754\n",
      "Epoch 33/100\n",
      "1249/1249 - 33s - loss: 0.5185 - accuracy: 0.7777\n",
      "Epoch 34/100\n",
      "1249/1249 - 33s - loss: 0.5175 - accuracy: 0.7768\n",
      "Epoch 35/100\n",
      "1249/1249 - 33s - loss: 0.5176 - accuracy: 0.7760\n",
      "Epoch 36/100\n",
      "1249/1249 - 33s - loss: 0.5149 - accuracy: 0.7773\n",
      "Epoch 37/100\n",
      "1249/1249 - 33s - loss: 0.5157 - accuracy: 0.7773\n",
      "Epoch 38/100\n",
      "1249/1249 - 33s - loss: 0.5140 - accuracy: 0.7789\n",
      "Epoch 39/100\n",
      "1249/1249 - 33s - loss: 0.5110 - accuracy: 0.7792\n",
      "Epoch 40/100\n",
      "1249/1249 - 33s - loss: 0.5117 - accuracy: 0.7781\n",
      "Epoch 41/100\n",
      "1249/1249 - 33s - loss: 0.5112 - accuracy: 0.7770\n",
      "Epoch 42/100\n",
      "1249/1249 - 33s - loss: 0.5100 - accuracy: 0.7767\n",
      "Epoch 43/100\n",
      "1249/1249 - 33s - loss: 0.5094 - accuracy: 0.7798\n",
      "Epoch 44/100\n",
      "1249/1249 - 33s - loss: 0.5074 - accuracy: 0.7787\n",
      "Epoch 45/100\n",
      "1249/1249 - 33s - loss: 0.5070 - accuracy: 0.7800\n",
      "Epoch 46/100\n",
      "1249/1249 - 33s - loss: 0.5065 - accuracy: 0.7785\n",
      "Epoch 47/100\n",
      "1249/1249 - 33s - loss: 0.5068 - accuracy: 0.7783\n",
      "Epoch 48/100\n",
      "1249/1249 - 33s - loss: 0.5047 - accuracy: 0.7781\n",
      "Epoch 49/100\n",
      "1249/1249 - 33s - loss: 0.5042 - accuracy: 0.7799\n",
      "Epoch 50/100\n",
      "1249/1249 - 33s - loss: 0.5032 - accuracy: 0.7802\n",
      "Epoch 51/100\n",
      "1249/1249 - 33s - loss: 0.5031 - accuracy: 0.7812\n",
      "Epoch 52/100\n",
      "1249/1249 - 33s - loss: 0.5028 - accuracy: 0.7815\n",
      "Epoch 53/100\n",
      "1249/1249 - 33s - loss: 0.5016 - accuracy: 0.7791\n",
      "Epoch 54/100\n",
      "1249/1249 - 33s - loss: 0.5009 - accuracy: 0.7818\n",
      "Epoch 55/100\n",
      "1249/1249 - 33s - loss: 0.4999 - accuracy: 0.7804\n",
      "Epoch 56/100\n",
      "1249/1249 - 33s - loss: 0.4994 - accuracy: 0.7818\n",
      "Epoch 57/100\n",
      "1249/1249 - 33s - loss: 0.4989 - accuracy: 0.7820\n",
      "Epoch 58/100\n",
      "1249/1249 - 33s - loss: 0.4992 - accuracy: 0.7812\n",
      "Epoch 59/100\n",
      "1249/1249 - 33s - loss: 0.4982 - accuracy: 0.7819\n",
      "Epoch 60/100\n",
      "1249/1249 - 33s - loss: 0.4972 - accuracy: 0.7820\n",
      "Epoch 61/100\n",
      "1249/1249 - 33s - loss: 0.4969 - accuracy: 0.7811\n",
      "Epoch 62/100\n",
      "1249/1249 - 33s - loss: 0.4970 - accuracy: 0.7818\n",
      "Epoch 63/100\n",
      "1249/1249 - 33s - loss: 0.4960 - accuracy: 0.7821\n",
      "Epoch 64/100\n",
      "1249/1249 - 33s - loss: 0.4952 - accuracy: 0.7821\n",
      "Epoch 65/100\n",
      "1249/1249 - 33s - loss: 0.4953 - accuracy: 0.7819\n",
      "Epoch 66/100\n",
      "1249/1249 - 33s - loss: 0.4959 - accuracy: 0.7829\n",
      "Epoch 67/100\n",
      "1249/1249 - 33s - loss: 0.4939 - accuracy: 0.7826\n",
      "Epoch 68/100\n",
      "1249/1249 - 33s - loss: 0.4944 - accuracy: 0.7838\n",
      "Epoch 69/100\n",
      "1249/1249 - 33s - loss: 0.4927 - accuracy: 0.7842\n",
      "Epoch 70/100\n",
      "1249/1249 - 33s - loss: 0.4926 - accuracy: 0.7820\n",
      "Epoch 71/100\n",
      "1249/1249 - 33s - loss: 0.4918 - accuracy: 0.7821\n",
      "Epoch 72/100\n",
      "1249/1249 - 33s - loss: 0.4921 - accuracy: 0.7843\n",
      "Epoch 73/100\n",
      "1249/1249 - 33s - loss: 0.4920 - accuracy: 0.7827\n",
      "Epoch 74/100\n",
      "1249/1249 - 33s - loss: 0.4913 - accuracy: 0.7831\n",
      "Epoch 75/100\n",
      "1249/1249 - 34s - loss: 0.4910 - accuracy: 0.7853\n",
      "Epoch 76/100\n",
      "1249/1249 - 33s - loss: 0.4906 - accuracy: 0.7831\n",
      "Epoch 77/100\n",
      "1249/1249 - 36s - loss: 0.4906 - accuracy: 0.7838\n",
      "Epoch 78/100\n",
      "1249/1249 - 33s - loss: 0.4904 - accuracy: 0.7826\n",
      "Epoch 79/100\n",
      "1249/1249 - 33s - loss: 0.4893 - accuracy: 0.7844\n",
      "Epoch 80/100\n",
      "1249/1249 - 33s - loss: 0.4881 - accuracy: 0.7836\n",
      "Epoch 81/100\n",
      "1249/1249 - 33s - loss: 0.4886 - accuracy: 0.7841\n",
      "Epoch 82/100\n",
      "1249/1249 - 33s - loss: 0.4886 - accuracy: 0.7847\n",
      "Epoch 83/100\n",
      "1249/1249 - 33s - loss: 0.4884 - accuracy: 0.7836\n",
      "Epoch 84/100\n",
      "1249/1249 - 33s - loss: 0.4881 - accuracy: 0.7872\n",
      "Epoch 85/100\n",
      "1249/1249 - 33s - loss: 0.4881 - accuracy: 0.7848\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00085: early stopping\n",
      "417/417 - 3s - loss: 0.5396 - accuracy: 0.7614\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "2082/2082 - 61s - loss: 0.7718 - accuracy: 0.7387\n",
      "Epoch 2/100\n",
      "2082/2082 - 60s - loss: 0.7484 - accuracy: 0.7405\n",
      "Epoch 3/100\n",
      "2082/2082 - 60s - loss: 0.7297 - accuracy: 0.7405\n",
      "Epoch 4/100\n",
      "2082/2082 - 60s - loss: 0.6628 - accuracy: 0.7408\n",
      "Epoch 5/100\n",
      "2082/2082 - 59s - loss: 0.6131 - accuracy: 0.7451\n",
      "Epoch 6/100\n",
      "2082/2082 - 59s - loss: 0.5990 - accuracy: 0.7509\n",
      "Epoch 7/100\n",
      "2082/2082 - 60s - loss: 0.5896 - accuracy: 0.7545\n",
      "Epoch 8/100\n",
      "2082/2082 - 59s - loss: 0.5857 - accuracy: 0.7550\n",
      "Epoch 9/100\n",
      "2082/2082 - 60s - loss: 0.5804 - accuracy: 0.7567\n",
      "Epoch 10/100\n",
      "2082/2082 - 60s - loss: 0.5757 - accuracy: 0.7575\n",
      "Epoch 11/100\n",
      "2082/2082 - 60s - loss: 0.5716 - accuracy: 0.7582\n",
      "Epoch 12/100\n",
      "2082/2082 - 60s - loss: 0.5656 - accuracy: 0.7588\n",
      "Epoch 13/100\n",
      "2082/2082 - 61s - loss: 0.5605 - accuracy: 0.7595\n",
      "Epoch 14/100\n",
      "2082/2082 - 61s - loss: 0.5554 - accuracy: 0.7612\n",
      "Epoch 15/100\n",
      "2082/2082 - 60s - loss: 0.5524 - accuracy: 0.7610\n",
      "Epoch 16/100\n",
      "2082/2082 - 61s - loss: 0.5495 - accuracy: 0.7619\n",
      "Epoch 17/100\n",
      "2082/2082 - 61s - loss: 0.5484 - accuracy: 0.7620\n",
      "Epoch 18/100\n",
      "2082/2082 - 61s - loss: 0.5450 - accuracy: 0.7625\n",
      "Epoch 19/100\n",
      "2082/2082 - 61s - loss: 0.5428 - accuracy: 0.7638\n",
      "Epoch 20/100\n",
      "2082/2082 - 60s - loss: 0.5381 - accuracy: 0.7635\n",
      "Epoch 21/100\n",
      "2082/2082 - 61s - loss: 0.5366 - accuracy: 0.7652\n",
      "Epoch 22/100\n",
      "2082/2082 - 61s - loss: 0.5342 - accuracy: 0.7641\n",
      "Epoch 23/100\n",
      "2082/2082 - 61s - loss: 0.5305 - accuracy: 0.7655\n",
      "Epoch 24/100\n",
      "2082/2082 - 61s - loss: 0.5280 - accuracy: 0.7651\n",
      "Epoch 25/100\n",
      "2082/2082 - 61s - loss: 0.5254 - accuracy: 0.7652\n",
      "Epoch 26/100\n",
      "2082/2082 - 61s - loss: 0.5235 - accuracy: 0.7648\n",
      "Epoch 27/100\n",
      "2082/2082 - 61s - loss: 0.5224 - accuracy: 0.7658\n",
      "Epoch 28/100\n",
      "2082/2082 - 61s - loss: 0.5195 - accuracy: 0.7678\n",
      "Epoch 29/100\n",
      "2082/2082 - 61s - loss: 0.5186 - accuracy: 0.7663\n",
      "Epoch 30/100\n",
      "2082/2082 - 61s - loss: 0.5176 - accuracy: 0.7669\n",
      "Epoch 31/100\n",
      "2082/2082 - 61s - loss: 0.5159 - accuracy: 0.7673\n",
      "Epoch 32/100\n",
      "2082/2082 - 61s - loss: 0.5146 - accuracy: 0.7669\n",
      "Epoch 33/100\n",
      "2082/2082 - 61s - loss: 0.5141 - accuracy: 0.7665\n",
      "Epoch 34/100\n",
      "2082/2082 - 62s - loss: 0.5120 - accuracy: 0.7677\n",
      "Epoch 35/100\n",
      "2082/2082 - 62s - loss: 0.5120 - accuracy: 0.7669\n",
      "Epoch 36/100\n",
      "2082/2082 - 62s - loss: 0.5102 - accuracy: 0.7690\n",
      "Epoch 37/100\n",
      "2082/2082 - 62s - loss: 0.5089 - accuracy: 0.7675\n",
      "Epoch 38/100\n",
      "2082/2082 - 61s - loss: 0.5087 - accuracy: 0.7693\n",
      "Epoch 39/100\n",
      "2082/2082 - 61s - loss: 0.5078 - accuracy: 0.7677\n",
      "Epoch 40/100\n",
      "2082/2082 - 61s - loss: 0.5060 - accuracy: 0.7697\n",
      "Epoch 41/100\n",
      "2082/2082 - 61s - loss: 0.5056 - accuracy: 0.7702\n",
      "Epoch 42/100\n",
      "2082/2082 - 62s - loss: 0.5043 - accuracy: 0.7694\n",
      "Epoch 43/100\n",
      "2082/2082 - 61s - loss: 0.5042 - accuracy: 0.7704\n",
      "Epoch 44/100\n",
      "2082/2082 - 61s - loss: 0.5032 - accuracy: 0.7709\n",
      "Epoch 45/100\n",
      "2082/2082 - 61s - loss: 0.5025 - accuracy: 0.7702\n",
      "Epoch 46/100\n",
      "2082/2082 - 66s - loss: 0.5017 - accuracy: 0.7710\n",
      "Epoch 47/100\n",
      "2082/2082 - 62s - loss: 0.5002 - accuracy: 0.7711\n",
      "Epoch 48/100\n",
      "2082/2082 - 62s - loss: 0.4998 - accuracy: 0.7718\n",
      "Epoch 49/100\n",
      "2082/2082 - 61s - loss: 0.4993 - accuracy: 0.7721\n",
      "Epoch 50/100\n",
      "2082/2082 - 62s - loss: 0.4983 - accuracy: 0.7733\n",
      "Epoch 51/100\n",
      "2082/2082 - 61s - loss: 0.4979 - accuracy: 0.7727\n",
      "Epoch 52/100\n",
      "2082/2082 - 62s - loss: 0.4969 - accuracy: 0.7715\n",
      "Epoch 53/100\n",
      "2082/2082 - 61s - loss: 0.4970 - accuracy: 0.7729\n",
      "Epoch 54/100\n",
      "2082/2082 - 62s - loss: 0.4962 - accuracy: 0.7714\n",
      "Epoch 55/100\n",
      "2082/2082 - 62s - loss: 0.4962 - accuracy: 0.7727\n",
      "Epoch 56/100\n",
      "2082/2082 - 62s - loss: 0.4949 - accuracy: 0.7731\n",
      "Epoch 57/100\n",
      "2082/2082 - 62s - loss: 0.4948 - accuracy: 0.7731\n",
      "Epoch 58/100\n",
      "2082/2082 - 62s - loss: 0.4944 - accuracy: 0.7726\n",
      "Epoch 59/100\n",
      "2082/2082 - 62s - loss: 0.4927 - accuracy: 0.7744\n",
      "Epoch 60/100\n",
      "2082/2082 - 62s - loss: 0.4933 - accuracy: 0.7742\n",
      "Epoch 61/100\n",
      "2082/2082 - 61s - loss: 0.4929 - accuracy: 0.7750\n",
      "Epoch 62/100\n",
      "2082/2082 - 61s - loss: 0.4927 - accuracy: 0.7740\n",
      "Epoch 63/100\n",
      "2082/2082 - 61s - loss: 0.4920 - accuracy: 0.7764\n",
      "Epoch 64/100\n",
      "2082/2082 - 61s - loss: 0.4910 - accuracy: 0.7762\n",
      "Epoch 65/100\n",
      "2082/2082 - 61s - loss: 0.4910 - accuracy: 0.7752\n",
      "Epoch 66/100\n",
      "2082/2082 - 61s - loss: 0.4902 - accuracy: 0.7758\n",
      "Epoch 67/100\n",
      "2082/2082 - 61s - loss: 0.4904 - accuracy: 0.7762\n",
      "Epoch 68/100\n",
      "2082/2082 - 61s - loss: 0.4904 - accuracy: 0.7763\n",
      "Epoch 69/100\n",
      "2082/2082 - 61s - loss: 0.4895 - accuracy: 0.7773\n",
      "Epoch 70/100\n",
      "2082/2082 - 64s - loss: 0.4892 - accuracy: 0.7771\n",
      "Epoch 71/100\n",
      "2082/2082 - 61s - loss: 0.4883 - accuracy: 0.7777\n",
      "Epoch 72/100\n",
      "2082/2082 - 61s - loss: 0.4883 - accuracy: 0.7777\n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2082/2082 - 61s - loss: 0.4878 - accuracy: 0.7767\n",
      "Epoch 74/100\n",
      "2082/2082 - 61s - loss: 0.4874 - accuracy: 0.7785\n",
      "Epoch 75/100\n",
      "2082/2082 - 61s - loss: 0.4869 - accuracy: 0.7775\n",
      "Epoch 76/100\n",
      "2082/2082 - 61s - loss: 0.4865 - accuracy: 0.7781\n",
      "Epoch 77/100\n",
      "2082/2082 - 61s - loss: 0.4863 - accuracy: 0.7787\n",
      "Epoch 78/100\n",
      "2082/2082 - 61s - loss: 0.4860 - accuracy: 0.7796\n",
      "Epoch 79/100\n",
      "2082/2082 - 61s - loss: 0.4856 - accuracy: 0.7782\n",
      "Epoch 80/100\n",
      "2082/2082 - 61s - loss: 0.4850 - accuracy: 0.7794\n",
      "Epoch 81/100\n",
      "2082/2082 - 61s - loss: 0.4855 - accuracy: 0.7790\n",
      "Epoch 82/100\n",
      "2082/2082 - 61s - loss: 0.4849 - accuracy: 0.7794\n",
      "Epoch 83/100\n",
      "2082/2082 - 61s - loss: 0.4848 - accuracy: 0.7798\n",
      "Epoch 84/100\n",
      "2082/2082 - 61s - loss: 0.4848 - accuracy: 0.7796\n",
      "Epoch 85/100\n",
      "2082/2082 - 61s - loss: 0.4836 - accuracy: 0.7785\n",
      "Epoch 86/100\n",
      "2082/2082 - 61s - loss: 0.4839 - accuracy: 0.7805\n",
      "Epoch 87/100\n",
      "2082/2082 - 61s - loss: 0.4833 - accuracy: 0.7805\n",
      "Epoch 88/100\n",
      "2082/2082 - 61s - loss: 0.4822 - accuracy: 0.7810\n",
      "Epoch 89/100\n",
      "2082/2082 - 61s - loss: 0.4824 - accuracy: 0.7798\n",
      "Epoch 90/100\n",
      "2082/2082 - 61s - loss: 0.4833 - accuracy: 0.7803\n",
      "Epoch 91/100\n",
      "2082/2082 - 61s - loss: 0.4822 - accuracy: 0.7809\n",
      "Epoch 92/100\n",
      "2082/2082 - 61s - loss: 0.4819 - accuracy: 0.7818\n",
      "Epoch 93/100\n",
      "2082/2082 - 61s - loss: 0.4814 - accuracy: 0.7812\n",
      "Epoch 94/100\n",
      "2082/2082 - 61s - loss: 0.4812 - accuracy: 0.7818\n",
      "Epoch 95/100\n",
      "2082/2082 - 61s - loss: 0.4810 - accuracy: 0.7818\n",
      "Epoch 96/100\n",
      "2082/2082 - 61s - loss: 0.4807 - accuracy: 0.7810\n",
      "Epoch 97/100\n",
      "2082/2082 - 61s - loss: 0.4799 - accuracy: 0.7823\n",
      "Epoch 98/100\n",
      "2082/2082 - 61s - loss: 0.4797 - accuracy: 0.7813\n",
      "Epoch 99/100\n",
      "2082/2082 - 61s - loss: 0.4803 - accuracy: 0.7814\n",
      "Epoch 100/100\n",
      "2082/2082 - 61s - loss: 0.4790 - accuracy: 0.7825\n",
      "417/417 - 3s - loss: 0.6044 - accuracy: 0.7366\n",
      "Epoch 1/100\n",
      "417/417 - 13s - loss: 0.8448 - accuracy: 0.7368\n",
      "Epoch 2/100\n",
      "417/417 - 12s - loss: 0.7544 - accuracy: 0.7389\n",
      "Epoch 3/100\n",
      "417/417 - 12s - loss: 0.7535 - accuracy: 0.7389\n",
      "Epoch 4/100\n",
      "417/417 - 12s - loss: 0.7530 - accuracy: 0.7389\n",
      "Epoch 5/100\n",
      "417/417 - 12s - loss: 0.7520 - accuracy: 0.7389\n",
      "Epoch 6/100\n",
      "417/417 - 12s - loss: 0.7512 - accuracy: 0.7389\n",
      "Epoch 7/100\n",
      "417/417 - 12s - loss: 0.7491 - accuracy: 0.7389\n",
      "Epoch 8/100\n",
      "417/417 - 12s - loss: 0.7463 - accuracy: 0.7389\n",
      "Epoch 9/100\n",
      "417/417 - 12s - loss: 0.7424 - accuracy: 0.7389\n",
      "Epoch 10/100\n",
      "417/417 - 12s - loss: 0.7362 - accuracy: 0.7389\n",
      "Epoch 11/100\n",
      "417/417 - 12s - loss: 0.7260 - accuracy: 0.7389\n",
      "Epoch 12/100\n",
      "417/417 - 12s - loss: 0.7109 - accuracy: 0.7389\n",
      "Epoch 13/100\n",
      "417/417 - 12s - loss: 0.6929 - accuracy: 0.7389\n",
      "Epoch 14/100\n",
      "417/417 - 12s - loss: 0.6748 - accuracy: 0.7389\n",
      "Epoch 15/100\n",
      "417/417 - 12s - loss: 0.6539 - accuracy: 0.7389\n",
      "Epoch 16/100\n",
      "417/417 - 12s - loss: 0.6427 - accuracy: 0.7394\n",
      "Epoch 17/100\n",
      "417/417 - 12s - loss: 0.6324 - accuracy: 0.7411\n",
      "Epoch 18/100\n",
      "417/417 - 12s - loss: 0.6243 - accuracy: 0.7438\n",
      "Epoch 19/100\n",
      "417/417 - 12s - loss: 0.6219 - accuracy: 0.7447\n",
      "Epoch 20/100\n",
      "417/417 - 12s - loss: 0.6145 - accuracy: 0.7473\n",
      "Epoch 21/100\n",
      "417/417 - 12s - loss: 0.6134 - accuracy: 0.7505\n",
      "Epoch 22/100\n",
      "417/417 - 12s - loss: 0.6098 - accuracy: 0.7469\n",
      "Epoch 23/100\n",
      "417/417 - 12s - loss: 0.6057 - accuracy: 0.7528\n",
      "Epoch 24/100\n",
      "417/417 - 12s - loss: 0.6050 - accuracy: 0.7503\n",
      "Epoch 25/100\n",
      "417/417 - 12s - loss: 0.6025 - accuracy: 0.7565\n",
      "Epoch 26/100\n",
      "417/417 - 12s - loss: 0.5996 - accuracy: 0.7532\n",
      "Epoch 27/100\n",
      "417/417 - 17s - loss: 0.5954 - accuracy: 0.7554\n",
      "Epoch 28/100\n",
      "417/417 - 12s - loss: 0.5926 - accuracy: 0.7512\n",
      "Epoch 29/100\n",
      "417/417 - 12s - loss: 0.5950 - accuracy: 0.7520\n",
      "Epoch 30/100\n",
      "417/417 - 12s - loss: 0.5914 - accuracy: 0.7563\n",
      "Epoch 31/100\n",
      "417/417 - 12s - loss: 0.5895 - accuracy: 0.7566\n",
      "Epoch 32/100\n",
      "417/417 - 12s - loss: 0.5906 - accuracy: 0.7578\n",
      "Epoch 33/100\n",
      "417/417 - 12s - loss: 0.5848 - accuracy: 0.7578\n",
      "Epoch 34/100\n",
      "417/417 - 12s - loss: 0.5833 - accuracy: 0.7603\n",
      "Epoch 35/100\n",
      "417/417 - 12s - loss: 0.5850 - accuracy: 0.7634\n",
      "Epoch 36/100\n",
      "417/417 - 12s - loss: 0.5827 - accuracy: 0.7612\n",
      "Epoch 37/100\n",
      "417/417 - 12s - loss: 0.5777 - accuracy: 0.7613\n",
      "Epoch 38/100\n",
      "417/417 - 12s - loss: 0.5719 - accuracy: 0.7614\n",
      "Epoch 39/100\n",
      "417/417 - 12s - loss: 0.5699 - accuracy: 0.7660\n",
      "Epoch 40/100\n",
      "417/417 - 12s - loss: 0.5680 - accuracy: 0.7650\n",
      "Epoch 41/100\n",
      "417/417 - 12s - loss: 0.5677 - accuracy: 0.7663\n",
      "Epoch 42/100\n",
      "417/417 - 12s - loss: 0.5629 - accuracy: 0.7663\n",
      "Epoch 43/100\n",
      "417/417 - 12s - loss: 0.5636 - accuracy: 0.7650\n",
      "Epoch 44/100\n",
      "417/417 - 12s - loss: 0.5620 - accuracy: 0.7665\n",
      "Epoch 45/100\n",
      "417/417 - 12s - loss: 0.5612 - accuracy: 0.7689\n",
      "Epoch 46/100\n",
      "417/417 - 12s - loss: 0.5574 - accuracy: 0.7695\n",
      "Epoch 47/100\n",
      "417/417 - 12s - loss: 0.5593 - accuracy: 0.7653\n",
      "Epoch 48/100\n",
      "417/417 - 12s - loss: 0.5551 - accuracy: 0.7686\n",
      "Epoch 49/100\n",
      "417/417 - 12s - loss: 0.5555 - accuracy: 0.7711\n",
      "Epoch 50/100\n",
      "417/417 - 12s - loss: 0.5554 - accuracy: 0.7663\n",
      "Epoch 51/100\n",
      "417/417 - 12s - loss: 0.5527 - accuracy: 0.7693\n",
      "Epoch 52/100\n",
      "417/417 - 12s - loss: 0.5545 - accuracy: 0.7695\n",
      "Epoch 53/100\n",
      "417/417 - 12s - loss: 0.5486 - accuracy: 0.7722\n",
      "Epoch 54/100\n",
      "417/417 - 12s - loss: 0.5490 - accuracy: 0.7685\n",
      "Epoch 55/100\n",
      "417/417 - 12s - loss: 0.5481 - accuracy: 0.7719\n",
      "Epoch 56/100\n",
      "417/417 - 12s - loss: 0.5472 - accuracy: 0.7724\n",
      "Epoch 57/100\n",
      "417/417 - 12s - loss: 0.5446 - accuracy: 0.7722\n",
      "Epoch 58/100\n",
      "417/417 - 12s - loss: 0.5441 - accuracy: 0.7746\n",
      "Epoch 59/100\n",
      "417/417 - 12s - loss: 0.5403 - accuracy: 0.7739\n",
      "Epoch 60/100\n",
      "417/417 - 12s - loss: 0.5432 - accuracy: 0.7729\n",
      "Epoch 61/100\n",
      "417/417 - 12s - loss: 0.5430 - accuracy: 0.7727\n",
      "Epoch 62/100\n",
      "417/417 - 12s - loss: 0.5394 - accuracy: 0.7729\n",
      "Epoch 63/100\n",
      "417/417 - 12s - loss: 0.5380 - accuracy: 0.7733\n",
      "Epoch 64/100\n",
      "417/417 - 12s - loss: 0.5390 - accuracy: 0.7735\n",
      "Epoch 65/100\n",
      "417/417 - 12s - loss: 0.5348 - accuracy: 0.7749\n",
      "Epoch 66/100\n",
      "417/417 - 12s - loss: 0.5364 - accuracy: 0.7736\n",
      "Epoch 67/100\n",
      "417/417 - 12s - loss: 0.5345 - accuracy: 0.7732\n",
      "Epoch 68/100\n",
      "417/417 - 12s - loss: 0.5342 - accuracy: 0.7758\n",
      "Epoch 69/100\n",
      "417/417 - 12s - loss: 0.5341 - accuracy: 0.7748\n",
      "Epoch 70/100\n",
      "417/417 - 12s - loss: 0.5318 - accuracy: 0.7761\n",
      "Epoch 71/100\n",
      "417/417 - 12s - loss: 0.5314 - accuracy: 0.7737\n",
      "Epoch 72/100\n",
      "417/417 - 12s - loss: 0.5323 - accuracy: 0.7760\n",
      "Epoch 73/100\n",
      "417/417 - 12s - loss: 0.5316 - accuracy: 0.7783\n",
      "Epoch 74/100\n",
      "417/417 - 12s - loss: 0.5279 - accuracy: 0.7783\n",
      "Epoch 75/100\n",
      "417/417 - 12s - loss: 0.5266 - accuracy: 0.7753\n",
      "Epoch 76/100\n",
      "417/417 - 12s - loss: 0.5263 - accuracy: 0.7770\n",
      "Epoch 77/100\n",
      "417/417 - 12s - loss: 0.5262 - accuracy: 0.7777\n",
      "Epoch 78/100\n",
      "417/417 - 12s - loss: 0.5283 - accuracy: 0.7776\n",
      "Epoch 79/100\n",
      "417/417 - 12s - loss: 0.5246 - accuracy: 0.7771\n",
      "Epoch 80/100\n",
      "417/417 - 12s - loss: 0.5236 - accuracy: 0.7765\n",
      "Epoch 81/100\n",
      "417/417 - 12s - loss: 0.5217 - accuracy: 0.7752\n",
      "Epoch 82/100\n",
      "417/417 - 12s - loss: 0.5246 - accuracy: 0.7777\n",
      "Epoch 83/100\n",
      "417/417 - 12s - loss: 0.5214 - accuracy: 0.7746\n",
      "Epoch 84/100\n",
      "417/417 - 12s - loss: 0.5212 - accuracy: 0.7774\n",
      "Epoch 85/100\n",
      "417/417 - 12s - loss: 0.5207 - accuracy: 0.7790\n",
      "Epoch 86/100\n",
      "417/417 - 12s - loss: 0.5182 - accuracy: 0.7791\n",
      "Epoch 87/100\n",
      "417/417 - 12s - loss: 0.5199 - accuracy: 0.7789\n",
      "Epoch 88/100\n",
      "417/417 - 12s - loss: 0.5175 - accuracy: 0.7809\n",
      "Epoch 89/100\n",
      "417/417 - 12s - loss: 0.5178 - accuracy: 0.7801\n",
      "Epoch 90/100\n",
      "417/417 - 12s - loss: 0.5197 - accuracy: 0.7774\n",
      "Epoch 91/100\n",
      "417/417 - 12s - loss: 0.5175 - accuracy: 0.7801\n",
      "Epoch 92/100\n",
      "417/417 - 12s - loss: 0.5151 - accuracy: 0.7809\n",
      "Epoch 93/100\n",
      "417/417 - 12s - loss: 0.5163 - accuracy: 0.7777\n",
      "Epoch 94/100\n",
      "417/417 - 12s - loss: 0.5173 - accuracy: 0.7798\n",
      "Epoch 95/100\n",
      "417/417 - 12s - loss: 0.5144 - accuracy: 0.7808\n",
      "Epoch 96/100\n",
      "417/417 - 12s - loss: 0.5141 - accuracy: 0.7810\n",
      "Epoch 97/100\n",
      "417/417 - 12s - loss: 0.5119 - accuracy: 0.7816\n",
      "Epoch 98/100\n",
      "417/417 - 12s - loss: 0.5137 - accuracy: 0.7792\n",
      "Epoch 99/100\n",
      "417/417 - 12s - loss: 0.5133 - accuracy: 0.7818\n",
      "Epoch 100/100\n",
      "417/417 - 12s - loss: 0.5137 - accuracy: 0.7809\n",
      "417/417 - 3s - loss: 0.5443 - accuracy: 0.7750\n",
      "Epoch 1/100\n",
      "833/833 - 25s - loss: 0.8031 - accuracy: 0.7292\n",
      "Epoch 2/100\n",
      "833/833 - 25s - loss: 0.7519 - accuracy: 0.7396\n",
      "Epoch 3/100\n",
      "833/833 - 25s - loss: 0.7509 - accuracy: 0.7396\n",
      "Epoch 4/100\n",
      "833/833 - 25s - loss: 0.7474 - accuracy: 0.7396\n",
      "Epoch 5/100\n",
      "833/833 - 25s - loss: 0.7418 - accuracy: 0.7396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "833/833 - 25s - loss: 0.7291 - accuracy: 0.7396\n",
      "Epoch 7/100\n",
      "833/833 - 25s - loss: 0.7061 - accuracy: 0.7396\n",
      "Epoch 8/100\n",
      "833/833 - 25s - loss: 0.6702 - accuracy: 0.7396\n",
      "Epoch 9/100\n",
      "833/833 - 25s - loss: 0.6367 - accuracy: 0.7409\n",
      "Epoch 10/100\n",
      "833/833 - 25s - loss: 0.6181 - accuracy: 0.7450\n",
      "Epoch 11/100\n",
      "833/833 - 25s - loss: 0.6057 - accuracy: 0.7499\n",
      "Epoch 12/100\n",
      "833/833 - 25s - loss: 0.5981 - accuracy: 0.7546\n",
      "Epoch 13/100\n",
      "833/833 - 25s - loss: 0.5941 - accuracy: 0.7596\n",
      "Epoch 14/100\n",
      "833/833 - 25s - loss: 0.5905 - accuracy: 0.7621\n",
      "Epoch 15/100\n",
      "833/833 - 25s - loss: 0.5873 - accuracy: 0.7607\n",
      "Epoch 16/100\n",
      "833/833 - 25s - loss: 0.5845 - accuracy: 0.7626\n",
      "Epoch 17/100\n",
      "833/833 - 25s - loss: 0.5798 - accuracy: 0.7655\n",
      "Epoch 18/100\n",
      "833/833 - 25s - loss: 0.5780 - accuracy: 0.7646\n",
      "Epoch 19/100\n",
      "833/833 - 25s - loss: 0.5758 - accuracy: 0.7643\n",
      "Epoch 20/100\n",
      "833/833 - 25s - loss: 0.5747 - accuracy: 0.7644\n",
      "Epoch 21/100\n",
      "833/833 - 25s - loss: 0.5735 - accuracy: 0.7645\n",
      "Epoch 22/100\n",
      "833/833 - 25s - loss: 0.5691 - accuracy: 0.7645\n",
      "Epoch 23/100\n",
      "833/833 - 25s - loss: 0.5641 - accuracy: 0.7669\n",
      "Epoch 24/100\n",
      "833/833 - 25s - loss: 0.5603 - accuracy: 0.7662\n",
      "Epoch 25/100\n",
      "833/833 - 25s - loss: 0.5561 - accuracy: 0.7699\n",
      "Epoch 26/100\n",
      "833/833 - 25s - loss: 0.5548 - accuracy: 0.7679\n",
      "Epoch 27/100\n",
      "833/833 - 25s - loss: 0.5537 - accuracy: 0.7680\n",
      "Epoch 28/100\n",
      "833/833 - 25s - loss: 0.5520 - accuracy: 0.7699\n",
      "Epoch 29/100\n",
      "833/833 - 25s - loss: 0.5500 - accuracy: 0.7701\n",
      "Epoch 30/100\n",
      "833/833 - 25s - loss: 0.5490 - accuracy: 0.7709\n",
      "Epoch 31/100\n",
      "833/833 - 25s - loss: 0.5468 - accuracy: 0.7701\n",
      "Epoch 32/100\n",
      "833/833 - 25s - loss: 0.5451 - accuracy: 0.7700\n",
      "Epoch 33/100\n",
      "833/833 - 25s - loss: 0.5435 - accuracy: 0.7719\n",
      "Epoch 34/100\n",
      "833/833 - 25s - loss: 0.5418 - accuracy: 0.7714\n",
      "Epoch 35/100\n",
      "833/833 - 25s - loss: 0.5417 - accuracy: 0.7710\n",
      "Epoch 36/100\n",
      "833/833 - 25s - loss: 0.5391 - accuracy: 0.7709\n",
      "Epoch 37/100\n",
      "833/833 - 25s - loss: 0.5386 - accuracy: 0.7722\n",
      "Epoch 38/100\n",
      "833/833 - 25s - loss: 0.5366 - accuracy: 0.7715\n",
      "Epoch 39/100\n",
      "833/833 - 25s - loss: 0.5342 - accuracy: 0.7718\n",
      "Epoch 40/100\n",
      "833/833 - 25s - loss: 0.5350 - accuracy: 0.7709\n",
      "Epoch 41/100\n",
      "833/833 - 25s - loss: 0.5308 - accuracy: 0.7731\n",
      "Epoch 42/100\n",
      "833/833 - 25s - loss: 0.5326 - accuracy: 0.7721\n",
      "Epoch 43/100\n",
      "833/833 - 25s - loss: 0.5307 - accuracy: 0.7737\n",
      "Epoch 44/100\n",
      "833/833 - 25s - loss: 0.5298 - accuracy: 0.7719\n",
      "Epoch 45/100\n",
      "833/833 - 25s - loss: 0.5295 - accuracy: 0.7712\n",
      "Epoch 46/100\n",
      "833/833 - 25s - loss: 0.5276 - accuracy: 0.7727\n",
      "Epoch 47/100\n",
      "833/833 - 25s - loss: 0.5283 - accuracy: 0.7734\n",
      "Epoch 48/100\n",
      "833/833 - 25s - loss: 0.5250 - accuracy: 0.7731\n",
      "Epoch 49/100\n",
      "833/833 - 25s - loss: 0.5242 - accuracy: 0.7715\n",
      "Epoch 50/100\n",
      "833/833 - 25s - loss: 0.5222 - accuracy: 0.7755\n",
      "Epoch 51/100\n",
      "833/833 - 25s - loss: 0.5227 - accuracy: 0.7735\n",
      "Epoch 52/100\n",
      "833/833 - 25s - loss: 0.5204 - accuracy: 0.7744\n",
      "Epoch 53/100\n",
      "833/833 - 25s - loss: 0.5196 - accuracy: 0.7728\n",
      "Epoch 54/100\n",
      "833/833 - 25s - loss: 0.5197 - accuracy: 0.7740\n",
      "Epoch 55/100\n",
      "833/833 - 25s - loss: 0.5195 - accuracy: 0.7754\n",
      "Epoch 56/100\n",
      "833/833 - 25s - loss: 0.5179 - accuracy: 0.7731\n",
      "Epoch 57/100\n",
      "833/833 - 25s - loss: 0.5173 - accuracy: 0.7760\n",
      "Epoch 58/100\n",
      "833/833 - 25s - loss: 0.5160 - accuracy: 0.7759\n",
      "Epoch 59/100\n",
      "833/833 - 25s - loss: 0.5148 - accuracy: 0.7757\n",
      "Epoch 60/100\n",
      "833/833 - 25s - loss: 0.5162 - accuracy: 0.7735\n",
      "Epoch 61/100\n",
      "833/833 - 25s - loss: 0.5131 - accuracy: 0.7761\n",
      "Epoch 62/100\n",
      "833/833 - 25s - loss: 0.5130 - accuracy: 0.7769\n",
      "Epoch 63/100\n",
      "833/833 - 25s - loss: 0.5133 - accuracy: 0.7781\n",
      "Epoch 64/100\n",
      "833/833 - 25s - loss: 0.5109 - accuracy: 0.7778\n",
      "Epoch 65/100\n",
      "833/833 - 25s - loss: 0.5102 - accuracy: 0.7766\n",
      "Epoch 66/100\n",
      "833/833 - 25s - loss: 0.5112 - accuracy: 0.7743\n",
      "Epoch 67/100\n",
      "833/833 - 25s - loss: 0.5104 - accuracy: 0.7775\n",
      "Epoch 68/100\n",
      "833/833 - 25s - loss: 0.5092 - accuracy: 0.7789\n",
      "Epoch 69/100\n",
      "833/833 - 25s - loss: 0.5081 - accuracy: 0.7773\n",
      "Epoch 70/100\n",
      "833/833 - 25s - loss: 0.5061 - accuracy: 0.7797\n",
      "Epoch 71/100\n",
      "833/833 - 25s - loss: 0.5063 - accuracy: 0.7797\n",
      "Epoch 72/100\n",
      "833/833 - 24s - loss: 0.5066 - accuracy: 0.7778\n",
      "Epoch 73/100\n",
      "833/833 - 24s - loss: 0.5061 - accuracy: 0.7821\n",
      "Epoch 74/100\n",
      "833/833 - 24s - loss: 0.5051 - accuracy: 0.7788\n",
      "Epoch 75/100\n",
      "833/833 - 24s - loss: 0.5060 - accuracy: 0.7778\n",
      "Epoch 76/100\n",
      "833/833 - 24s - loss: 0.5051 - accuracy: 0.7792\n",
      "Epoch 77/100\n",
      "833/833 - 24s - loss: 0.5046 - accuracy: 0.7789\n",
      "Epoch 78/100\n",
      "833/833 - 24s - loss: 0.5028 - accuracy: 0.7810\n",
      "Epoch 79/100\n",
      "833/833 - 24s - loss: 0.5029 - accuracy: 0.7819\n",
      "Epoch 80/100\n",
      "833/833 - 24s - loss: 0.5038 - accuracy: 0.7805\n",
      "Epoch 81/100\n",
      "833/833 - 24s - loss: 0.5035 - accuracy: 0.7783\n",
      "Epoch 82/100\n",
      "833/833 - 24s - loss: 0.5008 - accuracy: 0.7801\n",
      "Epoch 83/100\n",
      "833/833 - 24s - loss: 0.5015 - accuracy: 0.7816\n",
      "Epoch 84/100\n",
      "833/833 - 24s - loss: 0.5012 - accuracy: 0.7805\n",
      "Epoch 85/100\n",
      "833/833 - 24s - loss: 0.5005 - accuracy: 0.7783\n",
      "Epoch 86/100\n",
      "833/833 - 24s - loss: 0.5001 - accuracy: 0.7799\n",
      "Epoch 87/100\n",
      "833/833 - 24s - loss: 0.4998 - accuracy: 0.7801\n",
      "Epoch 88/100\n",
      "833/833 - 24s - loss: 0.4991 - accuracy: 0.7830\n",
      "Epoch 89/100\n",
      "833/833 - 24s - loss: 0.4995 - accuracy: 0.7804\n",
      "Epoch 90/100\n",
      "833/833 - 24s - loss: 0.4982 - accuracy: 0.7836\n",
      "Epoch 91/100\n",
      "833/833 - 24s - loss: 0.4992 - accuracy: 0.7813\n",
      "Epoch 92/100\n",
      "833/833 - 24s - loss: 0.4985 - accuracy: 0.7813\n",
      "Epoch 93/100\n",
      "833/833 - 24s - loss: 0.4961 - accuracy: 0.7840\n",
      "Epoch 94/100\n",
      "833/833 - 24s - loss: 0.4972 - accuracy: 0.7834\n",
      "Epoch 95/100\n",
      "833/833 - 24s - loss: 0.4971 - accuracy: 0.7816\n",
      "Epoch 96/100\n",
      "833/833 - 24s - loss: 0.4969 - accuracy: 0.7816\n",
      "Epoch 97/100\n",
      "833/833 - 24s - loss: 0.4966 - accuracy: 0.7830\n",
      "Epoch 98/100\n",
      "833/833 - 24s - loss: 0.4970 - accuracy: 0.7823\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00098: early stopping\n",
      "417/417 - 3s - loss: 0.5070 - accuracy: 0.7766\n",
      "Epoch 1/100\n",
      "1249/1249 - 35s - loss: 0.7837 - accuracy: 0.7321\n",
      "Epoch 2/100\n",
      "1249/1249 - 33s - loss: 0.7452 - accuracy: 0.7440\n",
      "Epoch 3/100\n",
      "1249/1249 - 33s - loss: 0.7434 - accuracy: 0.7440\n",
      "Epoch 4/100\n",
      "1249/1249 - 33s - loss: 0.7386 - accuracy: 0.7440\n",
      "Epoch 5/100\n",
      "1249/1249 - 33s - loss: 0.7234 - accuracy: 0.7440\n",
      "Epoch 6/100\n",
      "1249/1249 - 33s - loss: 0.6834 - accuracy: 0.7440\n",
      "Epoch 7/100\n",
      "1249/1249 - 32s - loss: 0.6300 - accuracy: 0.7452\n",
      "Epoch 8/100\n",
      "1249/1249 - 33s - loss: 0.6060 - accuracy: 0.7501\n",
      "Epoch 9/100\n",
      "1249/1249 - 33s - loss: 0.5963 - accuracy: 0.7557\n",
      "Epoch 10/100\n",
      "1249/1249 - 33s - loss: 0.5906 - accuracy: 0.7578\n",
      "Epoch 11/100\n",
      "1249/1249 - 33s - loss: 0.5845 - accuracy: 0.7606\n",
      "Epoch 12/100\n",
      "1249/1249 - 33s - loss: 0.5800 - accuracy: 0.7620\n",
      "Epoch 13/100\n",
      "1249/1249 - 33s - loss: 0.5747 - accuracy: 0.7618\n",
      "Epoch 14/100\n",
      "1249/1249 - 33s - loss: 0.5673 - accuracy: 0.7656\n",
      "Epoch 15/100\n",
      "1249/1249 - 33s - loss: 0.5640 - accuracy: 0.7656\n",
      "Epoch 16/100\n",
      "1249/1249 - 33s - loss: 0.5642 - accuracy: 0.7656\n",
      "Epoch 17/100\n",
      "1249/1249 - 33s - loss: 0.5605 - accuracy: 0.7676\n",
      "Epoch 18/100\n",
      "1249/1249 - 36s - loss: 0.5558 - accuracy: 0.7683\n",
      "Epoch 19/100\n",
      "1249/1249 - 33s - loss: 0.5545 - accuracy: 0.7677\n",
      "Epoch 20/100\n",
      "1249/1249 - 33s - loss: 0.5527 - accuracy: 0.7675\n",
      "Epoch 21/100\n",
      "1249/1249 - 33s - loss: 0.5489 - accuracy: 0.7687\n",
      "Epoch 22/100\n",
      "1249/1249 - 33s - loss: 0.5465 - accuracy: 0.7695\n",
      "Epoch 23/100\n",
      "1249/1249 - 33s - loss: 0.5436 - accuracy: 0.7720\n",
      "Epoch 24/100\n",
      "1249/1249 - 33s - loss: 0.5415 - accuracy: 0.7712\n",
      "Epoch 25/100\n",
      "1249/1249 - 33s - loss: 0.5406 - accuracy: 0.7701\n",
      "Epoch 26/100\n",
      "1249/1249 - 33s - loss: 0.5384 - accuracy: 0.7724\n",
      "Epoch 27/100\n",
      "1249/1249 - 33s - loss: 0.5352 - accuracy: 0.7724\n",
      "Epoch 28/100\n",
      "1249/1249 - 33s - loss: 0.5308 - accuracy: 0.7746\n",
      "Epoch 29/100\n",
      "1249/1249 - 33s - loss: 0.5308 - accuracy: 0.7743\n",
      "Epoch 30/100\n",
      "1249/1249 - 33s - loss: 0.5289 - accuracy: 0.7728\n",
      "Epoch 31/100\n",
      "1249/1249 - 33s - loss: 0.5277 - accuracy: 0.7742\n",
      "Epoch 32/100\n",
      "1249/1249 - 33s - loss: 0.5263 - accuracy: 0.7741\n",
      "Epoch 33/100\n",
      "1249/1249 - 33s - loss: 0.5238 - accuracy: 0.7748\n",
      "Epoch 34/100\n",
      "1249/1249 - 33s - loss: 0.5231 - accuracy: 0.7744\n",
      "Epoch 35/100\n",
      "1249/1249 - 33s - loss: 0.5224 - accuracy: 0.7735\n",
      "Epoch 36/100\n",
      "1249/1249 - 33s - loss: 0.5200 - accuracy: 0.7746\n",
      "Epoch 37/100\n",
      "1249/1249 - 33s - loss: 0.5192 - accuracy: 0.7773\n",
      "Epoch 38/100\n",
      "1249/1249 - 33s - loss: 0.5175 - accuracy: 0.7758\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1249/1249 - 33s - loss: 0.5176 - accuracy: 0.7753\n",
      "Epoch 40/100\n",
      "1249/1249 - 33s - loss: 0.5160 - accuracy: 0.7761\n",
      "Epoch 41/100\n",
      "1249/1249 - 33s - loss: 0.5146 - accuracy: 0.7750\n",
      "Epoch 42/100\n",
      "1249/1249 - 33s - loss: 0.5142 - accuracy: 0.7756\n",
      "Epoch 43/100\n",
      "1249/1249 - 33s - loss: 0.5122 - accuracy: 0.7765\n",
      "Epoch 44/100\n",
      "1249/1249 - 33s - loss: 0.5115 - accuracy: 0.7768\n",
      "Epoch 45/100\n",
      "1249/1249 - 33s - loss: 0.5108 - accuracy: 0.7774\n",
      "Epoch 46/100\n",
      "1249/1249 - 33s - loss: 0.5087 - accuracy: 0.7778\n",
      "Epoch 47/100\n",
      "1249/1249 - 33s - loss: 0.5100 - accuracy: 0.7773\n",
      "Epoch 48/100\n",
      "1249/1249 - 33s - loss: 0.5081 - accuracy: 0.7776\n",
      "Epoch 49/100\n",
      "1249/1249 - 33s - loss: 0.5080 - accuracy: 0.7778\n",
      "Epoch 50/100\n",
      "1249/1249 - 33s - loss: 0.5058 - accuracy: 0.7785\n",
      "Epoch 51/100\n",
      "1249/1249 - 33s - loss: 0.5063 - accuracy: 0.7767\n",
      "Epoch 52/100\n",
      "1249/1249 - 33s - loss: 0.5044 - accuracy: 0.7776\n",
      "Epoch 53/100\n",
      "1249/1249 - 36s - loss: 0.5039 - accuracy: 0.7806\n",
      "Epoch 54/100\n",
      "1249/1249 - 33s - loss: 0.5035 - accuracy: 0.7789\n",
      "Epoch 55/100\n",
      "1249/1249 - 34s - loss: 0.5024 - accuracy: 0.7804\n",
      "Epoch 56/100\n",
      "1249/1249 - 33s - loss: 0.5015 - accuracy: 0.7787\n",
      "Epoch 57/100\n",
      "1249/1249 - 33s - loss: 0.5017 - accuracy: 0.7806\n",
      "Epoch 58/100\n",
      "1249/1249 - 33s - loss: 0.5006 - accuracy: 0.7803\n",
      "Epoch 59/100\n",
      "1249/1249 - 33s - loss: 0.5008 - accuracy: 0.7777\n",
      "Epoch 60/100\n",
      "1249/1249 - 33s - loss: 0.5001 - accuracy: 0.7792\n",
      "Epoch 61/100\n",
      "1249/1249 - 33s - loss: 0.4996 - accuracy: 0.7780\n",
      "Epoch 62/100\n",
      "1249/1249 - 33s - loss: 0.4991 - accuracy: 0.7795\n",
      "Epoch 63/100\n",
      "1249/1249 - 33s - loss: 0.4989 - accuracy: 0.7808\n",
      "Epoch 64/100\n",
      "1249/1249 - 33s - loss: 0.4981 - accuracy: 0.7798\n",
      "Epoch 65/100\n",
      "1249/1249 - 33s - loss: 0.4970 - accuracy: 0.7821\n",
      "Epoch 66/100\n",
      "1249/1249 - 33s - loss: 0.4970 - accuracy: 0.7797\n",
      "Epoch 67/100\n",
      "1249/1249 - 33s - loss: 0.4960 - accuracy: 0.7816\n",
      "Epoch 68/100\n",
      "1249/1249 - 33s - loss: 0.4958 - accuracy: 0.7816\n",
      "Epoch 69/100\n",
      "1249/1249 - 33s - loss: 0.4953 - accuracy: 0.7821\n",
      "Epoch 70/100\n",
      "1249/1249 - 33s - loss: 0.4960 - accuracy: 0.7811\n",
      "Epoch 71/100\n",
      "1249/1249 - 33s - loss: 0.4959 - accuracy: 0.7800\n",
      "Epoch 72/100\n",
      "1249/1249 - 33s - loss: 0.4949 - accuracy: 0.7827\n",
      "Epoch 73/100\n",
      "1249/1249 - 33s - loss: 0.4938 - accuracy: 0.7840\n",
      "Epoch 74/100\n",
      "1249/1249 - 33s - loss: 0.4946 - accuracy: 0.7827\n",
      "Epoch 75/100\n",
      "1249/1249 - 33s - loss: 0.4936 - accuracy: 0.7820\n",
      "Epoch 76/100\n",
      "1249/1249 - 33s - loss: 0.4939 - accuracy: 0.7816\n",
      "Epoch 77/100\n",
      "1249/1249 - 33s - loss: 0.4927 - accuracy: 0.7838\n",
      "Epoch 78/100\n",
      "1249/1249 - 33s - loss: 0.4926 - accuracy: 0.7830\n",
      "Epoch 79/100\n",
      "1249/1249 - 37s - loss: 0.4922 - accuracy: 0.7826\n",
      "Epoch 80/100\n",
      "1249/1249 - 33s - loss: 0.4921 - accuracy: 0.7835\n",
      "Epoch 81/100\n",
      "1249/1249 - 33s - loss: 0.4914 - accuracy: 0.7831\n",
      "Epoch 82/100\n",
      "1249/1249 - 33s - loss: 0.4897 - accuracy: 0.7843\n",
      "Epoch 83/100\n",
      "1249/1249 - 33s - loss: 0.4906 - accuracy: 0.7842\n",
      "Epoch 84/100\n",
      "1249/1249 - 33s - loss: 0.4918 - accuracy: 0.7830\n",
      "Epoch 85/100\n",
      "1249/1249 - 33s - loss: 0.4898 - accuracy: 0.7839\n",
      "Epoch 86/100\n",
      "1249/1249 - 33s - loss: 0.4892 - accuracy: 0.7853\n",
      "Epoch 87/100\n",
      "1249/1249 - 33s - loss: 0.4882 - accuracy: 0.7858\n",
      "Epoch 88/100\n",
      "1249/1249 - 33s - loss: 0.4887 - accuracy: 0.7852\n",
      "Epoch 89/100\n",
      "1249/1249 - 33s - loss: 0.4881 - accuracy: 0.7847\n",
      "Epoch 90/100\n",
      "1249/1249 - 33s - loss: 0.4886 - accuracy: 0.7846\n",
      "Epoch 91/100\n",
      "1249/1249 - 33s - loss: 0.4886 - accuracy: 0.7863\n",
      "Epoch 92/100\n",
      "1249/1249 - 33s - loss: 0.4871 - accuracy: 0.7851\n",
      "Epoch 93/100\n",
      "1249/1249 - 33s - loss: 0.4870 - accuracy: 0.7859\n",
      "Epoch 94/100\n",
      "1249/1249 - 33s - loss: 0.4865 - accuracy: 0.7853\n",
      "Epoch 95/100\n",
      "1249/1249 - 33s - loss: 0.4864 - accuracy: 0.7851\n",
      "Epoch 96/100\n",
      "1249/1249 - 33s - loss: 0.4866 - accuracy: 0.7859\n",
      "Epoch 97/100\n",
      "1249/1249 - 33s - loss: 0.4859 - accuracy: 0.7859\n",
      "Epoch 98/100\n",
      "1249/1249 - 33s - loss: 0.4852 - accuracy: 0.7857\n",
      "Epoch 99/100\n",
      "1249/1249 - 33s - loss: 0.4851 - accuracy: 0.7874\n",
      "Epoch 100/100\n",
      "1249/1249 - 33s - loss: 0.4852 - accuracy: 0.7856\n",
      "417/417 - 3s - loss: 0.5385 - accuracy: 0.7553\n",
      "Epoch 1/100\n",
      "1666/1666 - 49s - loss: 0.7803 - accuracy: 0.7381\n",
      "Epoch 2/100\n",
      "1666/1666 - 49s - loss: 0.7492 - accuracy: 0.7413\n",
      "Epoch 3/100\n",
      "1666/1666 - 48s - loss: 0.7451 - accuracy: 0.7413\n",
      "Epoch 4/100\n",
      "1666/1666 - 48s - loss: 0.7221 - accuracy: 0.7413\n",
      "Epoch 5/100\n",
      "1666/1666 - 48s - loss: 0.6522 - accuracy: 0.7413\n",
      "Epoch 6/100\n",
      "1666/1666 - 48s - loss: 0.6236 - accuracy: 0.7420\n",
      "Epoch 7/100\n",
      "1666/1666 - 48s - loss: 0.6114 - accuracy: 0.7455\n",
      "Epoch 8/100\n",
      "1666/1666 - 48s - loss: 0.6016 - accuracy: 0.7507\n",
      "Epoch 9/100\n",
      "1666/1666 - 48s - loss: 0.5963 - accuracy: 0.7535\n",
      "Epoch 10/100\n",
      "1666/1666 - 48s - loss: 0.5917 - accuracy: 0.7556\n",
      "Epoch 11/100\n",
      "1666/1666 - 48s - loss: 0.5877 - accuracy: 0.7569\n",
      "Epoch 12/100\n",
      "1666/1666 - 48s - loss: 0.5838 - accuracy: 0.7576\n",
      "Epoch 13/100\n",
      "1666/1666 - 48s - loss: 0.5808 - accuracy: 0.7594\n",
      "Epoch 14/100\n",
      "1666/1666 - 48s - loss: 0.5772 - accuracy: 0.7591\n",
      "Epoch 15/100\n",
      "1666/1666 - 48s - loss: 0.5728 - accuracy: 0.7608\n",
      "Epoch 16/100\n",
      "1666/1666 - 48s - loss: 0.5695 - accuracy: 0.7612\n",
      "Epoch 17/100\n",
      "1666/1666 - 48s - loss: 0.5626 - accuracy: 0.7636\n",
      "Epoch 18/100\n",
      "1666/1666 - 48s - loss: 0.5553 - accuracy: 0.7645\n",
      "Epoch 19/100\n",
      "1666/1666 - 48s - loss: 0.5503 - accuracy: 0.7664\n",
      "Epoch 20/100\n",
      "1666/1666 - 48s - loss: 0.5442 - accuracy: 0.7651\n",
      "Epoch 21/100\n",
      "1666/1666 - 48s - loss: 0.5405 - accuracy: 0.7681\n",
      "Epoch 22/100\n",
      "1666/1666 - 48s - loss: 0.5374 - accuracy: 0.7689\n",
      "Epoch 23/100\n",
      "1666/1666 - 48s - loss: 0.5348 - accuracy: 0.7676\n",
      "Epoch 24/100\n",
      "1666/1666 - 49s - loss: 0.5312 - accuracy: 0.7698\n",
      "Epoch 25/100\n",
      "1666/1666 - 48s - loss: 0.5282 - accuracy: 0.7687\n",
      "Epoch 26/100\n",
      "1666/1666 - 48s - loss: 0.5262 - accuracy: 0.7701\n",
      "Epoch 27/100\n",
      "1666/1666 - 48s - loss: 0.5238 - accuracy: 0.7706\n",
      "Epoch 28/100\n",
      "1666/1666 - 48s - loss: 0.5230 - accuracy: 0.7714\n",
      "Epoch 29/100\n",
      "1666/1666 - 48s - loss: 0.5213 - accuracy: 0.7722\n",
      "Epoch 30/100\n",
      "1666/1666 - 49s - loss: 0.5179 - accuracy: 0.7706\n",
      "Epoch 31/100\n",
      "1666/1666 - 49s - loss: 0.5179 - accuracy: 0.7721\n",
      "Epoch 32/100\n",
      "1666/1666 - 49s - loss: 0.5160 - accuracy: 0.7730\n",
      "Epoch 33/100\n",
      "1666/1666 - 49s - loss: 0.5135 - accuracy: 0.7742\n",
      "Epoch 34/100\n",
      "1666/1666 - 48s - loss: 0.5133 - accuracy: 0.7732\n",
      "Epoch 35/100\n",
      "1666/1666 - 49s - loss: 0.5116 - accuracy: 0.7732\n",
      "Epoch 36/100\n",
      "1666/1666 - 49s - loss: 0.5115 - accuracy: 0.7734\n",
      "Epoch 37/100\n",
      "1666/1666 - 49s - loss: 0.5098 - accuracy: 0.7738\n",
      "Epoch 38/100\n",
      "1666/1666 - 49s - loss: 0.5084 - accuracy: 0.7758\n",
      "Epoch 39/100\n",
      "1666/1666 - 49s - loss: 0.5073 - accuracy: 0.7753\n",
      "Epoch 40/100\n",
      "1666/1666 - 48s - loss: 0.5067 - accuracy: 0.7759\n",
      "Epoch 41/100\n",
      "1666/1666 - 49s - loss: 0.5052 - accuracy: 0.7738\n",
      "Epoch 42/100\n",
      "1666/1666 - 53s - loss: 0.5048 - accuracy: 0.7754\n",
      "Epoch 43/100\n",
      "1666/1666 - 49s - loss: 0.5038 - accuracy: 0.7765\n",
      "Epoch 44/100\n",
      "1666/1666 - 49s - loss: 0.5031 - accuracy: 0.7754\n",
      "Epoch 45/100\n",
      "1666/1666 - 49s - loss: 0.5017 - accuracy: 0.7756\n",
      "Epoch 46/100\n",
      "1666/1666 - 49s - loss: 0.5008 - accuracy: 0.7763\n",
      "Epoch 47/100\n",
      "1666/1666 - 49s - loss: 0.5007 - accuracy: 0.7752\n",
      "Epoch 48/100\n",
      "1666/1666 - 49s - loss: 0.4996 - accuracy: 0.7762\n",
      "Epoch 49/100\n",
      "1666/1666 - 49s - loss: 0.4992 - accuracy: 0.7762\n",
      "Epoch 50/100\n",
      "1666/1666 - 49s - loss: 0.4983 - accuracy: 0.7764\n",
      "Epoch 51/100\n",
      "1666/1666 - 49s - loss: 0.4973 - accuracy: 0.7755\n",
      "Epoch 52/100\n",
      "1666/1666 - 49s - loss: 0.4978 - accuracy: 0.7772\n",
      "Epoch 53/100\n",
      "1666/1666 - 49s - loss: 0.4968 - accuracy: 0.7778\n",
      "Epoch 54/100\n",
      "1666/1666 - 49s - loss: 0.4954 - accuracy: 0.7784\n",
      "Epoch 55/100\n",
      "1666/1666 - 49s - loss: 0.4953 - accuracy: 0.7779\n",
      "Epoch 56/100\n",
      "1666/1666 - 49s - loss: 0.4944 - accuracy: 0.7780\n",
      "Epoch 57/100\n",
      "1666/1666 - 49s - loss: 0.4944 - accuracy: 0.7782\n",
      "Epoch 58/100\n",
      "1666/1666 - 49s - loss: 0.4926 - accuracy: 0.7781\n",
      "Epoch 59/100\n",
      "1666/1666 - 49s - loss: 0.4925 - accuracy: 0.7785\n",
      "Epoch 60/100\n",
      "1666/1666 - 49s - loss: 0.4931 - accuracy: 0.7775\n",
      "Epoch 61/100\n",
      "1666/1666 - 49s - loss: 0.4919 - accuracy: 0.7783\n",
      "Epoch 62/100\n",
      "1666/1666 - 49s - loss: 0.4925 - accuracy: 0.7789\n",
      "Epoch 63/100\n",
      "1666/1666 - 49s - loss: 0.4909 - accuracy: 0.7803\n",
      "Epoch 64/100\n",
      "1666/1666 - 49s - loss: 0.4909 - accuracy: 0.7797\n",
      "Epoch 65/100\n",
      "1666/1666 - 49s - loss: 0.4894 - accuracy: 0.7804\n",
      "Epoch 66/100\n",
      "1666/1666 - 49s - loss: 0.4889 - accuracy: 0.7801\n",
      "Epoch 67/100\n",
      "1666/1666 - 49s - loss: 0.4894 - accuracy: 0.7805\n",
      "Epoch 68/100\n",
      "1666/1666 - 48s - loss: 0.4891 - accuracy: 0.7783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/100\n",
      "1666/1666 - 48s - loss: 0.4886 - accuracy: 0.7804\n",
      "Epoch 70/100\n",
      "1666/1666 - 48s - loss: 0.4874 - accuracy: 0.7821\n",
      "Epoch 71/100\n",
      "1666/1666 - 48s - loss: 0.4877 - accuracy: 0.7807\n",
      "Epoch 72/100\n",
      "1666/1666 - 48s - loss: 0.4872 - accuracy: 0.7813\n",
      "Epoch 73/100\n",
      "1666/1666 - 49s - loss: 0.4865 - accuracy: 0.7818\n",
      "Epoch 74/100\n",
      "1666/1666 - 48s - loss: 0.4879 - accuracy: 0.7793\n",
      "Epoch 75/100\n",
      "1666/1666 - 48s - loss: 0.4860 - accuracy: 0.7814\n",
      "Epoch 76/100\n",
      "1666/1666 - 48s - loss: 0.4856 - accuracy: 0.7823\n",
      "Epoch 77/100\n",
      "1666/1666 - 48s - loss: 0.4863 - accuracy: 0.7821\n",
      "Epoch 78/100\n",
      "1666/1666 - 49s - loss: 0.4851 - accuracy: 0.7819\n",
      "Epoch 79/100\n",
      "1666/1666 - 49s - loss: 0.4848 - accuracy: 0.7834\n",
      "Epoch 80/100\n",
      "1666/1666 - 49s - loss: 0.4846 - accuracy: 0.7809\n",
      "Epoch 81/100\n",
      "1666/1666 - 49s - loss: 0.4839 - accuracy: 0.7836\n",
      "Epoch 82/100\n",
      "1666/1666 - 49s - loss: 0.4844 - accuracy: 0.7816\n",
      "Epoch 83/100\n",
      "1666/1666 - 49s - loss: 0.4831 - accuracy: 0.7831\n",
      "Epoch 84/100\n",
      "1666/1666 - 49s - loss: 0.4832 - accuracy: 0.7846\n",
      "Epoch 85/100\n",
      "1666/1666 - 49s - loss: 0.4829 - accuracy: 0.7831\n",
      "Epoch 86/100\n",
      "1666/1666 - 49s - loss: 0.4826 - accuracy: 0.7834\n",
      "Epoch 87/100\n",
      "1666/1666 - 49s - loss: 0.4819 - accuracy: 0.7843\n",
      "Epoch 88/100\n",
      "1666/1666 - 49s - loss: 0.4815 - accuracy: 0.7851\n",
      "Epoch 89/100\n",
      "1666/1666 - 49s - loss: 0.4822 - accuracy: 0.7852\n",
      "Epoch 90/100\n",
      "1666/1666 - 49s - loss: 0.4809 - accuracy: 0.7859\n",
      "Epoch 91/100\n",
      "1666/1666 - 49s - loss: 0.4797 - accuracy: 0.7856\n",
      "Epoch 92/100\n",
      "1666/1666 - 49s - loss: 0.4808 - accuracy: 0.7837\n",
      "Epoch 93/100\n",
      "1666/1666 - 49s - loss: 0.4792 - accuracy: 0.7850\n",
      "Epoch 94/100\n",
      "1666/1666 - 49s - loss: 0.4796 - accuracy: 0.7863\n",
      "Epoch 95/100\n",
      "1666/1666 - 49s - loss: 0.4796 - accuracy: 0.7858\n",
      "Epoch 96/100\n",
      "1666/1666 - 49s - loss: 0.4783 - accuracy: 0.7849\n",
      "Epoch 97/100\n",
      "1666/1666 - 49s - loss: 0.4779 - accuracy: 0.7866\n",
      "Epoch 98/100\n",
      "1666/1666 - 49s - loss: 0.4784 - accuracy: 0.7858\n",
      "Epoch 99/100\n",
      "1666/1666 - 49s - loss: 0.4775 - accuracy: 0.7862\n",
      "Epoch 100/100\n",
      "1666/1666 - 49s - loss: 0.4773 - accuracy: 0.7866\n",
      "417/417 - 3s - loss: 0.5210 - accuracy: 0.7608\n",
      "Epoch 1/100\n",
      "2082/2082 - 61s - loss: 0.7675 - accuracy: 0.7371\n",
      "Epoch 2/100\n",
      "2082/2082 - 60s - loss: 0.7471 - accuracy: 0.7405\n",
      "Epoch 3/100\n",
      "2082/2082 - 60s - loss: 0.7197 - accuracy: 0.7405\n",
      "Epoch 4/100\n",
      "2082/2082 - 60s - loss: 0.6474 - accuracy: 0.7406\n",
      "Epoch 5/100\n",
      "2082/2082 - 60s - loss: 0.6185 - accuracy: 0.7427\n",
      "Epoch 6/100\n",
      "2082/2082 - 60s - loss: 0.6052 - accuracy: 0.7467\n",
      "Epoch 7/100\n",
      "2082/2082 - 60s - loss: 0.5968 - accuracy: 0.7497\n",
      "Epoch 8/100\n",
      "2082/2082 - 60s - loss: 0.5922 - accuracy: 0.7499\n",
      "Epoch 9/100\n",
      "2082/2082 - 64s - loss: 0.5882 - accuracy: 0.7537\n",
      "Epoch 10/100\n",
      "2082/2082 - 60s - loss: 0.5839 - accuracy: 0.7557\n",
      "Epoch 11/100\n",
      "2082/2082 - 60s - loss: 0.5784 - accuracy: 0.7571\n",
      "Epoch 12/100\n",
      "2082/2082 - 60s - loss: 0.5752 - accuracy: 0.7578\n",
      "Epoch 13/100\n",
      "2082/2082 - 60s - loss: 0.5705 - accuracy: 0.7579\n",
      "Epoch 14/100\n",
      "2082/2082 - 60s - loss: 0.5658 - accuracy: 0.7587\n",
      "Epoch 15/100\n",
      "2082/2082 - 60s - loss: 0.5597 - accuracy: 0.7606\n",
      "Epoch 16/100\n",
      "2082/2082 - 60s - loss: 0.5547 - accuracy: 0.7619\n",
      "Epoch 17/100\n",
      "2082/2082 - 60s - loss: 0.5502 - accuracy: 0.7627\n",
      "Epoch 18/100\n",
      "2082/2082 - 60s - loss: 0.5450 - accuracy: 0.7651\n",
      "Epoch 19/100\n",
      "2082/2082 - 60s - loss: 0.5412 - accuracy: 0.7653\n",
      "Epoch 20/100\n",
      "2082/2082 - 60s - loss: 0.5364 - accuracy: 0.7660\n",
      "Epoch 21/100\n",
      "2082/2082 - 60s - loss: 0.5337 - accuracy: 0.7658\n",
      "Epoch 22/100\n",
      "2082/2082 - 60s - loss: 0.5285 - accuracy: 0.7661\n",
      "Epoch 23/100\n",
      "2082/2082 - 60s - loss: 0.5254 - accuracy: 0.7667\n",
      "Epoch 24/100\n",
      "2082/2082 - 60s - loss: 0.5223 - accuracy: 0.7682\n",
      "Epoch 25/100\n",
      "2082/2082 - 60s - loss: 0.5194 - accuracy: 0.7670\n",
      "Epoch 26/100\n",
      "2082/2082 - 60s - loss: 0.5166 - accuracy: 0.7688\n",
      "Epoch 27/100\n",
      "2082/2082 - 60s - loss: 0.5149 - accuracy: 0.7694\n",
      "Epoch 28/100\n",
      "2082/2082 - 59s - loss: 0.5127 - accuracy: 0.7686\n",
      "Epoch 29/100\n",
      "2082/2082 - 59s - loss: 0.5104 - accuracy: 0.7710\n",
      "Epoch 30/100\n",
      "2082/2082 - 59s - loss: 0.5090 - accuracy: 0.7709\n",
      "Epoch 31/100\n",
      "2082/2082 - 59s - loss: 0.5068 - accuracy: 0.7701\n",
      "Epoch 32/100\n",
      "2082/2082 - 59s - loss: 0.5054 - accuracy: 0.7712\n",
      "Epoch 33/100\n",
      "2082/2082 - 59s - loss: 0.5050 - accuracy: 0.7712\n",
      "Epoch 34/100\n",
      "2082/2082 - 60s - loss: 0.5030 - accuracy: 0.7718\n",
      "Epoch 35/100\n",
      "2082/2082 - 60s - loss: 0.5020 - accuracy: 0.7728\n",
      "Epoch 36/100\n",
      "2082/2082 - 60s - loss: 0.5008 - accuracy: 0.7717\n",
      "Epoch 37/100\n",
      "2082/2082 - 60s - loss: 0.4993 - accuracy: 0.7729\n",
      "Epoch 38/100\n",
      "2082/2082 - 60s - loss: 0.4989 - accuracy: 0.7747\n",
      "Epoch 39/100\n",
      "2082/2082 - 60s - loss: 0.4977 - accuracy: 0.7739\n",
      "Epoch 40/100\n",
      "2082/2082 - 59s - loss: 0.4977 - accuracy: 0.7732\n",
      "Epoch 41/100\n",
      "2082/2082 - 59s - loss: 0.4965 - accuracy: 0.7740\n",
      "Epoch 42/100\n",
      "2082/2082 - 60s - loss: 0.4955 - accuracy: 0.7750\n",
      "Epoch 43/100\n",
      "2082/2082 - 60s - loss: 0.4949 - accuracy: 0.7749\n",
      "Epoch 44/100\n",
      "2082/2082 - 60s - loss: 0.4938 - accuracy: 0.7752\n",
      "Epoch 45/100\n",
      "2082/2082 - 60s - loss: 0.4939 - accuracy: 0.7760\n",
      "Epoch 46/100\n",
      "2082/2082 - 60s - loss: 0.4926 - accuracy: 0.7766\n",
      "Epoch 47/100\n",
      "2082/2082 - 60s - loss: 0.4918 - accuracy: 0.7764\n",
      "Epoch 48/100\n",
      "2082/2082 - 61s - loss: 0.4916 - accuracy: 0.7756\n",
      "Epoch 49/100\n",
      "2082/2082 - 61s - loss: 0.4912 - accuracy: 0.7763\n",
      "Epoch 50/100\n",
      "2082/2082 - 61s - loss: 0.4905 - accuracy: 0.7766\n",
      "Epoch 51/100\n",
      "2082/2082 - 61s - loss: 0.4891 - accuracy: 0.7776\n",
      "Epoch 52/100\n",
      "2082/2082 - 61s - loss: 0.4894 - accuracy: 0.7773\n",
      "Epoch 53/100\n",
      "2082/2082 - 61s - loss: 0.4892 - accuracy: 0.7775\n",
      "Epoch 54/100\n",
      "2082/2082 - 61s - loss: 0.4878 - accuracy: 0.7790\n",
      "Epoch 55/100\n",
      "2082/2082 - 61s - loss: 0.4877 - accuracy: 0.7790\n",
      "Epoch 56/100\n",
      "2082/2082 - 61s - loss: 0.4874 - accuracy: 0.7779\n",
      "Epoch 57/100\n",
      "2082/2082 - 61s - loss: 0.4867 - accuracy: 0.7777\n",
      "Epoch 58/100\n",
      "2082/2082 - 59s - loss: 0.4860 - accuracy: 0.7791\n",
      "Epoch 59/100\n",
      "2082/2082 - 59s - loss: 0.4861 - accuracy: 0.7790\n",
      "Epoch 60/100\n",
      "2082/2082 - 60s - loss: 0.4852 - accuracy: 0.7796\n",
      "Epoch 61/100\n",
      "2082/2082 - 60s - loss: 0.4843 - accuracy: 0.7808\n",
      "Epoch 62/100\n",
      "2082/2082 - 61s - loss: 0.4830 - accuracy: 0.7809\n",
      "Epoch 63/100\n",
      "2082/2082 - 61s - loss: 0.4836 - accuracy: 0.7808\n",
      "Epoch 64/100\n",
      "2082/2082 - 61s - loss: 0.4830 - accuracy: 0.7817\n",
      "Epoch 65/100\n",
      "2082/2082 - 61s - loss: 0.4838 - accuracy: 0.7808\n",
      "Epoch 66/100\n",
      "2082/2082 - 61s - loss: 0.4822 - accuracy: 0.7808\n",
      "Epoch 67/100\n",
      "2082/2082 - 61s - loss: 0.4819 - accuracy: 0.7803\n",
      "Epoch 68/100\n",
      "2082/2082 - 61s - loss: 0.4809 - accuracy: 0.7813\n",
      "Epoch 69/100\n",
      "2082/2082 - 60s - loss: 0.4805 - accuracy: 0.7815\n",
      "Epoch 70/100\n",
      "2082/2082 - 61s - loss: 0.4806 - accuracy: 0.7824\n",
      "Epoch 71/100\n",
      "2082/2082 - 61s - loss: 0.4803 - accuracy: 0.7837\n",
      "Epoch 72/100\n",
      "2082/2082 - 61s - loss: 0.4796 - accuracy: 0.7835\n",
      "Epoch 73/100\n",
      "2082/2082 - 61s - loss: 0.4789 - accuracy: 0.7838\n",
      "Epoch 74/100\n",
      "2082/2082 - 61s - loss: 0.4792 - accuracy: 0.7829\n",
      "Epoch 75/100\n",
      "2082/2082 - 61s - loss: 0.4778 - accuracy: 0.7849\n",
      "Epoch 76/100\n",
      "2082/2082 - 62s - loss: 0.4777 - accuracy: 0.7849\n",
      "Epoch 77/100\n",
      "2082/2082 - 61s - loss: 0.4780 - accuracy: 0.7849\n",
      "Epoch 78/100\n",
      "2082/2082 - 61s - loss: 0.4777 - accuracy: 0.7841\n",
      "Epoch 79/100\n",
      "2082/2082 - 61s - loss: 0.4769 - accuracy: 0.7846\n",
      "Epoch 80/100\n",
      "2082/2082 - 60s - loss: 0.4769 - accuracy: 0.7853\n",
      "Epoch 81/100\n",
      "2082/2082 - 61s - loss: 0.4759 - accuracy: 0.7834\n",
      "Epoch 82/100\n",
      "2082/2082 - 61s - loss: 0.4760 - accuracy: 0.7852\n",
      "Epoch 83/100\n",
      "2082/2082 - 61s - loss: 0.4749 - accuracy: 0.7862\n",
      "Epoch 84/100\n",
      "2082/2082 - 61s - loss: 0.4751 - accuracy: 0.7852\n",
      "Epoch 85/100\n",
      "2082/2082 - 60s - loss: 0.4742 - accuracy: 0.7841\n",
      "Epoch 86/100\n",
      "2082/2082 - 60s - loss: 0.4744 - accuracy: 0.7863\n",
      "Epoch 87/100\n",
      "2082/2082 - 61s - loss: 0.4739 - accuracy: 0.7855\n",
      "Epoch 88/100\n",
      "2082/2082 - 61s - loss: 0.4733 - accuracy: 0.7870\n",
      "Epoch 89/100\n",
      "2082/2082 - 60s - loss: 0.4732 - accuracy: 0.7867\n",
      "Epoch 90/100\n",
      "2082/2082 - 61s - loss: 0.4729 - accuracy: 0.7867\n",
      "Epoch 91/100\n",
      "2082/2082 - 62s - loss: 0.4722 - accuracy: 0.7860\n",
      "Epoch 92/100\n",
      "2082/2082 - 61s - loss: 0.4714 - accuracy: 0.7870\n",
      "Epoch 93/100\n",
      "2082/2082 - 61s - loss: 0.4723 - accuracy: 0.7862\n",
      "Epoch 94/100\n",
      "2082/2082 - 61s - loss: 0.4705 - accuracy: 0.7879\n",
      "Epoch 95/100\n",
      "2082/2082 - 60s - loss: 0.4710 - accuracy: 0.7874\n",
      "Epoch 96/100\n",
      "2082/2082 - 61s - loss: 0.4709 - accuracy: 0.7868\n",
      "Epoch 97/100\n",
      "2082/2082 - 61s - loss: 0.4703 - accuracy: 0.7877\n",
      "Epoch 98/100\n",
      "2082/2082 - 61s - loss: 0.4697 - accuracy: 0.7881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/100\n",
      "2082/2082 - 61s - loss: 0.4692 - accuracy: 0.7880\n",
      "Epoch 100/100\n",
      "2082/2082 - 62s - loss: 0.4691 - accuracy: 0.7872\n",
      "417/417 - 3s - loss: 0.6017 - accuracy: 0.7418\n",
      "Epoch 1/100\n",
      "417/417 - 13s - loss: 0.8301 - accuracy: 0.7389\n",
      "Epoch 2/100\n",
      "417/417 - 12s - loss: 0.7542 - accuracy: 0.7389\n",
      "Epoch 3/100\n",
      "417/417 - 12s - loss: 0.7537 - accuracy: 0.7389\n",
      "Epoch 4/100\n",
      "417/417 - 12s - loss: 0.7531 - accuracy: 0.7389\n",
      "Epoch 5/100\n",
      "417/417 - 12s - loss: 0.7517 - accuracy: 0.7389\n",
      "Epoch 6/100\n",
      "417/417 - 12s - loss: 0.7504 - accuracy: 0.7389\n",
      "Epoch 7/100\n",
      "417/417 - 12s - loss: 0.7490 - accuracy: 0.7389\n",
      "Epoch 8/100\n",
      "417/417 - 12s - loss: 0.7466 - accuracy: 0.7389\n",
      "Epoch 9/100\n",
      "417/417 - 12s - loss: 0.7426 - accuracy: 0.7389\n",
      "Epoch 10/100\n",
      "417/417 - 12s - loss: 0.7364 - accuracy: 0.7389\n",
      "Epoch 11/100\n",
      "417/417 - 12s - loss: 0.7287 - accuracy: 0.7389\n",
      "Epoch 12/100\n",
      "417/417 - 12s - loss: 0.7163 - accuracy: 0.7389\n",
      "Epoch 13/100\n",
      "417/417 - 12s - loss: 0.7010 - accuracy: 0.7389\n",
      "Epoch 14/100\n",
      "417/417 - 12s - loss: 0.6809 - accuracy: 0.7389\n",
      "Epoch 15/100\n",
      "417/417 - 12s - loss: 0.6602 - accuracy: 0.7389\n",
      "Epoch 16/100\n",
      "417/417 - 12s - loss: 0.6447 - accuracy: 0.7401\n",
      "Epoch 17/100\n",
      "417/417 - 12s - loss: 0.6355 - accuracy: 0.7400\n",
      "Epoch 18/100\n",
      "417/417 - 12s - loss: 0.6269 - accuracy: 0.7419\n",
      "Epoch 19/100\n",
      "417/417 - 12s - loss: 0.6190 - accuracy: 0.7443\n",
      "Epoch 20/100\n",
      "417/417 - 12s - loss: 0.6145 - accuracy: 0.7470\n",
      "Epoch 21/100\n",
      "417/417 - 12s - loss: 0.6148 - accuracy: 0.7443\n",
      "Epoch 22/100\n",
      "417/417 - 12s - loss: 0.6075 - accuracy: 0.7467\n",
      "Epoch 23/100\n",
      "417/417 - 12s - loss: 0.6041 - accuracy: 0.7502\n",
      "Epoch 24/100\n",
      "417/417 - 12s - loss: 0.6046 - accuracy: 0.7512\n",
      "Epoch 25/100\n",
      "417/417 - 12s - loss: 0.5999 - accuracy: 0.7528\n",
      "Epoch 26/100\n",
      "417/417 - 12s - loss: 0.5985 - accuracy: 0.7536\n",
      "Epoch 27/100\n",
      "417/417 - 12s - loss: 0.5929 - accuracy: 0.7562\n",
      "Epoch 28/100\n",
      "417/417 - 12s - loss: 0.5902 - accuracy: 0.7575\n",
      "Epoch 29/100\n",
      "417/417 - 12s - loss: 0.5900 - accuracy: 0.7554\n",
      "Epoch 30/100\n",
      "417/417 - 12s - loss: 0.5879 - accuracy: 0.7548\n",
      "Epoch 31/100\n",
      "417/417 - 12s - loss: 0.5865 - accuracy: 0.7533\n",
      "Epoch 32/100\n",
      "417/417 - 12s - loss: 0.5846 - accuracy: 0.7582\n",
      "Epoch 33/100\n",
      "417/417 - 12s - loss: 0.5827 - accuracy: 0.7573\n",
      "Epoch 34/100\n",
      "417/417 - 12s - loss: 0.5832 - accuracy: 0.7579\n",
      "Epoch 35/100\n",
      "417/417 - 12s - loss: 0.5813 - accuracy: 0.7576\n",
      "Epoch 36/100\n",
      "417/417 - 12s - loss: 0.5799 - accuracy: 0.7590\n",
      "Epoch 37/100\n",
      "417/417 - 12s - loss: 0.5767 - accuracy: 0.7626\n",
      "Epoch 38/100\n",
      "417/417 - 12s - loss: 0.5756 - accuracy: 0.7622\n",
      "Epoch 39/100\n",
      "417/417 - 12s - loss: 0.5773 - accuracy: 0.7602\n",
      "Epoch 40/100\n",
      "417/417 - 12s - loss: 0.5734 - accuracy: 0.7632\n",
      "Epoch 41/100\n",
      "417/417 - 12s - loss: 0.5697 - accuracy: 0.7647\n",
      "Epoch 42/100\n",
      "417/417 - 12s - loss: 0.5714 - accuracy: 0.7638\n",
      "Epoch 43/100\n",
      "417/417 - 12s - loss: 0.5696 - accuracy: 0.7635\n",
      "Epoch 44/100\n",
      "417/417 - 12s - loss: 0.5678 - accuracy: 0.7637\n",
      "Epoch 45/100\n",
      "417/417 - 12s - loss: 0.5674 - accuracy: 0.7665\n",
      "Epoch 46/100\n",
      "417/417 - 12s - loss: 0.5644 - accuracy: 0.7656\n",
      "Epoch 47/100\n",
      "417/417 - 12s - loss: 0.5617 - accuracy: 0.7685\n",
      "Epoch 48/100\n",
      "417/417 - 12s - loss: 0.5621 - accuracy: 0.7666\n",
      "Epoch 49/100\n",
      "417/417 - 12s - loss: 0.5586 - accuracy: 0.7669\n",
      "Epoch 50/100\n",
      "417/417 - 12s - loss: 0.5559 - accuracy: 0.7691\n",
      "Epoch 51/100\n",
      "417/417 - 12s - loss: 0.5551 - accuracy: 0.7677\n",
      "Epoch 52/100\n",
      "417/417 - 12s - loss: 0.5534 - accuracy: 0.7703\n",
      "Epoch 53/100\n",
      "417/417 - 12s - loss: 0.5534 - accuracy: 0.7719\n",
      "Epoch 54/100\n",
      "417/417 - 12s - loss: 0.5504 - accuracy: 0.7729\n",
      "Epoch 55/100\n",
      "417/417 - 12s - loss: 0.5477 - accuracy: 0.7713\n",
      "Epoch 56/100\n",
      "417/417 - 12s - loss: 0.5473 - accuracy: 0.7744\n",
      "Epoch 57/100\n",
      "417/417 - 12s - loss: 0.5475 - accuracy: 0.7703\n",
      "Epoch 58/100\n",
      "417/417 - 12s - loss: 0.5449 - accuracy: 0.7721\n",
      "Epoch 59/100\n",
      "417/417 - 12s - loss: 0.5474 - accuracy: 0.7734\n",
      "Epoch 60/100\n",
      "417/417 - 12s - loss: 0.5431 - accuracy: 0.7750\n",
      "Epoch 61/100\n",
      "417/417 - 12s - loss: 0.5432 - accuracy: 0.7728\n",
      "Epoch 62/100\n",
      "417/417 - 12s - loss: 0.5378 - accuracy: 0.7783\n",
      "Epoch 63/100\n",
      "417/417 - 12s - loss: 0.5393 - accuracy: 0.7750\n",
      "Epoch 64/100\n",
      "417/417 - 12s - loss: 0.5384 - accuracy: 0.7741\n",
      "Epoch 65/100\n",
      "417/417 - 12s - loss: 0.5341 - accuracy: 0.7755\n",
      "Epoch 66/100\n",
      "417/417 - 12s - loss: 0.5376 - accuracy: 0.7768\n",
      "Epoch 67/100\n",
      "417/417 - 12s - loss: 0.5332 - accuracy: 0.7754\n",
      "Epoch 68/100\n",
      "417/417 - 12s - loss: 0.5357 - accuracy: 0.7763\n",
      "Epoch 69/100\n",
      "417/417 - 12s - loss: 0.5338 - accuracy: 0.7806\n",
      "Epoch 70/100\n",
      "417/417 - 12s - loss: 0.5336 - accuracy: 0.7775\n",
      "Epoch 71/100\n",
      "417/417 - 12s - loss: 0.5315 - accuracy: 0.7802\n",
      "Epoch 72/100\n",
      "417/417 - 12s - loss: 0.5311 - accuracy: 0.7764\n",
      "Epoch 73/100\n",
      "417/417 - 12s - loss: 0.5342 - accuracy: 0.7767\n",
      "Epoch 74/100\n",
      "417/417 - 12s - loss: 0.5318 - accuracy: 0.7760\n",
      "Epoch 75/100\n",
      "417/417 - 12s - loss: 0.5259 - accuracy: 0.7778\n",
      "Epoch 76/100\n",
      "417/417 - 12s - loss: 0.5285 - accuracy: 0.7767\n",
      "Epoch 77/100\n",
      "417/417 - 12s - loss: 0.5229 - accuracy: 0.7828\n",
      "Epoch 78/100\n",
      "417/417 - 12s - loss: 0.5227 - accuracy: 0.7798\n",
      "Epoch 79/100\n",
      "417/417 - 12s - loss: 0.5220 - accuracy: 0.7795\n",
      "Epoch 80/100\n",
      "417/417 - 12s - loss: 0.5205 - accuracy: 0.7823\n",
      "Epoch 81/100\n",
      "417/417 - 12s - loss: 0.5207 - accuracy: 0.7831\n",
      "Epoch 82/100\n",
      "417/417 - 12s - loss: 0.5211 - accuracy: 0.7801\n",
      "Epoch 83/100\n",
      "417/417 - 12s - loss: 0.5202 - accuracy: 0.7810\n",
      "Epoch 84/100\n",
      "417/417 - 12s - loss: 0.5177 - accuracy: 0.7846\n",
      "Epoch 85/100\n",
      "417/417 - 12s - loss: 0.5189 - accuracy: 0.7810\n",
      "Epoch 86/100\n",
      "417/417 - 12s - loss: 0.5221 - accuracy: 0.7810\n",
      "Epoch 87/100\n",
      "417/417 - 12s - loss: 0.5192 - accuracy: 0.7825\n",
      "Epoch 88/100\n",
      "417/417 - 12s - loss: 0.5190 - accuracy: 0.7822\n",
      "Epoch 89/100\n",
      "417/417 - 12s - loss: 0.5143 - accuracy: 0.7841\n",
      "Epoch 90/100\n",
      "417/417 - 12s - loss: 0.5150 - accuracy: 0.7825\n",
      "Epoch 91/100\n",
      "417/417 - 12s - loss: 0.5115 - accuracy: 0.7845\n",
      "Epoch 92/100\n",
      "417/417 - 12s - loss: 0.5145 - accuracy: 0.7843\n",
      "Epoch 93/100\n",
      "417/417 - 12s - loss: 0.5071 - accuracy: 0.7868\n",
      "Epoch 94/100\n",
      "417/417 - 12s - loss: 0.5109 - accuracy: 0.7883\n",
      "Epoch 95/100\n",
      "417/417 - 12s - loss: 0.5126 - accuracy: 0.7879\n",
      "Epoch 96/100\n",
      "417/417 - 12s - loss: 0.5119 - accuracy: 0.7853\n",
      "Epoch 97/100\n",
      "417/417 - 12s - loss: 0.5087 - accuracy: 0.7864\n",
      "Epoch 98/100\n",
      "417/417 - 12s - loss: 0.5094 - accuracy: 0.7855\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00098: early stopping\n",
      "417/417 - 3s - loss: 0.5591 - accuracy: 0.7692\n",
      "Epoch 1/100\n",
      "833/833 - 25s - loss: 0.8074 - accuracy: 0.7260\n",
      "Epoch 2/100\n",
      "833/833 - 24s - loss: 0.7519 - accuracy: 0.7396\n",
      "Epoch 3/100\n",
      "833/833 - 24s - loss: 0.7506 - accuracy: 0.7396\n",
      "Epoch 4/100\n",
      "833/833 - 24s - loss: 0.7480 - accuracy: 0.7396\n",
      "Epoch 5/100\n",
      "833/833 - 24s - loss: 0.7431 - accuracy: 0.7396\n",
      "Epoch 6/100\n",
      "833/833 - 24s - loss: 0.7339 - accuracy: 0.7396\n",
      "Epoch 7/100\n",
      "833/833 - 24s - loss: 0.7158 - accuracy: 0.7396\n",
      "Epoch 8/100\n",
      "833/833 - 24s - loss: 0.6848 - accuracy: 0.7396\n",
      "Epoch 9/100\n",
      "833/833 - 24s - loss: 0.6499 - accuracy: 0.7396\n",
      "Epoch 10/100\n",
      "833/833 - 24s - loss: 0.6261 - accuracy: 0.7413\n",
      "Epoch 11/100\n",
      "833/833 - 25s - loss: 0.6119 - accuracy: 0.7473\n",
      "Epoch 12/100\n",
      "833/833 - 25s - loss: 0.6046 - accuracy: 0.7522\n",
      "Epoch 13/100\n",
      "833/833 - 25s - loss: 0.5979 - accuracy: 0.7563\n",
      "Epoch 14/100\n",
      "833/833 - 25s - loss: 0.5946 - accuracy: 0.7569\n",
      "Epoch 15/100\n",
      "833/833 - 24s - loss: 0.5921 - accuracy: 0.7590\n",
      "Epoch 16/100\n",
      "833/833 - 24s - loss: 0.5883 - accuracy: 0.7597\n",
      "Epoch 17/100\n",
      "833/833 - 24s - loss: 0.5855 - accuracy: 0.7606\n",
      "Epoch 18/100\n",
      "833/833 - 24s - loss: 0.5832 - accuracy: 0.7624\n",
      "Epoch 19/100\n",
      "833/833 - 24s - loss: 0.5814 - accuracy: 0.7625\n",
      "Epoch 20/100\n",
      "833/833 - 24s - loss: 0.5787 - accuracy: 0.7631\n",
      "Epoch 21/100\n",
      "833/833 - 24s - loss: 0.5780 - accuracy: 0.7625\n",
      "Epoch 22/100\n",
      "833/833 - 24s - loss: 0.5759 - accuracy: 0.7647\n",
      "Epoch 23/100\n",
      "833/833 - 24s - loss: 0.5731 - accuracy: 0.7646\n",
      "Epoch 24/100\n",
      "833/833 - 24s - loss: 0.5712 - accuracy: 0.7676\n",
      "Epoch 25/100\n",
      "833/833 - 24s - loss: 0.5699 - accuracy: 0.7665\n",
      "Epoch 26/100\n",
      "833/833 - 24s - loss: 0.5690 - accuracy: 0.7647\n",
      "Epoch 27/100\n",
      "833/833 - 24s - loss: 0.5675 - accuracy: 0.7670\n",
      "Epoch 28/100\n",
      "833/833 - 24s - loss: 0.5655 - accuracy: 0.7672\n",
      "Epoch 29/100\n",
      "833/833 - 24s - loss: 0.5642 - accuracy: 0.7670\n",
      "Epoch 30/100\n",
      "833/833 - 24s - loss: 0.5639 - accuracy: 0.7679\n",
      "Epoch 31/100\n",
      "833/833 - 24s - loss: 0.5628 - accuracy: 0.7648\n",
      "Epoch 32/100\n",
      "833/833 - 24s - loss: 0.5603 - accuracy: 0.7670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100\n",
      "833/833 - 24s - loss: 0.5589 - accuracy: 0.7674\n",
      "Epoch 34/100\n",
      "833/833 - 24s - loss: 0.5585 - accuracy: 0.7679\n",
      "Epoch 35/100\n",
      "833/833 - 24s - loss: 0.5546 - accuracy: 0.7679\n",
      "Epoch 36/100\n",
      "833/833 - 24s - loss: 0.5564 - accuracy: 0.7664\n",
      "Epoch 37/100\n",
      "833/833 - 24s - loss: 0.5532 - accuracy: 0.7662\n",
      "Epoch 38/100\n",
      "833/833 - 24s - loss: 0.5526 - accuracy: 0.7684\n",
      "Epoch 39/100\n",
      "833/833 - 24s - loss: 0.5488 - accuracy: 0.7670\n",
      "Epoch 40/100\n",
      "833/833 - 24s - loss: 0.5483 - accuracy: 0.7683\n",
      "Epoch 41/100\n",
      "833/833 - 24s - loss: 0.5458 - accuracy: 0.7687\n",
      "Epoch 42/100\n",
      "833/833 - 24s - loss: 0.5461 - accuracy: 0.7679\n",
      "Epoch 43/100\n",
      "833/833 - 25s - loss: 0.5441 - accuracy: 0.7698\n",
      "Epoch 44/100\n",
      "833/833 - 24s - loss: 0.5420 - accuracy: 0.7701\n",
      "Epoch 45/100\n",
      "833/833 - 24s - loss: 0.5399 - accuracy: 0.7701\n",
      "Epoch 46/100\n",
      "833/833 - 24s - loss: 0.5387 - accuracy: 0.7691\n",
      "Epoch 47/100\n",
      "833/833 - 24s - loss: 0.5390 - accuracy: 0.7697\n",
      "Epoch 48/100\n",
      "833/833 - 25s - loss: 0.5359 - accuracy: 0.7716\n",
      "Epoch 49/100\n",
      "833/833 - 24s - loss: 0.5343 - accuracy: 0.7719\n",
      "Epoch 50/100\n",
      "833/833 - 24s - loss: 0.5344 - accuracy: 0.7729\n",
      "Epoch 51/100\n",
      "833/833 - 24s - loss: 0.5337 - accuracy: 0.7709\n",
      "Epoch 52/100\n",
      "833/833 - 24s - loss: 0.5317 - accuracy: 0.7742\n",
      "Epoch 53/100\n",
      "833/833 - 24s - loss: 0.5293 - accuracy: 0.7742\n",
      "Epoch 54/100\n",
      "833/833 - 24s - loss: 0.5291 - accuracy: 0.7714\n",
      "Epoch 55/100\n",
      "833/833 - 24s - loss: 0.5272 - accuracy: 0.7732\n",
      "Epoch 56/100\n",
      "833/833 - 24s - loss: 0.5271 - accuracy: 0.7731\n",
      "Epoch 57/100\n",
      "833/833 - 24s - loss: 0.5273 - accuracy: 0.7713\n",
      "Epoch 58/100\n",
      "833/833 - 24s - loss: 0.5254 - accuracy: 0.7746\n",
      "Epoch 59/100\n",
      "833/833 - 24s - loss: 0.5229 - accuracy: 0.7760\n",
      "Epoch 60/100\n",
      "833/833 - 24s - loss: 0.5240 - accuracy: 0.7762\n",
      "Epoch 61/100\n",
      "833/833 - 24s - loss: 0.5234 - accuracy: 0.7748\n",
      "Epoch 62/100\n",
      "833/833 - 24s - loss: 0.5227 - accuracy: 0.7728\n",
      "Epoch 63/100\n",
      "833/833 - 24s - loss: 0.5207 - accuracy: 0.7734\n",
      "Epoch 64/100\n",
      "833/833 - 24s - loss: 0.5206 - accuracy: 0.7762\n",
      "Epoch 65/100\n",
      "833/833 - 25s - loss: 0.5199 - accuracy: 0.7746\n",
      "Epoch 66/100\n",
      "833/833 - 25s - loss: 0.5198 - accuracy: 0.7750\n",
      "Epoch 67/100\n",
      "833/833 - 25s - loss: 0.5189 - accuracy: 0.7759\n",
      "Epoch 68/100\n",
      "833/833 - 25s - loss: 0.5183 - accuracy: 0.7770\n",
      "Epoch 69/100\n",
      "833/833 - 25s - loss: 0.5155 - accuracy: 0.7782\n",
      "Epoch 70/100\n",
      "833/833 - 25s - loss: 0.5164 - accuracy: 0.7754\n",
      "Epoch 71/100\n",
      "833/833 - 25s - loss: 0.5153 - accuracy: 0.7785\n",
      "Epoch 72/100\n",
      "833/833 - 25s - loss: 0.5157 - accuracy: 0.7782\n",
      "Epoch 73/100\n",
      "833/833 - 25s - loss: 0.5139 - accuracy: 0.7769\n",
      "Epoch 74/100\n",
      "833/833 - 25s - loss: 0.5144 - accuracy: 0.7785\n",
      "Epoch 75/100\n",
      "833/833 - 25s - loss: 0.5132 - accuracy: 0.7773\n",
      "Epoch 76/100\n",
      "833/833 - 25s - loss: 0.5130 - accuracy: 0.7778\n",
      "Epoch 77/100\n",
      "833/833 - 25s - loss: 0.5123 - accuracy: 0.7773\n",
      "Epoch 78/100\n",
      "833/833 - 25s - loss: 0.5115 - accuracy: 0.7773\n",
      "Epoch 79/100\n",
      "833/833 - 25s - loss: 0.5112 - accuracy: 0.7794\n",
      "Epoch 80/100\n",
      "833/833 - 25s - loss: 0.5110 - accuracy: 0.7787\n",
      "Epoch 81/100\n",
      "833/833 - 25s - loss: 0.5088 - accuracy: 0.7823\n",
      "Epoch 82/100\n",
      "833/833 - 25s - loss: 0.5089 - accuracy: 0.7785\n",
      "Epoch 83/100\n",
      "833/833 - 25s - loss: 0.5088 - accuracy: 0.7803\n",
      "Epoch 84/100\n",
      "833/833 - 25s - loss: 0.5083 - accuracy: 0.7797\n",
      "Epoch 85/100\n",
      "833/833 - 25s - loss: 0.5076 - accuracy: 0.7797\n",
      "Epoch 86/100\n",
      "833/833 - 25s - loss: 0.5069 - accuracy: 0.7805\n",
      "Epoch 87/100\n",
      "833/833 - 25s - loss: 0.5077 - accuracy: 0.7810\n",
      "Epoch 88/100\n",
      "833/833 - 25s - loss: 0.5063 - accuracy: 0.7805\n",
      "Epoch 89/100\n",
      "833/833 - 25s - loss: 0.5054 - accuracy: 0.7807\n",
      "Epoch 90/100\n",
      "833/833 - 25s - loss: 0.5061 - accuracy: 0.7784\n",
      "Epoch 91/100\n",
      "833/833 - 25s - loss: 0.5060 - accuracy: 0.7828\n",
      "Epoch 92/100\n",
      "833/833 - 25s - loss: 0.5039 - accuracy: 0.7815\n",
      "Epoch 93/100\n",
      "833/833 - 25s - loss: 0.5052 - accuracy: 0.7817\n",
      "Epoch 94/100\n",
      "833/833 - 25s - loss: 0.5047 - accuracy: 0.7811\n",
      "Epoch 95/100\n",
      "833/833 - 25s - loss: 0.5027 - accuracy: 0.7810\n",
      "Epoch 96/100\n",
      "833/833 - 25s - loss: 0.5017 - accuracy: 0.7819\n",
      "Epoch 97/100\n",
      "833/833 - 25s - loss: 0.5026 - accuracy: 0.7817\n",
      "Epoch 98/100\n",
      "833/833 - 25s - loss: 0.5013 - accuracy: 0.7821\n",
      "Epoch 99/100\n",
      "833/833 - 25s - loss: 0.5011 - accuracy: 0.7814\n",
      "Epoch 100/100\n",
      "833/833 - 25s - loss: 0.5010 - accuracy: 0.7833\n",
      "417/417 - 3s - loss: 0.5123 - accuracy: 0.7717\n",
      "Epoch 1/100\n",
      "1249/1249 - 33s - loss: 0.7714 - accuracy: 0.7439\n",
      "Epoch 2/100\n",
      "1249/1249 - 33s - loss: 0.7438 - accuracy: 0.7440\n",
      "Epoch 3/100\n",
      "1249/1249 - 32s - loss: 0.7398 - accuracy: 0.7440\n",
      "Epoch 4/100\n",
      "1249/1249 - 34s - loss: 0.7292 - accuracy: 0.7440\n",
      "Epoch 5/100\n",
      "1249/1249 - 33s - loss: 0.6989 - accuracy: 0.7440\n",
      "Epoch 6/100\n",
      "1249/1249 - 33s - loss: 0.6466 - accuracy: 0.7440\n",
      "Epoch 7/100\n",
      "1249/1249 - 33s - loss: 0.6124 - accuracy: 0.7456\n",
      "Epoch 8/100\n",
      "1249/1249 - 33s - loss: 0.6000 - accuracy: 0.7526\n",
      "Epoch 9/100\n",
      "1249/1249 - 33s - loss: 0.5922 - accuracy: 0.7569\n",
      "Epoch 10/100\n",
      "1249/1249 - 33s - loss: 0.5850 - accuracy: 0.7600\n",
      "Epoch 11/100\n",
      "1249/1249 - 33s - loss: 0.5816 - accuracy: 0.7606\n",
      "Epoch 12/100\n",
      "1249/1249 - 33s - loss: 0.5782 - accuracy: 0.7622\n",
      "Epoch 13/100\n",
      "1249/1249 - 33s - loss: 0.5750 - accuracy: 0.7631\n",
      "Epoch 14/100\n",
      "1249/1249 - 33s - loss: 0.5719 - accuracy: 0.7628\n",
      "Epoch 15/100\n",
      "1249/1249 - 33s - loss: 0.5713 - accuracy: 0.7634\n",
      "Epoch 16/100\n",
      "1249/1249 - 33s - loss: 0.5690 - accuracy: 0.7641\n",
      "Epoch 17/100\n",
      "1249/1249 - 33s - loss: 0.5670 - accuracy: 0.7638\n",
      "Epoch 18/100\n",
      "1249/1249 - 33s - loss: 0.5643 - accuracy: 0.7648\n",
      "Epoch 19/100\n",
      "1249/1249 - 33s - loss: 0.5617 - accuracy: 0.7658\n",
      "Epoch 20/100\n",
      "1249/1249 - 33s - loss: 0.5593 - accuracy: 0.7661\n",
      "Epoch 21/100\n",
      "1249/1249 - 33s - loss: 0.5599 - accuracy: 0.7649\n",
      "Epoch 22/100\n",
      "1249/1249 - 33s - loss: 0.5550 - accuracy: 0.7668\n",
      "Epoch 23/100\n",
      "1249/1249 - 33s - loss: 0.5547 - accuracy: 0.7669\n",
      "Epoch 24/100\n",
      "1249/1249 - 33s - loss: 0.5518 - accuracy: 0.7676\n",
      "Epoch 25/100\n",
      "1249/1249 - 33s - loss: 0.5476 - accuracy: 0.7676\n",
      "Epoch 26/100\n",
      "1249/1249 - 33s - loss: 0.5452 - accuracy: 0.7689\n",
      "Epoch 27/100\n",
      "1249/1249 - 33s - loss: 0.5434 - accuracy: 0.7684\n",
      "Epoch 28/100\n",
      "1249/1249 - 33s - loss: 0.5458 - accuracy: 0.7694\n",
      "Epoch 29/100\n",
      "1249/1249 - 33s - loss: 0.5404 - accuracy: 0.7695\n",
      "Epoch 30/100\n",
      "1249/1249 - 33s - loss: 0.5382 - accuracy: 0.7713\n",
      "Epoch 31/100\n",
      "1249/1249 - 33s - loss: 0.5350 - accuracy: 0.7724\n",
      "Epoch 32/100\n",
      "1249/1249 - 33s - loss: 0.5346 - accuracy: 0.7719\n",
      "Epoch 33/100\n",
      "1249/1249 - 33s - loss: 0.5322 - accuracy: 0.7730\n",
      "Epoch 34/100\n",
      "1249/1249 - 33s - loss: 0.5302 - accuracy: 0.7728\n",
      "Epoch 35/100\n",
      "1249/1249 - 33s - loss: 0.5291 - accuracy: 0.7727\n",
      "Epoch 36/100\n",
      "1249/1249 - 33s - loss: 0.5285 - accuracy: 0.7721\n",
      "Epoch 37/100\n",
      "1249/1249 - 33s - loss: 0.5256 - accuracy: 0.7731\n",
      "Epoch 38/100\n",
      "1249/1249 - 33s - loss: 0.5247 - accuracy: 0.7744\n",
      "Epoch 39/100\n",
      "1249/1249 - 33s - loss: 0.5238 - accuracy: 0.7727\n",
      "Epoch 40/100\n",
      "1249/1249 - 33s - loss: 0.5222 - accuracy: 0.7750\n",
      "Epoch 41/100\n",
      "1249/1249 - 33s - loss: 0.5202 - accuracy: 0.7741\n",
      "Epoch 42/100\n",
      "1249/1249 - 33s - loss: 0.5198 - accuracy: 0.7755\n",
      "Epoch 43/100\n",
      "1249/1249 - 35s - loss: 0.5180 - accuracy: 0.7759\n",
      "Epoch 44/100\n",
      "1249/1249 - 33s - loss: 0.5176 - accuracy: 0.7743\n",
      "Epoch 45/100\n",
      "1249/1249 - 33s - loss: 0.5154 - accuracy: 0.7766\n",
      "Epoch 46/100\n",
      "1249/1249 - 33s - loss: 0.5152 - accuracy: 0.7762\n",
      "Epoch 47/100\n",
      "1249/1249 - 33s - loss: 0.5136 - accuracy: 0.7753\n",
      "Epoch 48/100\n",
      "1249/1249 - 33s - loss: 0.5129 - accuracy: 0.7763\n",
      "Epoch 49/100\n",
      "1249/1249 - 33s - loss: 0.5128 - accuracy: 0.7771\n",
      "Epoch 50/100\n",
      "1249/1249 - 33s - loss: 0.5106 - accuracy: 0.7773\n",
      "Epoch 51/100\n",
      "1249/1249 - 33s - loss: 0.5123 - accuracy: 0.7764\n",
      "Epoch 52/100\n",
      "1249/1249 - 33s - loss: 0.5113 - accuracy: 0.7785\n",
      "Epoch 53/100\n",
      "1249/1249 - 33s - loss: 0.5096 - accuracy: 0.7772\n",
      "Epoch 54/100\n",
      "1249/1249 - 33s - loss: 0.5075 - accuracy: 0.7778\n",
      "Epoch 55/100\n",
      "1249/1249 - 33s - loss: 0.5083 - accuracy: 0.7772\n",
      "Epoch 56/100\n",
      "1249/1249 - 33s - loss: 0.5058 - accuracy: 0.7789\n",
      "Epoch 57/100\n",
      "1249/1249 - 33s - loss: 0.5058 - accuracy: 0.7785\n",
      "Epoch 58/100\n",
      "1249/1249 - 33s - loss: 0.5068 - accuracy: 0.7788\n",
      "Epoch 59/100\n",
      "1249/1249 - 33s - loss: 0.5043 - accuracy: 0.7803\n",
      "Epoch 60/100\n",
      "1249/1249 - 33s - loss: 0.5041 - accuracy: 0.7797\n",
      "Epoch 61/100\n",
      "1249/1249 - 33s - loss: 0.5035 - accuracy: 0.7792\n",
      "Epoch 62/100\n",
      "1249/1249 - 33s - loss: 0.5026 - accuracy: 0.7785\n",
      "Epoch 63/100\n",
      "1249/1249 - 34s - loss: 0.5025 - accuracy: 0.7783\n",
      "Epoch 64/100\n",
      "1249/1249 - 34s - loss: 0.5018 - accuracy: 0.7800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/100\n",
      "1249/1249 - 36s - loss: 0.5016 - accuracy: 0.7795\n",
      "Epoch 66/100\n",
      "1249/1249 - 33s - loss: 0.5003 - accuracy: 0.7798\n",
      "Epoch 67/100\n",
      "1249/1249 - 33s - loss: 0.5003 - accuracy: 0.7803\n",
      "Epoch 68/100\n",
      "1249/1249 - 36s - loss: 0.4996 - accuracy: 0.7789\n",
      "Epoch 69/100\n",
      "1249/1249 - 34s - loss: 0.4983 - accuracy: 0.7812\n",
      "Epoch 70/100\n",
      "1249/1249 - 33s - loss: 0.4975 - accuracy: 0.7805\n",
      "Epoch 71/100\n",
      "1249/1249 - 33s - loss: 0.4978 - accuracy: 0.7814\n",
      "Epoch 72/100\n",
      "1249/1249 - 33s - loss: 0.4972 - accuracy: 0.7827\n",
      "Epoch 73/100\n",
      "1249/1249 - 33s - loss: 0.4970 - accuracy: 0.7811\n",
      "Epoch 74/100\n",
      "1249/1249 - 33s - loss: 0.4955 - accuracy: 0.7807\n",
      "Epoch 75/100\n",
      "1249/1249 - 33s - loss: 0.4968 - accuracy: 0.7807\n",
      "Epoch 76/100\n",
      "1249/1249 - 34s - loss: 0.4962 - accuracy: 0.7805\n",
      "Epoch 77/100\n",
      "1249/1249 - 33s - loss: 0.4950 - accuracy: 0.7825\n",
      "Epoch 78/100\n",
      "1249/1249 - 34s - loss: 0.4943 - accuracy: 0.7800\n",
      "Epoch 79/100\n",
      "1249/1249 - 33s - loss: 0.4935 - accuracy: 0.7819\n",
      "Epoch 80/100\n",
      "1249/1249 - 35s - loss: 0.4942 - accuracy: 0.7826\n",
      "Epoch 81/100\n",
      "1249/1249 - 34s - loss: 0.4931 - accuracy: 0.7831\n",
      "Epoch 82/100\n",
      "1249/1249 - 33s - loss: 0.4934 - accuracy: 0.7799\n",
      "Epoch 83/100\n",
      "1249/1249 - 33s - loss: 0.4935 - accuracy: 0.7808\n",
      "Epoch 84/100\n",
      "1249/1249 - 33s - loss: 0.4906 - accuracy: 0.7837\n",
      "Epoch 85/100\n",
      "1249/1249 - 33s - loss: 0.4918 - accuracy: 0.7833\n",
      "Epoch 86/100\n",
      "1249/1249 - 33s - loss: 0.4915 - accuracy: 0.7824\n",
      "Epoch 87/100\n",
      "1249/1249 - 33s - loss: 0.4907 - accuracy: 0.7848\n",
      "Epoch 88/100\n",
      "1249/1249 - 33s - loss: 0.4900 - accuracy: 0.7841\n",
      "Epoch 89/100\n",
      "1249/1249 - 33s - loss: 0.4899 - accuracy: 0.7836\n",
      "Epoch 90/100\n",
      "1249/1249 - 33s - loss: 0.4897 - accuracy: 0.7840\n",
      "Epoch 91/100\n",
      "1249/1249 - 34s - loss: 0.4889 - accuracy: 0.7843\n",
      "Epoch 92/100\n",
      "1249/1249 - 33s - loss: 0.4895 - accuracy: 0.7837\n",
      "Epoch 93/100\n",
      "1249/1249 - 33s - loss: 0.4885 - accuracy: 0.7849\n",
      "Epoch 94/100\n",
      "1249/1249 - 34s - loss: 0.4878 - accuracy: 0.7840\n",
      "Epoch 95/100\n",
      "1249/1249 - 34s - loss: 0.4880 - accuracy: 0.7847\n",
      "Epoch 96/100\n",
      "1249/1249 - 33s - loss: 0.4886 - accuracy: 0.7843\n",
      "Epoch 97/100\n",
      "1249/1249 - 33s - loss: 0.4874 - accuracy: 0.7845\n",
      "Epoch 98/100\n",
      "1249/1249 - 33s - loss: 0.4873 - accuracy: 0.7828\n",
      "Epoch 99/100\n",
      "1249/1249 - 33s - loss: 0.4866 - accuracy: 0.7848\n",
      "Epoch 100/100\n",
      "1249/1249 - 33s - loss: 0.4862 - accuracy: 0.7846\n",
      "417/417 - 3s - loss: 0.5292 - accuracy: 0.7608\n",
      "Epoch 1/100\n",
      "1666/1666 - 49s - loss: 0.7767 - accuracy: 0.7412\n",
      "Epoch 2/100\n",
      "1666/1666 - 48s - loss: 0.7501 - accuracy: 0.7413\n",
      "Epoch 3/100\n",
      "1666/1666 - 48s - loss: 0.7486 - accuracy: 0.7413\n",
      "Epoch 4/100\n",
      "1666/1666 - 48s - loss: 0.7418 - accuracy: 0.7413\n",
      "Epoch 5/100\n",
      "1666/1666 - 48s - loss: 0.6980 - accuracy: 0.7413\n",
      "Epoch 6/100\n",
      "1666/1666 - 48s - loss: 0.6363 - accuracy: 0.7414\n",
      "Epoch 7/100\n",
      "1666/1666 - 48s - loss: 0.6208 - accuracy: 0.7443\n",
      "Epoch 8/100\n",
      "1666/1666 - 48s - loss: 0.6102 - accuracy: 0.7501\n",
      "Epoch 9/100\n",
      "1666/1666 - 48s - loss: 0.6026 - accuracy: 0.7522\n",
      "Epoch 10/100\n",
      "1666/1666 - 48s - loss: 0.5967 - accuracy: 0.7565\n",
      "Epoch 11/100\n",
      "1666/1666 - 48s - loss: 0.5929 - accuracy: 0.7563\n",
      "Epoch 12/100\n",
      "1666/1666 - 48s - loss: 0.5887 - accuracy: 0.7563\n",
      "Epoch 13/100\n",
      "1666/1666 - 48s - loss: 0.5834 - accuracy: 0.7594\n",
      "Epoch 14/100\n",
      "1666/1666 - 48s - loss: 0.5792 - accuracy: 0.7611\n",
      "Epoch 15/100\n",
      "1666/1666 - 48s - loss: 0.5738 - accuracy: 0.7614\n",
      "Epoch 16/100\n",
      "1666/1666 - 48s - loss: 0.5673 - accuracy: 0.7610\n",
      "Epoch 17/100\n",
      "1666/1666 - 48s - loss: 0.5602 - accuracy: 0.7636\n",
      "Epoch 18/100\n",
      "1666/1666 - 48s - loss: 0.5543 - accuracy: 0.7650\n",
      "Epoch 19/100\n",
      "1666/1666 - 48s - loss: 0.5501 - accuracy: 0.7648\n",
      "Epoch 20/100\n",
      "1666/1666 - 48s - loss: 0.5440 - accuracy: 0.7666\n",
      "Epoch 21/100\n",
      "1666/1666 - 48s - loss: 0.5408 - accuracy: 0.7664\n",
      "Epoch 22/100\n",
      "1666/1666 - 48s - loss: 0.5364 - accuracy: 0.7667\n",
      "Epoch 23/100\n",
      "1666/1666 - 48s - loss: 0.5352 - accuracy: 0.7680\n",
      "Epoch 24/100\n",
      "1666/1666 - 48s - loss: 0.5321 - accuracy: 0.7694\n",
      "Epoch 25/100\n",
      "1666/1666 - 48s - loss: 0.5297 - accuracy: 0.7683\n",
      "Epoch 26/100\n",
      "1666/1666 - 48s - loss: 0.5286 - accuracy: 0.7682\n",
      "Epoch 27/100\n",
      "1666/1666 - 48s - loss: 0.5266 - accuracy: 0.7680\n",
      "Epoch 28/100\n",
      "1666/1666 - 48s - loss: 0.5242 - accuracy: 0.7693\n",
      "Epoch 29/100\n",
      "1666/1666 - 48s - loss: 0.5216 - accuracy: 0.7709\n",
      "Epoch 30/100\n",
      "1666/1666 - 48s - loss: 0.5200 - accuracy: 0.7708\n",
      "Epoch 31/100\n",
      "1666/1666 - 48s - loss: 0.5197 - accuracy: 0.7695\n",
      "Epoch 32/100\n",
      "1666/1666 - 48s - loss: 0.5174 - accuracy: 0.7725\n",
      "Epoch 33/100\n",
      "1666/1666 - 48s - loss: 0.5177 - accuracy: 0.7708\n",
      "Epoch 34/100\n",
      "1666/1666 - 48s - loss: 0.5152 - accuracy: 0.7724\n",
      "Epoch 35/100\n",
      "1666/1666 - 48s - loss: 0.5146 - accuracy: 0.7725\n",
      "Epoch 36/100\n",
      "1666/1666 - 48s - loss: 0.5136 - accuracy: 0.7714\n",
      "Epoch 37/100\n",
      "1666/1666 - 48s - loss: 0.5123 - accuracy: 0.7723\n",
      "Epoch 38/100\n",
      "1666/1666 - 48s - loss: 0.5111 - accuracy: 0.7723\n",
      "Epoch 39/100\n",
      "1666/1666 - 48s - loss: 0.5108 - accuracy: 0.7731\n",
      "Epoch 40/100\n",
      "1666/1666 - 48s - loss: 0.5101 - accuracy: 0.7724\n",
      "Epoch 41/100\n",
      "1666/1666 - 48s - loss: 0.5078 - accuracy: 0.7739\n",
      "Epoch 42/100\n",
      "1666/1666 - 48s - loss: 0.5082 - accuracy: 0.7729\n",
      "Epoch 43/100\n",
      "1666/1666 - 48s - loss: 0.5064 - accuracy: 0.7747\n",
      "Epoch 44/100\n",
      "1666/1666 - 48s - loss: 0.5068 - accuracy: 0.7737\n",
      "Epoch 45/100\n",
      "1666/1666 - 48s - loss: 0.5062 - accuracy: 0.7741\n",
      "Epoch 46/100\n",
      "1666/1666 - 48s - loss: 0.5056 - accuracy: 0.7734\n",
      "Epoch 47/100\n",
      "1666/1666 - 48s - loss: 0.5037 - accuracy: 0.7738\n",
      "Epoch 48/100\n",
      "1666/1666 - 48s - loss: 0.5030 - accuracy: 0.7752\n",
      "Epoch 49/100\n",
      "1666/1666 - 48s - loss: 0.5032 - accuracy: 0.7740\n",
      "Epoch 50/100\n",
      "1666/1666 - 48s - loss: 0.5021 - accuracy: 0.7749\n",
      "Epoch 51/100\n",
      "1666/1666 - 48s - loss: 0.5009 - accuracy: 0.7747\n",
      "Epoch 52/100\n",
      "1666/1666 - 48s - loss: 0.5017 - accuracy: 0.7770\n",
      "Epoch 53/100\n",
      "1666/1666 - 48s - loss: 0.5011 - accuracy: 0.7741\n",
      "Epoch 54/100\n",
      "1666/1666 - 48s - loss: 0.5002 - accuracy: 0.7757\n",
      "Epoch 55/100\n",
      "1666/1666 - 49s - loss: 0.4989 - accuracy: 0.7758\n",
      "Epoch 56/100\n",
      "1666/1666 - 49s - loss: 0.4987 - accuracy: 0.7761\n",
      "Epoch 57/100\n",
      "1666/1666 - 50s - loss: 0.4991 - accuracy: 0.7763\n",
      "Epoch 58/100\n",
      "1666/1666 - 49s - loss: 0.4975 - accuracy: 0.7775\n",
      "Epoch 59/100\n",
      "1666/1666 - 49s - loss: 0.4975 - accuracy: 0.7771\n",
      "Epoch 60/100\n",
      "1666/1666 - 49s - loss: 0.4960 - accuracy: 0.7762\n",
      "Epoch 61/100\n",
      "1666/1666 - 49s - loss: 0.4956 - accuracy: 0.7768\n",
      "Epoch 62/100\n",
      "1666/1666 - 49s - loss: 0.4951 - accuracy: 0.7773\n",
      "Epoch 63/100\n",
      "1666/1666 - 49s - loss: 0.4952 - accuracy: 0.7786\n",
      "Epoch 64/100\n",
      "1666/1666 - 49s - loss: 0.4942 - accuracy: 0.7803\n",
      "Epoch 65/100\n",
      "1666/1666 - 49s - loss: 0.4939 - accuracy: 0.7779\n",
      "Epoch 66/100\n",
      "1666/1666 - 49s - loss: 0.4937 - accuracy: 0.7780\n",
      "Epoch 67/100\n",
      "1666/1666 - 49s - loss: 0.4924 - accuracy: 0.7794\n",
      "Epoch 68/100\n",
      "1666/1666 - 49s - loss: 0.4915 - accuracy: 0.7796\n",
      "Epoch 69/100\n",
      "1666/1666 - 49s - loss: 0.4920 - accuracy: 0.7802\n",
      "Epoch 70/100\n",
      "1666/1666 - 49s - loss: 0.4913 - accuracy: 0.7801\n",
      "Epoch 71/100\n",
      "1666/1666 - 49s - loss: 0.4912 - accuracy: 0.7793\n",
      "Epoch 72/100\n",
      "1666/1666 - 49s - loss: 0.4903 - accuracy: 0.7803\n",
      "Epoch 73/100\n",
      "1666/1666 - 49s - loss: 0.4899 - accuracy: 0.7794\n",
      "Epoch 74/100\n",
      "1666/1666 - 49s - loss: 0.4888 - accuracy: 0.7803\n",
      "Epoch 75/100\n",
      "1666/1666 - 49s - loss: 0.4892 - accuracy: 0.7816\n",
      "Epoch 76/100\n",
      "1666/1666 - 49s - loss: 0.4887 - accuracy: 0.7794\n",
      "Epoch 77/100\n",
      "1666/1666 - 49s - loss: 0.4879 - accuracy: 0.7806\n",
      "Epoch 78/100\n",
      "1666/1666 - 49s - loss: 0.4878 - accuracy: 0.7817\n",
      "Epoch 79/100\n",
      "1666/1666 - 49s - loss: 0.4876 - accuracy: 0.7798\n",
      "Epoch 80/100\n",
      "1666/1666 - 49s - loss: 0.4868 - accuracy: 0.7812\n",
      "Epoch 81/100\n",
      "1666/1666 - 49s - loss: 0.4866 - accuracy: 0.7820\n",
      "Epoch 82/100\n",
      "1666/1666 - 49s - loss: 0.4851 - accuracy: 0.7819\n",
      "Epoch 83/100\n",
      "1666/1666 - 49s - loss: 0.4848 - accuracy: 0.7814\n",
      "Epoch 84/100\n",
      "1666/1666 - 49s - loss: 0.4860 - accuracy: 0.7829\n",
      "Epoch 85/100\n",
      "1666/1666 - 49s - loss: 0.4854 - accuracy: 0.7828\n",
      "Epoch 86/100\n",
      "1666/1666 - 49s - loss: 0.4839 - accuracy: 0.7830\n",
      "Epoch 87/100\n",
      "1666/1666 - 49s - loss: 0.4841 - accuracy: 0.7819\n",
      "Epoch 88/100\n",
      "1666/1666 - 49s - loss: 0.4840 - accuracy: 0.7840\n",
      "Epoch 89/100\n",
      "1666/1666 - 49s - loss: 0.4835 - accuracy: 0.7843\n",
      "Epoch 90/100\n",
      "1666/1666 - 49s - loss: 0.4816 - accuracy: 0.7850\n",
      "Epoch 91/100\n",
      "1666/1666 - 49s - loss: 0.4824 - accuracy: 0.7836\n",
      "Epoch 92/100\n",
      "1666/1666 - 49s - loss: 0.4818 - accuracy: 0.7839\n",
      "Epoch 93/100\n",
      "1666/1666 - 49s - loss: 0.4812 - accuracy: 0.7841\n",
      "Epoch 94/100\n",
      "1666/1666 - 49s - loss: 0.4807 - accuracy: 0.7845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/100\n",
      "1666/1666 - 49s - loss: 0.4805 - accuracy: 0.7849\n",
      "Epoch 96/100\n",
      "1666/1666 - 49s - loss: 0.4802 - accuracy: 0.7853\n",
      "Epoch 97/100\n",
      "1666/1666 - 49s - loss: 0.4798 - accuracy: 0.7865\n",
      "Epoch 98/100\n",
      "1666/1666 - 49s - loss: 0.4800 - accuracy: 0.7847\n",
      "Epoch 99/100\n",
      "1666/1666 - 52s - loss: 0.4782 - accuracy: 0.7848\n",
      "Epoch 100/100\n",
      "1666/1666 - 49s - loss: 0.4783 - accuracy: 0.7847\n",
      "417/417 - 3s - loss: 0.4963 - accuracy: 0.7645\n",
      "Epoch 1/100\n",
      "2082/2082 - 62s - loss: 0.7694 - accuracy: 0.7405\n",
      "Epoch 2/100\n",
      "2082/2082 - 61s - loss: 0.7468 - accuracy: 0.7405\n",
      "Epoch 3/100\n",
      "2082/2082 - 61s - loss: 0.7308 - accuracy: 0.7405\n",
      "Epoch 4/100\n",
      "2082/2082 - 60s - loss: 0.6805 - accuracy: 0.7406\n",
      "Epoch 5/100\n",
      "2082/2082 - 61s - loss: 0.6215 - accuracy: 0.7455\n",
      "Epoch 6/100\n",
      "2082/2082 - 61s - loss: 0.6027 - accuracy: 0.7515\n",
      "Epoch 7/100\n",
      "2082/2082 - 61s - loss: 0.5948 - accuracy: 0.7533\n",
      "Epoch 8/100\n",
      "2082/2082 - 61s - loss: 0.5894 - accuracy: 0.7548\n",
      "Epoch 9/100\n",
      "2082/2082 - 61s - loss: 0.5848 - accuracy: 0.7557\n",
      "Epoch 10/100\n",
      "2082/2082 - 60s - loss: 0.5797 - accuracy: 0.7572\n",
      "Epoch 11/100\n",
      "2082/2082 - 60s - loss: 0.5763 - accuracy: 0.7561\n",
      "Epoch 12/100\n",
      "2082/2082 - 60s - loss: 0.5719 - accuracy: 0.7590\n",
      "Epoch 13/100\n",
      "2082/2082 - 60s - loss: 0.5684 - accuracy: 0.7573\n",
      "Epoch 14/100\n",
      "2082/2082 - 60s - loss: 0.5631 - accuracy: 0.7591\n",
      "Epoch 15/100\n",
      "2082/2082 - 60s - loss: 0.5585 - accuracy: 0.7603\n",
      "Epoch 16/100\n",
      "2082/2082 - 61s - loss: 0.5537 - accuracy: 0.7614\n",
      "Epoch 17/100\n",
      "2082/2082 - 60s - loss: 0.5499 - accuracy: 0.7622\n",
      "Epoch 18/100\n",
      "2082/2082 - 61s - loss: 0.5465 - accuracy: 0.7621\n",
      "Epoch 19/100\n",
      "2082/2082 - 60s - loss: 0.5435 - accuracy: 0.7616\n",
      "Epoch 20/100\n",
      "2082/2082 - 61s - loss: 0.5395 - accuracy: 0.7644\n",
      "Epoch 21/100\n",
      "2082/2082 - 61s - loss: 0.5363 - accuracy: 0.7649\n",
      "Epoch 22/100\n",
      "2082/2082 - 61s - loss: 0.5335 - accuracy: 0.7640\n",
      "Epoch 23/100\n",
      "2082/2082 - 61s - loss: 0.5321 - accuracy: 0.7657\n",
      "Epoch 24/100\n",
      "2082/2082 - 61s - loss: 0.5287 - accuracy: 0.7654\n",
      "Epoch 25/100\n",
      "2082/2082 - 61s - loss: 0.5267 - accuracy: 0.7649\n",
      "Epoch 26/100\n",
      "2082/2082 - 61s - loss: 0.5242 - accuracy: 0.7660\n",
      "Epoch 27/100\n",
      "2082/2082 - 61s - loss: 0.5227 - accuracy: 0.7677\n",
      "Epoch 28/100\n",
      "2082/2082 - 61s - loss: 0.5208 - accuracy: 0.7672\n",
      "Epoch 29/100\n",
      "2082/2082 - 61s - loss: 0.5203 - accuracy: 0.7658\n",
      "Epoch 30/100\n",
      "2082/2082 - 61s - loss: 0.5177 - accuracy: 0.7670\n",
      "Epoch 31/100\n",
      "2082/2082 - 61s - loss: 0.5155 - accuracy: 0.7680\n",
      "Epoch 32/100\n",
      "2082/2082 - 61s - loss: 0.5143 - accuracy: 0.7697\n",
      "Epoch 33/100\n",
      "2082/2082 - 61s - loss: 0.5132 - accuracy: 0.7683\n",
      "Epoch 34/100\n",
      "2082/2082 - 61s - loss: 0.5120 - accuracy: 0.7691\n",
      "Epoch 35/100\n",
      "2082/2082 - 61s - loss: 0.5110 - accuracy: 0.7682\n",
      "Epoch 36/100\n",
      "2082/2082 - 61s - loss: 0.5102 - accuracy: 0.7676\n",
      "Epoch 37/100\n",
      "2082/2082 - 61s - loss: 0.5093 - accuracy: 0.7693\n",
      "Epoch 38/100\n",
      "2082/2082 - 61s - loss: 0.5087 - accuracy: 0.7709\n",
      "Epoch 39/100\n",
      "2082/2082 - 61s - loss: 0.5071 - accuracy: 0.7703\n",
      "Epoch 40/100\n",
      "2082/2082 - 61s - loss: 0.5058 - accuracy: 0.7706\n",
      "Epoch 41/100\n",
      "2082/2082 - 61s - loss: 0.5062 - accuracy: 0.7692\n",
      "Epoch 42/100\n",
      "2082/2082 - 61s - loss: 0.5047 - accuracy: 0.7714\n",
      "Epoch 43/100\n",
      "2082/2082 - 61s - loss: 0.5041 - accuracy: 0.7718\n",
      "Epoch 44/100\n",
      "2082/2082 - 65s - loss: 0.5026 - accuracy: 0.7722\n",
      "Epoch 45/100\n",
      "2082/2082 - 61s - loss: 0.5028 - accuracy: 0.7710\n",
      "Epoch 46/100\n",
      "2082/2082 - 61s - loss: 0.5022 - accuracy: 0.7721\n",
      "Epoch 47/100\n",
      "2082/2082 - 61s - loss: 0.5011 - accuracy: 0.7731\n",
      "Epoch 48/100\n",
      "2082/2082 - 61s - loss: 0.5003 - accuracy: 0.7726\n",
      "Epoch 49/100\n",
      "2082/2082 - 61s - loss: 0.5002 - accuracy: 0.7730\n",
      "Epoch 50/100\n",
      "2082/2082 - 61s - loss: 0.4990 - accuracy: 0.7734\n",
      "Epoch 51/100\n",
      "2082/2082 - 61s - loss: 0.4993 - accuracy: 0.7731\n",
      "Epoch 52/100\n",
      "2082/2082 - 61s - loss: 0.4984 - accuracy: 0.7738\n",
      "Epoch 53/100\n",
      "2082/2082 - 61s - loss: 0.4980 - accuracy: 0.7736\n",
      "Epoch 54/100\n",
      "2082/2082 - 61s - loss: 0.4979 - accuracy: 0.7736\n",
      "Epoch 55/100\n",
      "2082/2082 - 61s - loss: 0.4970 - accuracy: 0.7745\n",
      "Epoch 56/100\n",
      "2082/2082 - 61s - loss: 0.4968 - accuracy: 0.7754\n",
      "Epoch 57/100\n",
      "2082/2082 - 61s - loss: 0.4960 - accuracy: 0.7737\n",
      "Epoch 58/100\n",
      "2082/2082 - 61s - loss: 0.4955 - accuracy: 0.7751\n",
      "Epoch 59/100\n",
      "2082/2082 - 61s - loss: 0.4954 - accuracy: 0.7742\n",
      "Epoch 60/100\n",
      "2082/2082 - 61s - loss: 0.4953 - accuracy: 0.7758\n",
      "Epoch 61/100\n",
      "2082/2082 - 61s - loss: 0.4948 - accuracy: 0.7747\n",
      "Epoch 62/100\n",
      "2082/2082 - 61s - loss: 0.4942 - accuracy: 0.7747\n",
      "Epoch 63/100\n",
      "2082/2082 - 61s - loss: 0.4940 - accuracy: 0.7750\n",
      "Epoch 64/100\n",
      "2082/2082 - 61s - loss: 0.4934 - accuracy: 0.7764\n",
      "Epoch 65/100\n",
      "2082/2082 - 61s - loss: 0.4936 - accuracy: 0.7760\n",
      "Epoch 66/100\n",
      "2082/2082 - 61s - loss: 0.4920 - accuracy: 0.7770\n",
      "Epoch 67/100\n",
      "2082/2082 - 61s - loss: 0.4919 - accuracy: 0.7772\n",
      "Epoch 68/100\n",
      "2082/2082 - 61s - loss: 0.4911 - accuracy: 0.7784\n",
      "Epoch 69/100\n",
      "2082/2082 - 61s - loss: 0.4910 - accuracy: 0.7782\n",
      "Epoch 70/100\n",
      "2082/2082 - 61s - loss: 0.4911 - accuracy: 0.7766\n",
      "Epoch 71/100\n",
      "2082/2082 - 61s - loss: 0.4905 - accuracy: 0.7791\n",
      "Epoch 72/100\n",
      "2082/2082 - 61s - loss: 0.4895 - accuracy: 0.7792\n",
      "Epoch 73/100\n",
      "2082/2082 - 61s - loss: 0.4902 - accuracy: 0.7797\n",
      "Epoch 74/100\n",
      "2082/2082 - 61s - loss: 0.4894 - accuracy: 0.7783\n",
      "Epoch 75/100\n",
      "2082/2082 - 61s - loss: 0.4888 - accuracy: 0.7787\n",
      "Epoch 76/100\n",
      "2082/2082 - 61s - loss: 0.4881 - accuracy: 0.7796\n",
      "Epoch 77/100\n",
      "2082/2082 - 61s - loss: 0.4874 - accuracy: 0.7797\n",
      "Epoch 78/100\n",
      "2082/2082 - 61s - loss: 0.4879 - accuracy: 0.7791\n",
      "Epoch 79/100\n",
      "2082/2082 - 61s - loss: 0.4873 - accuracy: 0.7801\n",
      "Epoch 80/100\n",
      "2082/2082 - 61s - loss: 0.4872 - accuracy: 0.7810\n",
      "Epoch 81/100\n",
      "2082/2082 - 61s - loss: 0.4874 - accuracy: 0.7799\n",
      "Epoch 82/100\n",
      "2082/2082 - 61s - loss: 0.4856 - accuracy: 0.7813\n",
      "Epoch 83/100\n",
      "2082/2082 - 61s - loss: 0.4854 - accuracy: 0.7811\n",
      "Epoch 84/100\n",
      "2082/2082 - 61s - loss: 0.4865 - accuracy: 0.7808\n",
      "Epoch 85/100\n",
      "2082/2082 - 61s - loss: 0.4860 - accuracy: 0.7819\n",
      "Epoch 86/100\n",
      "2082/2082 - 61s - loss: 0.4853 - accuracy: 0.7814\n",
      "Epoch 87/100\n",
      "2082/2082 - 61s - loss: 0.4846 - accuracy: 0.7818\n",
      "Epoch 88/100\n",
      "2082/2082 - 61s - loss: 0.4842 - accuracy: 0.7822\n",
      "Epoch 89/100\n",
      "2082/2082 - 61s - loss: 0.4837 - accuracy: 0.7828\n",
      "Epoch 90/100\n",
      "2082/2082 - 61s - loss: 0.4835 - accuracy: 0.7829\n",
      "Epoch 91/100\n",
      "2082/2082 - 61s - loss: 0.4833 - accuracy: 0.7824\n",
      "Epoch 92/100\n",
      "2082/2082 - 61s - loss: 0.4835 - accuracy: 0.7819\n",
      "Epoch 93/100\n",
      "2082/2082 - 61s - loss: 0.4826 - accuracy: 0.7841\n",
      "Epoch 94/100\n",
      "2082/2082 - 61s - loss: 0.4823 - accuracy: 0.7832\n",
      "Epoch 95/100\n",
      "2082/2082 - 61s - loss: 0.4825 - accuracy: 0.7826\n",
      "Epoch 96/100\n",
      "2082/2082 - 61s - loss: 0.4822 - accuracy: 0.7838\n",
      "Epoch 97/100\n",
      "2082/2082 - 61s - loss: 0.4822 - accuracy: 0.7829\n",
      "Epoch 98/100\n",
      "2082/2082 - 62s - loss: 0.4814 - accuracy: 0.7843\n",
      "Epoch 99/100\n",
      "2082/2082 - 61s - loss: 0.4810 - accuracy: 0.7839\n",
      "Epoch 100/100\n",
      "2082/2082 - 61s - loss: 0.4804 - accuracy: 0.7854\n",
      "417/417 - 3s - loss: 0.6021 - accuracy: 0.7451\n",
      "Epoch 1/100\n",
      "417/417 - 13s - loss: 0.8231 - accuracy: 0.7359\n",
      "Epoch 2/100\n",
      "417/417 - 12s - loss: 0.7549 - accuracy: 0.7389\n",
      "Epoch 3/100\n",
      "417/417 - 12s - loss: 0.7556 - accuracy: 0.7389\n",
      "Epoch 4/100\n",
      "417/417 - 12s - loss: 0.7544 - accuracy: 0.7389\n",
      "Epoch 5/100\n",
      "417/417 - 12s - loss: 0.7545 - accuracy: 0.7389\n",
      "Epoch 6/100\n",
      "417/417 - 12s - loss: 0.7535 - accuracy: 0.7389\n",
      "Epoch 7/100\n",
      "417/417 - 12s - loss: 0.7525 - accuracy: 0.7389\n",
      "Epoch 8/100\n",
      "417/417 - 12s - loss: 0.7521 - accuracy: 0.7389\n",
      "Epoch 9/100\n",
      "417/417 - 12s - loss: 0.7499 - accuracy: 0.7389\n",
      "Epoch 10/100\n",
      "417/417 - 12s - loss: 0.7483 - accuracy: 0.7389\n",
      "Epoch 11/100\n",
      "417/417 - 12s - loss: 0.7459 - accuracy: 0.7389\n",
      "Epoch 12/100\n",
      "417/417 - 13s - loss: 0.7416 - accuracy: 0.7389\n",
      "Epoch 13/100\n",
      "417/417 - 12s - loss: 0.7352 - accuracy: 0.7389\n",
      "Epoch 14/100\n",
      "417/417 - 12s - loss: 0.7257 - accuracy: 0.7389\n",
      "Epoch 15/100\n",
      "417/417 - 12s - loss: 0.7136 - accuracy: 0.7389\n",
      "Epoch 16/100\n",
      "417/417 - 12s - loss: 0.6966 - accuracy: 0.7389\n",
      "Epoch 17/100\n",
      "417/417 - 12s - loss: 0.6790 - accuracy: 0.7389\n",
      "Epoch 18/100\n",
      "417/417 - 12s - loss: 0.6613 - accuracy: 0.7387\n",
      "Epoch 19/100\n",
      "417/417 - 12s - loss: 0.6501 - accuracy: 0.7404\n",
      "Epoch 20/100\n",
      "417/417 - 12s - loss: 0.6392 - accuracy: 0.7428\n",
      "Epoch 21/100\n",
      "417/417 - 12s - loss: 0.6307 - accuracy: 0.7440\n",
      "Epoch 22/100\n",
      "417/417 - 12s - loss: 0.6270 - accuracy: 0.7479\n",
      "Epoch 23/100\n",
      "417/417 - 12s - loss: 0.6205 - accuracy: 0.7487\n",
      "Epoch 24/100\n",
      "417/417 - 12s - loss: 0.6183 - accuracy: 0.7505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100\n",
      "417/417 - 12s - loss: 0.6150 - accuracy: 0.7524\n",
      "Epoch 26/100\n",
      "417/417 - 12s - loss: 0.6120 - accuracy: 0.7495\n",
      "Epoch 27/100\n",
      "417/417 - 12s - loss: 0.6102 - accuracy: 0.7518\n",
      "Epoch 28/100\n",
      "417/417 - 12s - loss: 0.6077 - accuracy: 0.7525\n",
      "Epoch 29/100\n",
      "417/417 - 12s - loss: 0.6060 - accuracy: 0.7560\n",
      "Epoch 30/100\n",
      "417/417 - 12s - loss: 0.6021 - accuracy: 0.7551\n",
      "Epoch 31/100\n",
      "417/417 - 12s - loss: 0.6037 - accuracy: 0.7551\n",
      "Epoch 32/100\n",
      "417/417 - 12s - loss: 0.6015 - accuracy: 0.7537\n",
      "Epoch 33/100\n",
      "417/417 - 12s - loss: 0.5979 - accuracy: 0.7541\n",
      "Epoch 34/100\n",
      "417/417 - 12s - loss: 0.5968 - accuracy: 0.7557\n",
      "Epoch 35/100\n",
      "417/417 - 12s - loss: 0.5958 - accuracy: 0.7588\n",
      "Epoch 36/100\n",
      "417/417 - 12s - loss: 0.5930 - accuracy: 0.7565\n",
      "Epoch 37/100\n",
      "417/417 - 12s - loss: 0.5901 - accuracy: 0.7585\n",
      "Epoch 38/100\n",
      "417/417 - 12s - loss: 0.5858 - accuracy: 0.7611\n",
      "Epoch 39/100\n",
      "417/417 - 12s - loss: 0.5858 - accuracy: 0.7608\n",
      "Epoch 40/100\n",
      "417/417 - 12s - loss: 0.5801 - accuracy: 0.7614\n",
      "Epoch 41/100\n",
      "417/417 - 12s - loss: 0.5769 - accuracy: 0.7626\n",
      "Epoch 42/100\n",
      "417/417 - 12s - loss: 0.5770 - accuracy: 0.7629\n",
      "Epoch 43/100\n",
      "417/417 - 12s - loss: 0.5766 - accuracy: 0.7596\n",
      "Epoch 44/100\n",
      "417/417 - 12s - loss: 0.5767 - accuracy: 0.7641\n",
      "Epoch 45/100\n",
      "417/417 - 12s - loss: 0.5697 - accuracy: 0.7638\n",
      "Epoch 46/100\n",
      "417/417 - 12s - loss: 0.5704 - accuracy: 0.7641\n",
      "Epoch 47/100\n",
      "417/417 - 12s - loss: 0.5685 - accuracy: 0.7647\n",
      "Epoch 48/100\n",
      "417/417 - 12s - loss: 0.5691 - accuracy: 0.7641\n",
      "Epoch 49/100\n",
      "417/417 - 12s - loss: 0.5680 - accuracy: 0.7674\n",
      "Epoch 50/100\n",
      "417/417 - 12s - loss: 0.5630 - accuracy: 0.7670\n",
      "Epoch 51/100\n",
      "417/417 - 12s - loss: 0.5644 - accuracy: 0.7656\n",
      "Epoch 52/100\n",
      "417/417 - 12s - loss: 0.5648 - accuracy: 0.7660\n",
      "Epoch 53/100\n",
      "417/417 - 12s - loss: 0.5600 - accuracy: 0.7697\n",
      "Epoch 54/100\n",
      "417/417 - 12s - loss: 0.5576 - accuracy: 0.7719\n",
      "Epoch 55/100\n",
      "417/417 - 12s - loss: 0.5605 - accuracy: 0.7670\n",
      "Epoch 56/100\n",
      "417/417 - 12s - loss: 0.5592 - accuracy: 0.7689\n",
      "Epoch 57/100\n",
      "417/417 - 12s - loss: 0.5598 - accuracy: 0.7671\n",
      "Epoch 58/100\n",
      "417/417 - 12s - loss: 0.5574 - accuracy: 0.7682\n",
      "Epoch 59/100\n",
      "417/417 - 12s - loss: 0.5546 - accuracy: 0.7698\n",
      "Epoch 60/100\n",
      "417/417 - 12s - loss: 0.5552 - accuracy: 0.7694\n",
      "Epoch 61/100\n",
      "417/417 - 12s - loss: 0.5539 - accuracy: 0.7704\n",
      "Epoch 62/100\n",
      "417/417 - 12s - loss: 0.5530 - accuracy: 0.7711\n",
      "Epoch 63/100\n",
      "417/417 - 12s - loss: 0.5501 - accuracy: 0.7721\n",
      "Epoch 64/100\n",
      "417/417 - 12s - loss: 0.5511 - accuracy: 0.7707\n",
      "Epoch 65/100\n",
      "417/417 - 12s - loss: 0.5486 - accuracy: 0.7710\n",
      "Epoch 66/100\n",
      "417/417 - 12s - loss: 0.5492 - accuracy: 0.7710\n",
      "Epoch 67/100\n",
      "417/417 - 12s - loss: 0.5480 - accuracy: 0.7706\n",
      "Epoch 68/100\n",
      "417/417 - 12s - loss: 0.5487 - accuracy: 0.7722\n",
      "Epoch 69/100\n",
      "417/417 - 12s - loss: 0.5446 - accuracy: 0.7722\n",
      "Epoch 70/100\n",
      "417/417 - 12s - loss: 0.5450 - accuracy: 0.7732\n",
      "Epoch 71/100\n",
      "417/417 - 12s - loss: 0.5430 - accuracy: 0.7744\n",
      "Epoch 72/100\n",
      "417/417 - 12s - loss: 0.5439 - accuracy: 0.7749\n",
      "Epoch 73/100\n",
      "417/417 - 12s - loss: 0.5414 - accuracy: 0.7725\n",
      "Epoch 74/100\n",
      "417/417 - 12s - loss: 0.5398 - accuracy: 0.7722\n",
      "Epoch 75/100\n",
      "417/417 - 12s - loss: 0.5393 - accuracy: 0.7753\n",
      "Epoch 76/100\n",
      "417/417 - 12s - loss: 0.5396 - accuracy: 0.7713\n",
      "Epoch 77/100\n",
      "417/417 - 12s - loss: 0.5380 - accuracy: 0.7744\n",
      "Epoch 78/100\n",
      "417/417 - 12s - loss: 0.5361 - accuracy: 0.7753\n",
      "Epoch 79/100\n",
      "417/417 - 12s - loss: 0.5367 - accuracy: 0.7755\n",
      "Epoch 80/100\n",
      "417/417 - 12s - loss: 0.5357 - accuracy: 0.7742\n",
      "Epoch 81/100\n",
      "417/417 - 12s - loss: 0.5350 - accuracy: 0.7750\n",
      "Epoch 82/100\n",
      "417/417 - 12s - loss: 0.5306 - accuracy: 0.7783\n",
      "Epoch 83/100\n",
      "417/417 - 12s - loss: 0.5345 - accuracy: 0.7735\n",
      "Epoch 84/100\n",
      "417/417 - 12s - loss: 0.5325 - accuracy: 0.7763\n",
      "Epoch 85/100\n",
      "417/417 - 12s - loss: 0.5311 - accuracy: 0.7741\n",
      "Epoch 86/100\n",
      "417/417 - 12s - loss: 0.5308 - accuracy: 0.7756\n",
      "Epoch 87/100\n",
      "417/417 - 12s - loss: 0.5330 - accuracy: 0.7744\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00087: early stopping\n",
      "417/417 - 3s - loss: 0.5521 - accuracy: 0.7750\n",
      "Epoch 1/100\n",
      "833/833 - 25s - loss: 0.7986 - accuracy: 0.7358\n",
      "Epoch 2/100\n",
      "833/833 - 24s - loss: 0.7534 - accuracy: 0.7396\n",
      "Epoch 3/100\n",
      "833/833 - 24s - loss: 0.7526 - accuracy: 0.7396\n",
      "Epoch 4/100\n",
      "833/833 - 24s - loss: 0.7513 - accuracy: 0.7396\n",
      "Epoch 5/100\n",
      "833/833 - 24s - loss: 0.7491 - accuracy: 0.7396\n",
      "Epoch 6/100\n",
      "833/833 - 24s - loss: 0.7443 - accuracy: 0.7396\n",
      "Epoch 7/100\n",
      "833/833 - 24s - loss: 0.7318 - accuracy: 0.7396\n",
      "Epoch 8/100\n",
      "833/833 - 24s - loss: 0.7046 - accuracy: 0.7396\n",
      "Epoch 9/100\n",
      "833/833 - 24s - loss: 0.6577 - accuracy: 0.7395\n",
      "Epoch 10/100\n",
      "833/833 - 24s - loss: 0.6281 - accuracy: 0.7411\n",
      "Epoch 11/100\n",
      "833/833 - 24s - loss: 0.6170 - accuracy: 0.7426\n",
      "Epoch 12/100\n",
      "833/833 - 24s - loss: 0.6083 - accuracy: 0.7483\n",
      "Epoch 13/100\n",
      "833/833 - 24s - loss: 0.6039 - accuracy: 0.7508\n",
      "Epoch 14/100\n",
      "833/833 - 24s - loss: 0.5995 - accuracy: 0.7516\n",
      "Epoch 15/100\n",
      "833/833 - 24s - loss: 0.5956 - accuracy: 0.7552\n",
      "Epoch 16/100\n",
      "833/833 - 24s - loss: 0.5926 - accuracy: 0.7557\n",
      "Epoch 17/100\n",
      "833/833 - 24s - loss: 0.5902 - accuracy: 0.7573\n",
      "Epoch 18/100\n",
      "833/833 - 24s - loss: 0.5881 - accuracy: 0.7610\n",
      "Epoch 19/100\n",
      "833/833 - 24s - loss: 0.5843 - accuracy: 0.7599\n",
      "Epoch 20/100\n",
      "833/833 - 24s - loss: 0.5808 - accuracy: 0.7628\n",
      "Epoch 21/100\n",
      "833/833 - 24s - loss: 0.5782 - accuracy: 0.7625\n",
      "Epoch 22/100\n",
      "833/833 - 24s - loss: 0.5766 - accuracy: 0.7641\n",
      "Epoch 23/100\n",
      "833/833 - 24s - loss: 0.5733 - accuracy: 0.7651\n",
      "Epoch 24/100\n",
      "833/833 - 24s - loss: 0.5697 - accuracy: 0.7659\n",
      "Epoch 25/100\n",
      "833/833 - 24s - loss: 0.5673 - accuracy: 0.7672\n",
      "Epoch 26/100\n",
      "833/833 - 24s - loss: 0.5663 - accuracy: 0.7659\n",
      "Epoch 27/100\n",
      "833/833 - 24s - loss: 0.5638 - accuracy: 0.7662\n",
      "Epoch 28/100\n",
      "833/833 - 24s - loss: 0.5626 - accuracy: 0.7677\n",
      "Epoch 29/100\n",
      "833/833 - 24s - loss: 0.5590 - accuracy: 0.7688\n",
      "Epoch 30/100\n",
      "833/833 - 24s - loss: 0.5567 - accuracy: 0.7682\n",
      "Epoch 31/100\n",
      "833/833 - 24s - loss: 0.5533 - accuracy: 0.7706\n",
      "Epoch 32/100\n",
      "833/833 - 24s - loss: 0.5532 - accuracy: 0.7669\n",
      "Epoch 33/100\n",
      "833/833 - 24s - loss: 0.5489 - accuracy: 0.7716\n",
      "Epoch 34/100\n",
      "833/833 - 24s - loss: 0.5449 - accuracy: 0.7707\n",
      "Epoch 35/100\n",
      "833/833 - 24s - loss: 0.5445 - accuracy: 0.7712\n",
      "Epoch 36/100\n",
      "833/833 - 24s - loss: 0.5424 - accuracy: 0.7716\n",
      "Epoch 37/100\n",
      "833/833 - 24s - loss: 0.5395 - accuracy: 0.7722\n",
      "Epoch 38/100\n",
      "833/833 - 24s - loss: 0.5377 - accuracy: 0.7751\n",
      "Epoch 39/100\n",
      "833/833 - 24s - loss: 0.5367 - accuracy: 0.7727\n",
      "Epoch 40/100\n",
      "833/833 - 24s - loss: 0.5360 - accuracy: 0.7734\n",
      "Epoch 41/100\n",
      "833/833 - 24s - loss: 0.5345 - accuracy: 0.7746\n",
      "Epoch 42/100\n",
      "833/833 - 24s - loss: 0.5315 - accuracy: 0.7755\n",
      "Epoch 43/100\n",
      "833/833 - 24s - loss: 0.5301 - accuracy: 0.7749\n",
      "Epoch 44/100\n",
      "833/833 - 24s - loss: 0.5304 - accuracy: 0.7737\n",
      "Epoch 45/100\n",
      "833/833 - 24s - loss: 0.5270 - accuracy: 0.7766\n",
      "Epoch 46/100\n",
      "833/833 - 24s - loss: 0.5282 - accuracy: 0.7764\n",
      "Epoch 47/100\n",
      "833/833 - 25s - loss: 0.5269 - accuracy: 0.7737\n",
      "Epoch 48/100\n",
      "833/833 - 24s - loss: 0.5254 - accuracy: 0.7757\n",
      "Epoch 49/100\n",
      "833/833 - 24s - loss: 0.5250 - accuracy: 0.7758\n",
      "Epoch 50/100\n",
      "833/833 - 24s - loss: 0.5231 - accuracy: 0.7789\n",
      "Epoch 51/100\n",
      "833/833 - 24s - loss: 0.5215 - accuracy: 0.7759\n",
      "Epoch 52/100\n",
      "833/833 - 24s - loss: 0.5209 - accuracy: 0.7741\n",
      "Epoch 53/100\n",
      "833/833 - 24s - loss: 0.5180 - accuracy: 0.7781\n",
      "Epoch 54/100\n",
      "833/833 - 24s - loss: 0.5190 - accuracy: 0.7754\n",
      "Epoch 55/100\n",
      "833/833 - 24s - loss: 0.5199 - accuracy: 0.7764\n",
      "Epoch 56/100\n",
      "833/833 - 24s - loss: 0.5168 - accuracy: 0.7764\n",
      "Epoch 57/100\n",
      "833/833 - 24s - loss: 0.5167 - accuracy: 0.7785\n",
      "Epoch 58/100\n",
      "833/833 - 25s - loss: 0.5149 - accuracy: 0.7799\n",
      "Epoch 59/100\n",
      "833/833 - 24s - loss: 0.5152 - accuracy: 0.7774\n",
      "Epoch 60/100\n",
      "833/833 - 24s - loss: 0.5142 - accuracy: 0.7776\n",
      "Epoch 61/100\n",
      "833/833 - 24s - loss: 0.5137 - accuracy: 0.7778\n",
      "Epoch 62/100\n",
      "833/833 - 29s - loss: 0.5129 - accuracy: 0.7805\n",
      "Epoch 63/100\n",
      "833/833 - 24s - loss: 0.5123 - accuracy: 0.7798\n",
      "Epoch 64/100\n",
      "833/833 - 24s - loss: 0.5107 - accuracy: 0.7804\n",
      "Epoch 65/100\n",
      "833/833 - 24s - loss: 0.5098 - accuracy: 0.7792\n",
      "Epoch 66/100\n",
      "833/833 - 24s - loss: 0.5087 - accuracy: 0.7805\n",
      "Epoch 67/100\n",
      "833/833 - 24s - loss: 0.5076 - accuracy: 0.7805\n",
      "Epoch 68/100\n",
      "833/833 - 24s - loss: 0.5078 - accuracy: 0.7796\n",
      "Epoch 69/100\n",
      "833/833 - 24s - loss: 0.5053 - accuracy: 0.7819\n",
      "Epoch 70/100\n",
      "833/833 - 24s - loss: 0.5062 - accuracy: 0.7811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/100\n",
      "833/833 - 24s - loss: 0.5055 - accuracy: 0.7829\n",
      "Epoch 72/100\n",
      "833/833 - 24s - loss: 0.5038 - accuracy: 0.7805\n",
      "Epoch 73/100\n",
      "833/833 - 24s - loss: 0.5039 - accuracy: 0.7828\n",
      "Epoch 74/100\n",
      "833/833 - 24s - loss: 0.5028 - accuracy: 0.7836\n",
      "Epoch 75/100\n",
      "833/833 - 24s - loss: 0.5028 - accuracy: 0.7819\n",
      "Epoch 76/100\n",
      "833/833 - 24s - loss: 0.5012 - accuracy: 0.7824\n",
      "Epoch 77/100\n",
      "833/833 - 24s - loss: 0.5022 - accuracy: 0.7826\n",
      "Epoch 78/100\n",
      "833/833 - 24s - loss: 0.5003 - accuracy: 0.7828\n",
      "Epoch 79/100\n",
      "833/833 - 24s - loss: 0.5004 - accuracy: 0.7824\n",
      "Epoch 80/100\n",
      "833/833 - 24s - loss: 0.5020 - accuracy: 0.7829\n",
      "Epoch 81/100\n",
      "833/833 - 24s - loss: 0.5010 - accuracy: 0.7820\n",
      "Epoch 82/100\n",
      "833/833 - 24s - loss: 0.4981 - accuracy: 0.7851\n",
      "Epoch 83/100\n",
      "833/833 - 24s - loss: 0.4975 - accuracy: 0.7846\n",
      "Epoch 84/100\n",
      "833/833 - 24s - loss: 0.4980 - accuracy: 0.7853\n",
      "Epoch 85/100\n",
      "833/833 - 24s - loss: 0.4977 - accuracy: 0.7837\n",
      "Epoch 86/100\n",
      "833/833 - 24s - loss: 0.4962 - accuracy: 0.7866\n",
      "Epoch 87/100\n",
      "833/833 - 24s - loss: 0.4948 - accuracy: 0.7843\n",
      "Epoch 88/100\n",
      "833/833 - 24s - loss: 0.4951 - accuracy: 0.7838\n",
      "Epoch 89/100\n",
      "833/833 - 24s - loss: 0.4959 - accuracy: 0.7850\n",
      "Epoch 90/100\n",
      "833/833 - 24s - loss: 0.4941 - accuracy: 0.7867\n",
      "Epoch 91/100\n",
      "833/833 - 24s - loss: 0.4929 - accuracy: 0.7857\n",
      "Epoch 92/100\n",
      "833/833 - 24s - loss: 0.4954 - accuracy: 0.7857\n",
      "Epoch 93/100\n",
      "833/833 - 24s - loss: 0.4935 - accuracy: 0.7845\n",
      "Epoch 94/100\n",
      "833/833 - 24s - loss: 0.4930 - accuracy: 0.7858\n",
      "Epoch 95/100\n",
      "833/833 - 24s - loss: 0.4932 - accuracy: 0.7864\n",
      "Epoch 96/100\n",
      "833/833 - 24s - loss: 0.4907 - accuracy: 0.7884\n",
      "Epoch 97/100\n",
      "833/833 - 24s - loss: 0.4909 - accuracy: 0.7882\n",
      "Epoch 98/100\n",
      "833/833 - 25s - loss: 0.4910 - accuracy: 0.7857\n",
      "Epoch 99/100\n",
      "833/833 - 24s - loss: 0.4892 - accuracy: 0.7868\n",
      "Epoch 100/100\n",
      "833/833 - 24s - loss: 0.4898 - accuracy: 0.7863\n",
      "417/417 - 3s - loss: 0.5152 - accuracy: 0.7758\n",
      "Epoch 1/100\n",
      "1249/1249 - 33s - loss: 0.7904 - accuracy: 0.7282\n",
      "Epoch 2/100\n",
      "1249/1249 - 32s - loss: 0.7429 - accuracy: 0.7440\n",
      "Epoch 3/100\n",
      "1249/1249 - 32s - loss: 0.7373 - accuracy: 0.7440\n",
      "Epoch 4/100\n",
      "1249/1249 - 32s - loss: 0.7153 - accuracy: 0.7440\n",
      "Epoch 5/100\n",
      "1249/1249 - 32s - loss: 0.6564 - accuracy: 0.7439\n",
      "Epoch 6/100\n",
      "1249/1249 - 32s - loss: 0.6155 - accuracy: 0.7478\n",
      "Epoch 7/100\n",
      "1249/1249 - 32s - loss: 0.6014 - accuracy: 0.7507\n",
      "Epoch 8/100\n",
      "1249/1249 - 32s - loss: 0.5927 - accuracy: 0.7538\n",
      "Epoch 9/100\n",
      "1249/1249 - 32s - loss: 0.5887 - accuracy: 0.7567\n",
      "Epoch 10/100\n",
      "1249/1249 - 32s - loss: 0.5844 - accuracy: 0.7595\n",
      "Epoch 11/100\n",
      "1249/1249 - 32s - loss: 0.5808 - accuracy: 0.7593\n",
      "Epoch 12/100\n",
      "1249/1249 - 32s - loss: 0.5778 - accuracy: 0.7616\n",
      "Epoch 13/100\n",
      "1249/1249 - 32s - loss: 0.5743 - accuracy: 0.7630\n",
      "Epoch 14/100\n",
      "1249/1249 - 32s - loss: 0.5719 - accuracy: 0.7626\n",
      "Epoch 15/100\n",
      "1249/1249 - 33s - loss: 0.5691 - accuracy: 0.7653\n",
      "Epoch 16/100\n",
      "1249/1249 - 33s - loss: 0.5673 - accuracy: 0.7664\n",
      "Epoch 17/100\n",
      "1249/1249 - 33s - loss: 0.5637 - accuracy: 0.7667\n",
      "Epoch 18/100\n",
      "1249/1249 - 33s - loss: 0.5634 - accuracy: 0.7676\n",
      "Epoch 19/100\n",
      "1249/1249 - 33s - loss: 0.5603 - accuracy: 0.7683\n",
      "Epoch 20/100\n",
      "1249/1249 - 33s - loss: 0.5592 - accuracy: 0.7658\n",
      "Epoch 21/100\n",
      "1249/1249 - 33s - loss: 0.5548 - accuracy: 0.7698\n",
      "Epoch 22/100\n",
      "1249/1249 - 33s - loss: 0.5542 - accuracy: 0.7675\n",
      "Epoch 23/100\n",
      "1249/1249 - 33s - loss: 0.5523 - accuracy: 0.7685\n",
      "Epoch 24/100\n",
      "1249/1249 - 33s - loss: 0.5502 - accuracy: 0.7696\n",
      "Epoch 25/100\n",
      "1249/1249 - 33s - loss: 0.5474 - accuracy: 0.7694\n",
      "Epoch 26/100\n",
      "1249/1249 - 33s - loss: 0.5461 - accuracy: 0.7693\n",
      "Epoch 27/100\n",
      "1249/1249 - 33s - loss: 0.5434 - accuracy: 0.7705\n",
      "Epoch 28/100\n",
      "1249/1249 - 33s - loss: 0.5416 - accuracy: 0.7702\n",
      "Epoch 29/100\n",
      "1249/1249 - 33s - loss: 0.5394 - accuracy: 0.7701\n",
      "Epoch 30/100\n",
      "1249/1249 - 33s - loss: 0.5388 - accuracy: 0.7703\n",
      "Epoch 31/100\n",
      "1249/1249 - 33s - loss: 0.5378 - accuracy: 0.7707\n",
      "Epoch 32/100\n",
      "1249/1249 - 33s - loss: 0.5364 - accuracy: 0.7711\n",
      "Epoch 33/100\n",
      "1249/1249 - 33s - loss: 0.5348 - accuracy: 0.7709\n",
      "Epoch 34/100\n",
      "1249/1249 - 33s - loss: 0.5332 - accuracy: 0.7716\n",
      "Epoch 35/100\n",
      "1249/1249 - 33s - loss: 0.5326 - accuracy: 0.7719\n",
      "Epoch 36/100\n",
      "1249/1249 - 33s - loss: 0.5311 - accuracy: 0.7714\n",
      "Epoch 37/100\n",
      "1249/1249 - 33s - loss: 0.5293 - accuracy: 0.7733\n",
      "Epoch 38/100\n",
      "1249/1249 - 33s - loss: 0.5270 - accuracy: 0.7732\n",
      "Epoch 39/100\n",
      "1249/1249 - 33s - loss: 0.5257 - accuracy: 0.7732\n",
      "Epoch 40/100\n",
      "1249/1249 - 33s - loss: 0.5230 - accuracy: 0.7729\n",
      "Epoch 41/100\n",
      "1249/1249 - 33s - loss: 0.5214 - accuracy: 0.7738\n",
      "Epoch 42/100\n",
      "1249/1249 - 33s - loss: 0.5208 - accuracy: 0.7743\n",
      "Epoch 43/100\n",
      "1249/1249 - 33s - loss: 0.5193 - accuracy: 0.7741\n",
      "Epoch 44/100\n",
      "1249/1249 - 33s - loss: 0.5173 - accuracy: 0.7760\n",
      "Epoch 45/100\n",
      "1249/1249 - 33s - loss: 0.5166 - accuracy: 0.7749\n",
      "Epoch 46/100\n",
      "1249/1249 - 33s - loss: 0.5153 - accuracy: 0.7777\n",
      "Epoch 47/100\n",
      "1249/1249 - 33s - loss: 0.5154 - accuracy: 0.7751\n",
      "Epoch 48/100\n",
      "1249/1249 - 33s - loss: 0.5133 - accuracy: 0.7767\n",
      "Epoch 49/100\n",
      "1249/1249 - 34s - loss: 0.5138 - accuracy: 0.7759\n",
      "Epoch 50/100\n",
      "1249/1249 - 33s - loss: 0.5129 - accuracy: 0.7761\n",
      "Epoch 51/100\n",
      "1249/1249 - 33s - loss: 0.5123 - accuracy: 0.7764\n",
      "Epoch 52/100\n",
      "1249/1249 - 33s - loss: 0.5102 - accuracy: 0.7779\n",
      "Epoch 53/100\n",
      "1249/1249 - 33s - loss: 0.5102 - accuracy: 0.7776\n",
      "Epoch 54/100\n",
      "1249/1249 - 34s - loss: 0.5094 - accuracy: 0.7790\n",
      "Epoch 55/100\n",
      "1249/1249 - 33s - loss: 0.5095 - accuracy: 0.7775\n",
      "Epoch 56/100\n",
      "1249/1249 - 33s - loss: 0.5087 - accuracy: 0.7790\n",
      "Epoch 57/100\n",
      "1249/1249 - 34s - loss: 0.5094 - accuracy: 0.7779\n",
      "Epoch 58/100\n",
      "1249/1249 - 33s - loss: 0.5088 - accuracy: 0.7777\n",
      "Epoch 59/100\n",
      "1249/1249 - 34s - loss: 0.5071 - accuracy: 0.7794\n",
      "Epoch 60/100\n",
      "1249/1249 - 34s - loss: 0.5058 - accuracy: 0.7785\n",
      "Epoch 61/100\n",
      "1249/1249 - 34s - loss: 0.5060 - accuracy: 0.7795\n",
      "Epoch 62/100\n",
      "1249/1249 - 33s - loss: 0.5063 - accuracy: 0.7787\n",
      "Epoch 63/100\n",
      "1249/1249 - 33s - loss: 0.5043 - accuracy: 0.7789\n",
      "Epoch 64/100\n",
      "1249/1249 - 33s - loss: 0.5040 - accuracy: 0.7805\n",
      "Epoch 65/100\n",
      "1249/1249 - 34s - loss: 0.5046 - accuracy: 0.7797\n",
      "Epoch 66/100\n",
      "1249/1249 - 34s - loss: 0.5043 - accuracy: 0.7794\n",
      "Epoch 67/100\n",
      "1249/1249 - 34s - loss: 0.5026 - accuracy: 0.7813\n",
      "Epoch 68/100\n",
      "1249/1249 - 34s - loss: 0.5022 - accuracy: 0.7804\n",
      "Epoch 69/100\n",
      "1249/1249 - 33s - loss: 0.5011 - accuracy: 0.7818\n",
      "Epoch 70/100\n",
      "1249/1249 - 33s - loss: 0.4999 - accuracy: 0.7803\n",
      "Epoch 71/100\n",
      "1249/1249 - 33s - loss: 0.5015 - accuracy: 0.7811\n",
      "Epoch 72/100\n",
      "1249/1249 - 34s - loss: 0.5001 - accuracy: 0.7817\n",
      "Epoch 73/100\n",
      "1249/1249 - 34s - loss: 0.5006 - accuracy: 0.7798\n",
      "Epoch 74/100\n",
      "1249/1249 - 33s - loss: 0.5001 - accuracy: 0.7802\n",
      "Epoch 75/100\n",
      "1249/1249 - 33s - loss: 0.4981 - accuracy: 0.7821\n",
      "Epoch 76/100\n",
      "1249/1249 - 33s - loss: 0.4989 - accuracy: 0.7801\n",
      "Epoch 77/100\n",
      "1249/1249 - 33s - loss: 0.4988 - accuracy: 0.7825\n",
      "Epoch 78/100\n",
      "1249/1249 - 33s - loss: 0.4973 - accuracy: 0.7828\n",
      "Epoch 79/100\n",
      "1249/1249 - 34s - loss: 0.4984 - accuracy: 0.7818\n",
      "Epoch 80/100\n",
      "1249/1249 - 34s - loss: 0.4976 - accuracy: 0.7829\n",
      "Epoch 81/100\n",
      "1249/1249 - 34s - loss: 0.4966 - accuracy: 0.7839\n",
      "Epoch 82/100\n",
      "1249/1249 - 33s - loss: 0.4970 - accuracy: 0.7826\n",
      "Epoch 83/100\n",
      "1249/1249 - 34s - loss: 0.4960 - accuracy: 0.7815\n",
      "Epoch 84/100\n",
      "1249/1249 - 34s - loss: 0.4956 - accuracy: 0.7842\n",
      "Epoch 85/100\n",
      "1249/1249 - 34s - loss: 0.4960 - accuracy: 0.7832\n",
      "Epoch 86/100\n",
      "1249/1249 - 34s - loss: 0.4953 - accuracy: 0.7826\n",
      "Epoch 87/100\n",
      "1249/1249 - 34s - loss: 0.4946 - accuracy: 0.7830\n",
      "Epoch 88/100\n",
      "1249/1249 - 37s - loss: 0.4955 - accuracy: 0.7837\n",
      "Epoch 89/100\n",
      "1249/1249 - 34s - loss: 0.4939 - accuracy: 0.7851\n",
      "Epoch 90/100\n",
      "1249/1249 - 34s - loss: 0.4940 - accuracy: 0.7832\n",
      "Epoch 91/100\n",
      "1249/1249 - 34s - loss: 0.4929 - accuracy: 0.7844\n",
      "Epoch 92/100\n",
      "1249/1249 - 34s - loss: 0.4941 - accuracy: 0.7838\n",
      "Epoch 93/100\n",
      "1249/1249 - 34s - loss: 0.4927 - accuracy: 0.7839\n",
      "Epoch 94/100\n",
      "1249/1249 - 34s - loss: 0.4922 - accuracy: 0.7849\n",
      "Epoch 95/100\n",
      "1249/1249 - 34s - loss: 0.4923 - accuracy: 0.7847\n",
      "Epoch 96/100\n",
      "1249/1249 - 34s - loss: 0.4915 - accuracy: 0.7842\n",
      "Epoch 97/100\n",
      "1249/1249 - 34s - loss: 0.4900 - accuracy: 0.7850\n",
      "Epoch 98/100\n",
      "1249/1249 - 34s - loss: 0.4905 - accuracy: 0.7844\n",
      "Epoch 99/100\n",
      "1249/1249 - 34s - loss: 0.4900 - accuracy: 0.7868\n",
      "Epoch 100/100\n",
      "1249/1249 - 34s - loss: 0.4912 - accuracy: 0.7838\n",
      "417/417 - 3s - loss: 0.5366 - accuracy: 0.7546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1666/1666 - 50s - loss: 0.7765 - accuracy: 0.7390\n",
      "Epoch 2/100\n",
      "1666/1666 - 48s - loss: 0.7501 - accuracy: 0.7413\n",
      "Epoch 3/100\n",
      "1666/1666 - 48s - loss: 0.7489 - accuracy: 0.7413\n",
      "Epoch 4/100\n",
      "1666/1666 - 48s - loss: 0.7463 - accuracy: 0.7413\n",
      "Epoch 5/100\n",
      "1666/1666 - 48s - loss: 0.7377 - accuracy: 0.7413\n",
      "Epoch 6/100\n",
      "1666/1666 - 48s - loss: 0.7054 - accuracy: 0.7413\n",
      "Epoch 7/100\n",
      "1666/1666 - 48s - loss: 0.6457 - accuracy: 0.7425\n",
      "Epoch 8/100\n",
      "1666/1666 - 49s - loss: 0.6184 - accuracy: 0.7489\n",
      "Epoch 9/100\n",
      "1666/1666 - 49s - loss: 0.6069 - accuracy: 0.7540\n",
      "Epoch 10/100\n",
      "1666/1666 - 49s - loss: 0.6016 - accuracy: 0.7558\n",
      "Epoch 11/100\n",
      "1666/1666 - 49s - loss: 0.5963 - accuracy: 0.7573\n",
      "Epoch 12/100\n",
      "1666/1666 - 49s - loss: 0.5929 - accuracy: 0.7577\n",
      "Epoch 13/100\n",
      "1666/1666 - 49s - loss: 0.5878 - accuracy: 0.7587\n",
      "Epoch 14/100\n",
      "1666/1666 - 49s - loss: 0.5846 - accuracy: 0.7598\n",
      "Epoch 15/100\n",
      "1666/1666 - 50s - loss: 0.5814 - accuracy: 0.7611\n",
      "Epoch 16/100\n",
      "1666/1666 - 49s - loss: 0.5758 - accuracy: 0.7616\n",
      "Epoch 17/100\n",
      "1666/1666 - 48s - loss: 0.5702 - accuracy: 0.7617\n",
      "Epoch 18/100\n",
      "1666/1666 - 48s - loss: 0.5732 - accuracy: 0.7618\n",
      "Epoch 19/100\n",
      "1666/1666 - 48s - loss: 0.5747 - accuracy: 0.7616\n",
      "Epoch 20/100\n",
      "1666/1666 - 48s - loss: 0.5652 - accuracy: 0.7633\n",
      "Epoch 21/100\n",
      "1666/1666 - 48s - loss: 0.5603 - accuracy: 0.7631\n",
      "Epoch 22/100\n",
      "1666/1666 - 48s - loss: 0.5568 - accuracy: 0.7642\n",
      "Epoch 23/100\n",
      "1666/1666 - 48s - loss: 0.5536 - accuracy: 0.7655\n",
      "Epoch 24/100\n",
      "1666/1666 - 48s - loss: 0.5517 - accuracy: 0.7649\n",
      "Epoch 25/100\n",
      "1666/1666 - 48s - loss: 0.5488 - accuracy: 0.7637\n",
      "Epoch 26/100\n",
      "1666/1666 - 48s - loss: 0.5461 - accuracy: 0.7653\n",
      "Epoch 27/100\n",
      "1666/1666 - 48s - loss: 0.5439 - accuracy: 0.7665\n",
      "Epoch 28/100\n",
      "1666/1666 - 48s - loss: 0.5417 - accuracy: 0.7680\n",
      "Epoch 29/100\n",
      "1666/1666 - 48s - loss: 0.5395 - accuracy: 0.7671\n",
      "Epoch 30/100\n",
      "1666/1666 - 48s - loss: 0.5371 - accuracy: 0.7671\n",
      "Epoch 31/100\n",
      "1666/1666 - 48s - loss: 0.5346 - accuracy: 0.7693\n",
      "Epoch 32/100\n",
      "1666/1666 - 48s - loss: 0.5339 - accuracy: 0.7687\n",
      "Epoch 33/100\n",
      "1666/1666 - 48s - loss: 0.5317 - accuracy: 0.7677\n",
      "Epoch 34/100\n",
      "1666/1666 - 48s - loss: 0.5307 - accuracy: 0.7697\n",
      "Epoch 35/100\n",
      "1666/1666 - 48s - loss: 0.5294 - accuracy: 0.7692\n",
      "Epoch 36/100\n",
      "1666/1666 - 48s - loss: 0.5272 - accuracy: 0.7695\n",
      "Epoch 37/100\n",
      "1666/1666 - 48s - loss: 0.5268 - accuracy: 0.7693\n",
      "Epoch 38/100\n",
      "1666/1666 - 48s - loss: 0.5248 - accuracy: 0.7694\n",
      "Epoch 39/100\n",
      "1666/1666 - 48s - loss: 0.5242 - accuracy: 0.7702\n",
      "Epoch 40/100\n",
      "1666/1666 - 48s - loss: 0.5231 - accuracy: 0.7695\n",
      "Epoch 41/100\n",
      "1666/1666 - 48s - loss: 0.5217 - accuracy: 0.7715\n",
      "Epoch 42/100\n",
      "1666/1666 - 48s - loss: 0.5202 - accuracy: 0.7719\n",
      "Epoch 43/100\n",
      "1666/1666 - 49s - loss: 0.5188 - accuracy: 0.7712\n",
      "Epoch 44/100\n",
      "1666/1666 - 49s - loss: 0.5180 - accuracy: 0.7717\n",
      "Epoch 45/100\n",
      "1666/1666 - 49s - loss: 0.5169 - accuracy: 0.7722\n",
      "Epoch 46/100\n",
      "1666/1666 - 49s - loss: 0.5160 - accuracy: 0.7732\n",
      "Epoch 47/100\n",
      "1666/1666 - 49s - loss: 0.5157 - accuracy: 0.7725\n",
      "Epoch 48/100\n",
      "1666/1666 - 49s - loss: 0.5146 - accuracy: 0.7704\n",
      "Epoch 49/100\n",
      "1666/1666 - 49s - loss: 0.5132 - accuracy: 0.7717\n",
      "Epoch 50/100\n",
      "1666/1666 - 49s - loss: 0.5117 - accuracy: 0.7726\n",
      "Epoch 51/100\n",
      "1666/1666 - 49s - loss: 0.5121 - accuracy: 0.7726\n",
      "Epoch 52/100\n",
      "1666/1666 - 49s - loss: 0.5104 - accuracy: 0.7725\n",
      "Epoch 53/100\n",
      "1666/1666 - 49s - loss: 0.5090 - accuracy: 0.7726\n",
      "Epoch 54/100\n",
      "1666/1666 - 49s - loss: 0.5097 - accuracy: 0.7714\n",
      "Epoch 55/100\n",
      "1666/1666 - 49s - loss: 0.5078 - accuracy: 0.7743\n",
      "Epoch 56/100\n",
      "1666/1666 - 49s - loss: 0.5080 - accuracy: 0.7746\n",
      "Epoch 57/100\n",
      "1666/1666 - 49s - loss: 0.5065 - accuracy: 0.7742\n",
      "Epoch 58/100\n",
      "1666/1666 - 49s - loss: 0.5060 - accuracy: 0.7749\n",
      "Epoch 59/100\n",
      "1666/1666 - 49s - loss: 0.5049 - accuracy: 0.7754\n",
      "Epoch 60/100\n",
      "1666/1666 - 49s - loss: 0.5057 - accuracy: 0.7742\n",
      "Epoch 61/100\n",
      "1666/1666 - 49s - loss: 0.5047 - accuracy: 0.7740\n",
      "Epoch 62/100\n",
      "1666/1666 - 49s - loss: 0.5041 - accuracy: 0.7755\n",
      "Epoch 63/100\n",
      "1666/1666 - 49s - loss: 0.5030 - accuracy: 0.7762\n",
      "Epoch 64/100\n",
      "1666/1666 - 49s - loss: 0.5018 - accuracy: 0.7747\n",
      "Epoch 65/100\n",
      "1666/1666 - 49s - loss: 0.5028 - accuracy: 0.7755\n",
      "Epoch 66/100\n",
      "1666/1666 - 49s - loss: 0.5016 - accuracy: 0.7750\n",
      "Epoch 67/100\n",
      "1666/1666 - 49s - loss: 0.5011 - accuracy: 0.7756\n",
      "Epoch 68/100\n",
      "1666/1666 - 49s - loss: 0.5013 - accuracy: 0.7760\n",
      "Epoch 69/100\n",
      "1666/1666 - 49s - loss: 0.5010 - accuracy: 0.7762\n",
      "Epoch 70/100\n",
      "1666/1666 - 49s - loss: 0.4997 - accuracy: 0.7769\n",
      "Epoch 71/100\n",
      "1666/1666 - 49s - loss: 0.4999 - accuracy: 0.7775\n",
      "Epoch 72/100\n",
      "1666/1666 - 49s - loss: 0.4990 - accuracy: 0.7767\n",
      "Epoch 73/100\n",
      "1666/1666 - 49s - loss: 0.4987 - accuracy: 0.7766\n",
      "Epoch 74/100\n",
      "1666/1666 - 49s - loss: 0.4978 - accuracy: 0.7766\n",
      "Epoch 75/100\n",
      "1666/1666 - 49s - loss: 0.4979 - accuracy: 0.7778\n",
      "Epoch 76/100\n",
      "1666/1666 - 49s - loss: 0.4977 - accuracy: 0.7772\n",
      "Epoch 77/100\n",
      "1666/1666 - 49s - loss: 0.4973 - accuracy: 0.7775\n",
      "Epoch 78/100\n",
      "1666/1666 - 49s - loss: 0.4970 - accuracy: 0.7777\n",
      "Epoch 79/100\n",
      "1666/1666 - 49s - loss: 0.4962 - accuracy: 0.7777\n",
      "Epoch 80/100\n",
      "1666/1666 - 49s - loss: 0.4955 - accuracy: 0.7777\n",
      "Epoch 81/100\n",
      "1666/1666 - 49s - loss: 0.4947 - accuracy: 0.7780\n",
      "Epoch 82/100\n",
      "1666/1666 - 49s - loss: 0.4954 - accuracy: 0.7787\n",
      "Epoch 83/100\n",
      "1666/1666 - 49s - loss: 0.4950 - accuracy: 0.7787\n",
      "Epoch 84/100\n",
      "1666/1666 - 49s - loss: 0.4948 - accuracy: 0.7784\n",
      "Epoch 85/100\n",
      "1666/1666 - 49s - loss: 0.4942 - accuracy: 0.7783\n",
      "Epoch 86/100\n",
      "1666/1666 - 49s - loss: 0.4942 - accuracy: 0.7791\n",
      "Epoch 87/100\n",
      "1666/1666 - 49s - loss: 0.4928 - accuracy: 0.7803\n",
      "Epoch 88/100\n",
      "1666/1666 - 49s - loss: 0.4936 - accuracy: 0.7783\n",
      "Epoch 89/100\n",
      "1666/1666 - 49s - loss: 0.4925 - accuracy: 0.7782\n",
      "Epoch 90/100\n",
      "1666/1666 - 49s - loss: 0.4929 - accuracy: 0.7793\n",
      "Epoch 91/100\n",
      "1666/1666 - 49s - loss: 0.4922 - accuracy: 0.7790\n",
      "Epoch 92/100\n",
      "1666/1666 - 49s - loss: 0.4923 - accuracy: 0.7802\n",
      "Epoch 93/100\n",
      "1666/1666 - 48s - loss: 0.4917 - accuracy: 0.7797\n",
      "Epoch 94/100\n",
      "1666/1666 - 49s - loss: 0.4914 - accuracy: 0.7808\n",
      "Epoch 95/100\n",
      "1666/1666 - 49s - loss: 0.4913 - accuracy: 0.7791\n",
      "Epoch 96/100\n",
      "1666/1666 - 48s - loss: 0.4908 - accuracy: 0.7812\n",
      "Epoch 97/100\n",
      "1666/1666 - 48s - loss: 0.4910 - accuracy: 0.7782\n",
      "Epoch 98/100\n",
      "1666/1666 - 49s - loss: 0.4901 - accuracy: 0.7796\n",
      "Epoch 99/100\n",
      "1666/1666 - 48s - loss: 0.4894 - accuracy: 0.7799\n",
      "Epoch 100/100\n",
      "1666/1666 - 48s - loss: 0.4901 - accuracy: 0.7805\n",
      "417/417 - 3s - loss: 0.5067 - accuracy: 0.7584\n",
      "Epoch 1/100\n",
      "2082/2082 - 61s - loss: 0.7730 - accuracy: 0.7341\n",
      "Epoch 2/100\n",
      "2082/2082 - 61s - loss: 0.7470 - accuracy: 0.7405\n",
      "Epoch 3/100\n",
      "2082/2082 - 60s - loss: 0.7099 - accuracy: 0.7405\n",
      "Epoch 4/100\n",
      "2082/2082 - 60s - loss: 0.6369 - accuracy: 0.7399\n",
      "Epoch 5/100\n",
      "2082/2082 - 60s - loss: 0.6152 - accuracy: 0.7423\n",
      "Epoch 6/100\n",
      "2082/2082 - 60s - loss: 0.6045 - accuracy: 0.7460\n",
      "Epoch 7/100\n",
      "2082/2082 - 60s - loss: 0.5981 - accuracy: 0.7470\n",
      "Epoch 8/100\n",
      "2082/2082 - 60s - loss: 0.5932 - accuracy: 0.7491\n",
      "Epoch 9/100\n",
      "2082/2082 - 60s - loss: 0.5891 - accuracy: 0.7519\n",
      "Epoch 10/100\n",
      "2082/2082 - 60s - loss: 0.5854 - accuracy: 0.7542\n",
      "Epoch 11/100\n",
      "2082/2082 - 65s - loss: 0.5821 - accuracy: 0.7546\n",
      "Epoch 12/100\n",
      "2082/2082 - 61s - loss: 0.5807 - accuracy: 0.7558\n",
      "Epoch 13/100\n",
      "2082/2082 - 60s - loss: 0.5774 - accuracy: 0.7578\n",
      "Epoch 14/100\n",
      "2082/2082 - 61s - loss: 0.5735 - accuracy: 0.7587\n",
      "Epoch 15/100\n",
      "2082/2082 - 61s - loss: 0.5701 - accuracy: 0.7601\n",
      "Epoch 16/100\n",
      "2082/2082 - 61s - loss: 0.5667 - accuracy: 0.7616\n",
      "Epoch 17/100\n",
      "2082/2082 - 60s - loss: 0.5632 - accuracy: 0.7614\n",
      "Epoch 18/100\n",
      "2082/2082 - 61s - loss: 0.5591 - accuracy: 0.7622\n",
      "Epoch 19/100\n",
      "2082/2082 - 61s - loss: 0.5543 - accuracy: 0.7635\n",
      "Epoch 20/100\n",
      "2082/2082 - 61s - loss: 0.5490 - accuracy: 0.7629\n",
      "Epoch 21/100\n",
      "2082/2082 - 61s - loss: 0.5427 - accuracy: 0.7652\n",
      "Epoch 22/100\n",
      "2082/2082 - 61s - loss: 0.5397 - accuracy: 0.7641\n",
      "Epoch 23/100\n",
      "2082/2082 - 61s - loss: 0.5363 - accuracy: 0.7644\n",
      "Epoch 24/100\n",
      "2082/2082 - 61s - loss: 0.5326 - accuracy: 0.7652\n",
      "Epoch 25/100\n",
      "2082/2082 - 61s - loss: 0.5289 - accuracy: 0.7671\n",
      "Epoch 26/100\n",
      "2082/2082 - 61s - loss: 0.5270 - accuracy: 0.7664\n",
      "Epoch 27/100\n",
      "2082/2082 - 61s - loss: 0.5257 - accuracy: 0.7664\n",
      "Epoch 28/100\n",
      "2082/2082 - 61s - loss: 0.5228 - accuracy: 0.7664\n",
      "Epoch 29/100\n",
      "2082/2082 - 61s - loss: 0.5199 - accuracy: 0.7670\n",
      "Epoch 30/100\n",
      "2082/2082 - 61s - loss: 0.5181 - accuracy: 0.7677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100\n",
      "2082/2082 - 61s - loss: 0.5178 - accuracy: 0.7687\n",
      "Epoch 32/100\n",
      "2082/2082 - 61s - loss: 0.5157 - accuracy: 0.7686\n",
      "Epoch 33/100\n",
      "2082/2082 - 61s - loss: 0.5137 - accuracy: 0.7685\n",
      "Epoch 34/100\n",
      "2082/2082 - 61s - loss: 0.5139 - accuracy: 0.7685\n",
      "Epoch 35/100\n",
      "2082/2082 - 61s - loss: 0.5115 - accuracy: 0.7701\n",
      "Epoch 36/100\n",
      "2082/2082 - 61s - loss: 0.5088 - accuracy: 0.7724\n",
      "Epoch 37/100\n",
      "2082/2082 - 61s - loss: 0.5090 - accuracy: 0.7705\n",
      "Epoch 38/100\n",
      "2082/2082 - 61s - loss: 0.5075 - accuracy: 0.7709\n",
      "Epoch 39/100\n",
      "2082/2082 - 61s - loss: 0.5068 - accuracy: 0.7717\n",
      "Epoch 40/100\n",
      "2082/2082 - 61s - loss: 0.5055 - accuracy: 0.7724\n",
      "Epoch 41/100\n",
      "2082/2082 - 61s - loss: 0.5039 - accuracy: 0.7730\n",
      "Epoch 42/100\n",
      "2082/2082 - 61s - loss: 0.5044 - accuracy: 0.7729\n",
      "Epoch 43/100\n",
      "2082/2082 - 61s - loss: 0.5027 - accuracy: 0.7736\n",
      "Epoch 44/100\n",
      "2082/2082 - 61s - loss: 0.5018 - accuracy: 0.7724\n",
      "Epoch 45/100\n",
      "2082/2082 - 61s - loss: 0.5014 - accuracy: 0.7735\n",
      "Epoch 46/100\n",
      "2082/2082 - 61s - loss: 0.5004 - accuracy: 0.7742\n",
      "Epoch 47/100\n",
      "2082/2082 - 61s - loss: 0.5001 - accuracy: 0.7734\n",
      "Epoch 48/100\n",
      "2082/2082 - 61s - loss: 0.4999 - accuracy: 0.7725\n",
      "Epoch 49/100\n",
      "2082/2082 - 61s - loss: 0.4980 - accuracy: 0.7747\n",
      "Epoch 50/100\n",
      "2082/2082 - 61s - loss: 0.4978 - accuracy: 0.7743\n",
      "Epoch 51/100\n",
      "2082/2082 - 61s - loss: 0.4981 - accuracy: 0.7738\n",
      "Epoch 52/100\n",
      "2082/2082 - 61s - loss: 0.4961 - accuracy: 0.7760\n",
      "Epoch 53/100\n",
      "2082/2082 - 61s - loss: 0.4956 - accuracy: 0.7760\n",
      "Epoch 54/100\n",
      "2082/2082 - 61s - loss: 0.4949 - accuracy: 0.7754\n",
      "Epoch 55/100\n",
      "2082/2082 - 61s - loss: 0.4951 - accuracy: 0.7755\n",
      "Epoch 56/100\n",
      "2082/2082 - 61s - loss: 0.4934 - accuracy: 0.7764\n",
      "Epoch 57/100\n",
      "2082/2082 - 61s - loss: 0.4931 - accuracy: 0.7766\n",
      "Epoch 58/100\n",
      "2082/2082 - 61s - loss: 0.4935 - accuracy: 0.7767\n",
      "Epoch 59/100\n",
      "2082/2082 - 61s - loss: 0.4920 - accuracy: 0.7774\n",
      "Epoch 60/100\n",
      "2082/2082 - 61s - loss: 0.4921 - accuracy: 0.7769\n",
      "Epoch 61/100\n",
      "2082/2082 - 61s - loss: 0.4920 - accuracy: 0.7773\n",
      "Epoch 62/100\n",
      "2082/2082 - 61s - loss: 0.4909 - accuracy: 0.7775\n",
      "Epoch 63/100\n",
      "2082/2082 - 61s - loss: 0.4909 - accuracy: 0.7776\n",
      "Epoch 64/100\n",
      "2082/2082 - 61s - loss: 0.4899 - accuracy: 0.7785\n",
      "Epoch 65/100\n",
      "2082/2082 - 61s - loss: 0.4899 - accuracy: 0.7784\n",
      "Epoch 66/100\n",
      "2082/2082 - 61s - loss: 0.4901 - accuracy: 0.7793\n",
      "Epoch 67/100\n",
      "2082/2082 - 61s - loss: 0.4889 - accuracy: 0.7805\n",
      "Epoch 68/100\n",
      "2082/2082 - 61s - loss: 0.4877 - accuracy: 0.7794\n",
      "Epoch 69/100\n",
      "2082/2082 - 61s - loss: 0.4882 - accuracy: 0.7794\n",
      "Epoch 70/100\n",
      "2082/2082 - 61s - loss: 0.4872 - accuracy: 0.7795\n",
      "Epoch 71/100\n",
      "2082/2082 - 61s - loss: 0.4874 - accuracy: 0.7793\n",
      "Epoch 72/100\n",
      "2082/2082 - 61s - loss: 0.4863 - accuracy: 0.7804\n",
      "Epoch 73/100\n",
      "2082/2082 - 61s - loss: 0.4873 - accuracy: 0.7795\n",
      "Epoch 74/100\n",
      "2082/2082 - 61s - loss: 0.4883 - accuracy: 0.7785\n",
      "Epoch 75/100\n",
      "2082/2082 - 61s - loss: 0.4854 - accuracy: 0.7811\n",
      "Epoch 76/100\n",
      "2082/2082 - 61s - loss: 0.4855 - accuracy: 0.7801\n",
      "Epoch 77/100\n",
      "2082/2082 - 61s - loss: 0.4851 - accuracy: 0.7814\n",
      "Epoch 78/100\n",
      "2082/2082 - 61s - loss: 0.4893 - accuracy: 0.7808\n",
      "Epoch 79/100\n",
      "2082/2082 - 61s - loss: 0.4845 - accuracy: 0.7803\n",
      "Epoch 80/100\n",
      "2082/2082 - 61s - loss: 0.4835 - accuracy: 0.7805\n",
      "Epoch 81/100\n",
      "2082/2082 - 61s - loss: 0.4839 - accuracy: 0.7813\n",
      "Epoch 82/100\n",
      "2082/2082 - 61s - loss: 0.4838 - accuracy: 0.7817\n",
      "Epoch 83/100\n",
      "2082/2082 - 61s - loss: 0.4824 - accuracy: 0.7834\n",
      "Epoch 84/100\n",
      "2082/2082 - 61s - loss: 0.4810 - accuracy: 0.7822\n",
      "Epoch 85/100\n",
      "2082/2082 - 61s - loss: 0.4815 - accuracy: 0.7838\n",
      "Epoch 86/100\n",
      "2082/2082 - 61s - loss: 0.4813 - accuracy: 0.7828\n",
      "Epoch 87/100\n",
      "2082/2082 - 61s - loss: 0.4815 - accuracy: 0.7823\n",
      "Epoch 88/100\n",
      "2082/2082 - 61s - loss: 0.4805 - accuracy: 0.7832\n",
      "Epoch 89/100\n",
      "2082/2082 - 61s - loss: 0.4811 - accuracy: 0.7829\n",
      "Epoch 90/100\n",
      "2082/2082 - 61s - loss: 0.4798 - accuracy: 0.7842\n",
      "Epoch 91/100\n",
      "2082/2082 - 61s - loss: 0.4801 - accuracy: 0.7833\n",
      "Epoch 92/100\n",
      "2082/2082 - 61s - loss: 0.4796 - accuracy: 0.7831\n",
      "Epoch 93/100\n",
      "2082/2082 - 61s - loss: 0.4794 - accuracy: 0.7837\n",
      "Epoch 94/100\n",
      "2082/2082 - 61s - loss: 0.4792 - accuracy: 0.7835\n",
      "Epoch 95/100\n",
      "2082/2082 - 61s - loss: 0.4788 - accuracy: 0.7847\n",
      "Epoch 96/100\n",
      "2082/2082 - 61s - loss: 0.4782 - accuracy: 0.7848\n",
      "Epoch 97/100\n",
      "2082/2082 - 61s - loss: 0.4776 - accuracy: 0.7846\n",
      "Epoch 98/100\n",
      "2082/2082 - 61s - loss: 0.4808 - accuracy: 0.7841\n",
      "Epoch 99/100\n",
      "2082/2082 - 61s - loss: 0.4771 - accuracy: 0.7859\n",
      "Epoch 100/100\n",
      "2082/2082 - 61s - loss: 0.4781 - accuracy: 0.7846\n",
      "417/417 - 3s - loss: 0.6119 - accuracy: 0.7450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "1 fits failed out of a total of 25.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/keras/wrappers/scikit_learn.py\", line 163, in fit\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\", line 1183, in fit\n",
      "    'train',\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\n",
      "    # Implements GenericFunction.__call__.\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\", line 917, in _call\n",
      "    new_tracing_count = self.experimental_get_tracing_count()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/eager/function.py\", line 3023, in __call__\n",
      "    if any(not isinstance(arg, (ops.Tensor, tensor_spec.DenseSpec,\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/eager/function.py\", line 1960, in _call_flat\n",
      "    placeholder=None,\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/eager/function.py\", line 591, in call\n",
      "    self._num_inference_outputs = len(self._func_graph.outputs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\n",
      "    raise core._status_to_exception(e) from None\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError:  Cannot update variable with shape [] using a Tensor with shape [0], shapes must be equal.\n",
      "\t [[node AssignAddVariableOp (defined at opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/keras/wrappers/scikit_learn.py:163) ]] [Op:__inference_train_function_8547414]\n",
      "\n",
      "Function call stack:\n",
      "train_function\n",
      "\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [-0.54928383         nan -0.54248751 -0.53979847 -0.5445055 ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2498/2498 - 73s - loss: 0.7799 - accuracy: 0.7310\n",
      "Epoch 2/100\n",
      "2498/2498 - 73s - loss: 0.7608 - accuracy: 0.7344\n",
      "Epoch 3/100\n",
      "2498/2498 - 73s - loss: 0.7510 - accuracy: 0.7344\n",
      "Epoch 4/100\n",
      "2498/2498 - 72s - loss: 0.6893 - accuracy: 0.7346\n",
      "Epoch 5/100\n",
      "2498/2498 - 73s - loss: 0.6286 - accuracy: 0.7379\n",
      "Epoch 6/100\n",
      "2498/2498 - 73s - loss: 0.6126 - accuracy: 0.7437\n",
      "Epoch 7/100\n",
      "2498/2498 - 73s - loss: 0.6037 - accuracy: 0.7464\n",
      "Epoch 8/100\n",
      "2498/2498 - 73s - loss: 0.5949 - accuracy: 0.7503\n",
      "Epoch 9/100\n",
      "2498/2498 - 73s - loss: 0.5850 - accuracy: 0.7526\n",
      "Epoch 10/100\n",
      "2498/2498 - 73s - loss: 0.5787 - accuracy: 0.7542\n",
      "Epoch 11/100\n",
      "2498/2498 - 73s - loss: 0.5751 - accuracy: 0.7540\n",
      "Epoch 12/100\n",
      "2498/2498 - 73s - loss: 0.5710 - accuracy: 0.7552\n",
      "Epoch 13/100\n",
      "2498/2498 - 73s - loss: 0.5668 - accuracy: 0.7562\n",
      "Epoch 14/100\n",
      "2498/2498 - 73s - loss: 0.5615 - accuracy: 0.7582\n",
      "Epoch 15/100\n",
      "2498/2498 - 73s - loss: 0.5576 - accuracy: 0.7594\n",
      "Epoch 16/100\n",
      "2498/2498 - 73s - loss: 0.5543 - accuracy: 0.7592\n",
      "Epoch 17/100\n",
      "2498/2498 - 73s - loss: 0.5495 - accuracy: 0.7615\n",
      "Epoch 18/100\n",
      "2498/2498 - 73s - loss: 0.5460 - accuracy: 0.7617\n",
      "Epoch 19/100\n",
      "2498/2498 - 73s - loss: 0.5414 - accuracy: 0.7611\n",
      "Epoch 20/100\n",
      "2498/2498 - 73s - loss: 0.5390 - accuracy: 0.7623\n",
      "Epoch 21/100\n",
      "2498/2498 - 73s - loss: 0.5358 - accuracy: 0.7640\n",
      "Epoch 22/100\n",
      "2498/2498 - 73s - loss: 0.5331 - accuracy: 0.7633\n",
      "Epoch 23/100\n",
      "2498/2498 - 73s - loss: 0.5308 - accuracy: 0.7638\n",
      "Epoch 24/100\n",
      "2498/2498 - 73s - loss: 0.5292 - accuracy: 0.7645\n",
      "Epoch 25/100\n",
      "2498/2498 - 73s - loss: 0.5269 - accuracy: 0.7637\n",
      "Epoch 26/100\n",
      "2498/2498 - 73s - loss: 0.5253 - accuracy: 0.7649\n",
      "Epoch 27/100\n",
      "2498/2498 - 73s - loss: 0.5246 - accuracy: 0.7647\n",
      "Epoch 28/100\n",
      "2498/2498 - 73s - loss: 0.5231 - accuracy: 0.7662\n",
      "Epoch 29/100\n",
      "2498/2498 - 76s - loss: 0.5213 - accuracy: 0.7661\n",
      "Epoch 30/100\n",
      "2498/2498 - 73s - loss: 0.5195 - accuracy: 0.7661\n",
      "Epoch 31/100\n",
      "2498/2498 - 73s - loss: 0.5188 - accuracy: 0.7676\n",
      "Epoch 32/100\n",
      "2498/2498 - 73s - loss: 0.5173 - accuracy: 0.7673\n",
      "Epoch 33/100\n",
      "2498/2498 - 74s - loss: 0.5174 - accuracy: 0.7674\n",
      "Epoch 34/100\n",
      "2498/2498 - 73s - loss: 0.5164 - accuracy: 0.7683\n",
      "Epoch 35/100\n",
      "2498/2498 - 73s - loss: 0.5146 - accuracy: 0.7687\n",
      "Epoch 36/100\n",
      "2498/2498 - 73s - loss: 0.5136 - accuracy: 0.7673\n",
      "Epoch 37/100\n",
      "2498/2498 - 73s - loss: 0.5141 - accuracy: 0.7684\n",
      "Epoch 38/100\n",
      "2498/2498 - 73s - loss: 0.5123 - accuracy: 0.7685\n",
      "Epoch 39/100\n",
      "2498/2498 - 73s - loss: 0.5120 - accuracy: 0.7688\n",
      "Epoch 40/100\n",
      "2498/2498 - 73s - loss: 0.5111 - accuracy: 0.7681\n",
      "Epoch 41/100\n",
      "2498/2498 - 73s - loss: 0.5101 - accuracy: 0.7707\n",
      "Epoch 42/100\n",
      "2498/2498 - 73s - loss: 0.5095 - accuracy: 0.7702\n",
      "Epoch 43/100\n",
      "2498/2498 - 73s - loss: 0.5093 - accuracy: 0.7695\n",
      "Epoch 44/100\n",
      "2498/2498 - 73s - loss: 0.5089 - accuracy: 0.7698\n",
      "Epoch 45/100\n",
      "2498/2498 - 73s - loss: 0.5084 - accuracy: 0.7709\n",
      "Epoch 46/100\n",
      "2498/2498 - 73s - loss: 0.5079 - accuracy: 0.7708\n",
      "Epoch 47/100\n",
      "2498/2498 - 73s - loss: 0.5071 - accuracy: 0.7716\n",
      "Epoch 48/100\n",
      "2498/2498 - 72s - loss: 0.5066 - accuracy: 0.7705\n",
      "Epoch 49/100\n",
      "2498/2498 - 72s - loss: 0.5065 - accuracy: 0.7709\n",
      "Epoch 50/100\n",
      "2498/2498 - 72s - loss: 0.5062 - accuracy: 0.7697\n",
      "Epoch 51/100\n",
      "2498/2498 - 72s - loss: 0.5045 - accuracy: 0.7705\n",
      "Epoch 52/100\n",
      "2498/2498 - 72s - loss: 0.5055 - accuracy: 0.7716\n",
      "Epoch 53/100\n",
      "2498/2498 - 72s - loss: 0.5043 - accuracy: 0.7728\n",
      "Epoch 54/100\n",
      "2498/2498 - 72s - loss: 0.5040 - accuracy: 0.7738\n",
      "Epoch 55/100\n",
      "2498/2498 - 74s - loss: 0.5031 - accuracy: 0.7718\n",
      "Epoch 56/100\n",
      "2498/2498 - 72s - loss: 0.5031 - accuracy: 0.7719\n",
      "Epoch 57/100\n",
      "2498/2498 - 73s - loss: 0.5024 - accuracy: 0.7729\n",
      "Epoch 58/100\n",
      "2498/2498 - 72s - loss: 0.5020 - accuracy: 0.7733\n",
      "Epoch 59/100\n",
      "2498/2498 - 73s - loss: 0.5020 - accuracy: 0.7731\n",
      "Epoch 60/100\n",
      "2498/2498 - 72s - loss: 0.5015 - accuracy: 0.7729\n",
      "Epoch 61/100\n",
      "2498/2498 - 72s - loss: 0.5008 - accuracy: 0.7732\n",
      "Epoch 62/100\n",
      "2498/2498 - 72s - loss: 0.5006 - accuracy: 0.7732\n",
      "Epoch 63/100\n",
      "2498/2498 - 72s - loss: 0.5005 - accuracy: 0.7730\n",
      "Epoch 64/100\n",
      "2498/2498 - 72s - loss: 0.4994 - accuracy: 0.7736\n",
      "Epoch 65/100\n",
      "2498/2498 - 73s - loss: 0.4993 - accuracy: 0.7736\n",
      "Epoch 66/100\n",
      "2498/2498 - 73s - loss: 0.4995 - accuracy: 0.7736\n",
      "Epoch 67/100\n",
      "2498/2498 - 73s - loss: 0.4995 - accuracy: 0.7735\n",
      "Epoch 68/100\n",
      "2498/2498 - 73s - loss: 0.4983 - accuracy: 0.7733\n",
      "Epoch 69/100\n",
      "2498/2498 - 73s - loss: 0.4982 - accuracy: 0.7735\n",
      "Epoch 70/100\n",
      "2498/2498 - 73s - loss: 0.4983 - accuracy: 0.7740\n",
      "Epoch 71/100\n",
      "2498/2498 - 73s - loss: 0.4978 - accuracy: 0.7750\n",
      "Epoch 72/100\n",
      "2498/2498 - 73s - loss: 0.4968 - accuracy: 0.7759\n",
      "Epoch 73/100\n",
      "2498/2498 - 78s - loss: 0.4969 - accuracy: 0.7747\n",
      "Epoch 74/100\n",
      "2498/2498 - 74s - loss: 0.4957 - accuracy: 0.7754\n",
      "Epoch 75/100\n",
      "2498/2498 - 73s - loss: 0.4962 - accuracy: 0.7758\n",
      "Epoch 76/100\n",
      "2498/2498 - 74s - loss: 0.4957 - accuracy: 0.7757\n",
      "Epoch 77/100\n",
      "2498/2498 - 74s - loss: 0.4957 - accuracy: 0.7750\n",
      "Epoch 78/100\n",
      "2498/2498 - 73s - loss: 0.4951 - accuracy: 0.7753\n",
      "Epoch 79/100\n",
      "2498/2498 - 73s - loss: 0.4946 - accuracy: 0.7757\n",
      "Epoch 80/100\n",
      "2498/2498 - 73s - loss: 0.4947 - accuracy: 0.7771\n",
      "Epoch 81/100\n",
      "2498/2498 - 73s - loss: 0.4938 - accuracy: 0.7754\n",
      "Epoch 82/100\n",
      "2498/2498 - 73s - loss: 0.4943 - accuracy: 0.7758\n",
      "Epoch 83/100\n",
      "2498/2498 - 73s - loss: 0.4944 - accuracy: 0.7759\n",
      "Epoch 84/100\n",
      "2498/2498 - 73s - loss: 0.4939 - accuracy: 0.7768\n",
      "Epoch 85/100\n",
      "2498/2498 - 73s - loss: 0.4936 - accuracy: 0.7753\n",
      "Epoch 86/100\n",
      "2498/2498 - 73s - loss: 0.4934 - accuracy: 0.7760\n",
      "Epoch 87/100\n",
      "2498/2498 - 73s - loss: 0.4928 - accuracy: 0.7763\n",
      "Epoch 88/100\n",
      "2498/2498 - 73s - loss: 0.4926 - accuracy: 0.7768\n",
      "Epoch 89/100\n",
      "2498/2498 - 73s - loss: 0.4925 - accuracy: 0.7766\n",
      "Epoch 90/100\n",
      "2498/2498 - 73s - loss: 0.4924 - accuracy: 0.7761\n",
      "Epoch 91/100\n",
      "2498/2498 - 73s - loss: 0.4918 - accuracy: 0.7788\n",
      "Epoch 92/100\n",
      "2498/2498 - 74s - loss: 0.4919 - accuracy: 0.7776\n",
      "Epoch 93/100\n",
      "2498/2498 - 73s - loss: 0.4912 - accuracy: 0.7768\n",
      "Epoch 94/100\n",
      "2498/2498 - 73s - loss: 0.4909 - accuracy: 0.7776\n",
      "Epoch 95/100\n",
      "2498/2498 - 73s - loss: 0.4909 - accuracy: 0.7771\n",
      "Epoch 96/100\n",
      "2498/2498 - 73s - loss: 0.4906 - accuracy: 0.7773\n",
      "Epoch 97/100\n",
      "2498/2498 - 73s - loss: 0.4902 - accuracy: 0.7767\n",
      "Epoch 98/100\n",
      "2498/2498 - 73s - loss: 0.4896 - accuracy: 0.7782\n",
      "Epoch 99/100\n",
      "2498/2498 - 73s - loss: 0.4894 - accuracy: 0.7783\n",
      "Epoch 100/100\n",
      "2498/2498 - 73s - loss: 0.4898 - accuracy: 0.7784\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Epoch 1/100\n",
      "417/417 - 13s - loss: 0.8355 - accuracy: 0.7356\n",
      "Epoch 2/100\n",
      "417/417 - 12s - loss: 0.7548 - accuracy: 0.7389\n",
      "Epoch 3/100\n",
      "417/417 - 12s - loss: 0.7539 - accuracy: 0.7389\n",
      "Epoch 4/100\n",
      "417/417 - 12s - loss: 0.7531 - accuracy: 0.7389\n",
      "Epoch 5/100\n",
      "417/417 - 12s - loss: 0.7524 - accuracy: 0.7389\n",
      "Epoch 6/100\n",
      "417/417 - 12s - loss: 0.7512 - accuracy: 0.7389\n",
      "Epoch 7/100\n",
      "417/417 - 12s - loss: 0.7500 - accuracy: 0.7389\n",
      "Epoch 8/100\n",
      "417/417 - 12s - loss: 0.7482 - accuracy: 0.7389\n",
      "Epoch 9/100\n",
      "417/417 - 12s - loss: 0.7455 - accuracy: 0.7389\n",
      "Epoch 10/100\n",
      "417/417 - 12s - loss: 0.7423 - accuracy: 0.7389\n",
      "Epoch 11/100\n",
      "417/417 - 12s - loss: 0.7375 - accuracy: 0.7389\n",
      "Epoch 12/100\n",
      "417/417 - 12s - loss: 0.7312 - accuracy: 0.7389\n",
      "Epoch 13/100\n",
      "417/417 - 12s - loss: 0.7229 - accuracy: 0.7389\n",
      "Epoch 14/100\n",
      "417/417 - 12s - loss: 0.7126 - accuracy: 0.7389\n",
      "Epoch 15/100\n",
      "417/417 - 12s - loss: 0.7010 - accuracy: 0.7389\n",
      "Epoch 16/100\n",
      "417/417 - 12s - loss: 0.6876 - accuracy: 0.7389\n",
      "Epoch 17/100\n",
      "417/417 - 12s - loss: 0.6763 - accuracy: 0.7389\n",
      "Epoch 18/100\n",
      "417/417 - 12s - loss: 0.6659 - accuracy: 0.7390\n",
      "Epoch 19/100\n",
      "417/417 - 12s - loss: 0.6595 - accuracy: 0.7387\n",
      "Epoch 20/100\n",
      "417/417 - 12s - loss: 0.6523 - accuracy: 0.7390\n",
      "Epoch 21/100\n",
      "417/417 - 12s - loss: 0.6492 - accuracy: 0.7387\n",
      "Epoch 22/100\n",
      "417/417 - 12s - loss: 0.6462 - accuracy: 0.7383\n",
      "Epoch 23/100\n",
      "417/417 - 12s - loss: 0.6428 - accuracy: 0.7405\n",
      "Epoch 24/100\n",
      "417/417 - 12s - loss: 0.6424 - accuracy: 0.7385\n",
      "Epoch 25/100\n",
      "417/417 - 12s - loss: 0.6384 - accuracy: 0.7408\n",
      "Epoch 26/100\n",
      "417/417 - 12s - loss: 0.6360 - accuracy: 0.7414\n",
      "Epoch 27/100\n",
      "417/417 - 12s - loss: 0.6361 - accuracy: 0.7396\n",
      "Epoch 28/100\n",
      "417/417 - 12s - loss: 0.6350 - accuracy: 0.7423\n",
      "Epoch 29/100\n",
      "417/417 - 12s - loss: 0.6311 - accuracy: 0.7404\n",
      "Epoch 30/100\n",
      "417/417 - 12s - loss: 0.6296 - accuracy: 0.7414\n",
      "Epoch 31/100\n",
      "417/417 - 12s - loss: 0.6283 - accuracy: 0.7435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100\n",
      "417/417 - 12s - loss: 0.6266 - accuracy: 0.7425\n",
      "Epoch 33/100\n",
      "417/417 - 12s - loss: 0.6254 - accuracy: 0.7403\n",
      "Epoch 34/100\n",
      "417/417 - 12s - loss: 0.6237 - accuracy: 0.7450\n",
      "Epoch 35/100\n",
      "417/417 - 12s - loss: 0.6230 - accuracy: 0.7408\n",
      "Epoch 36/100\n",
      "417/417 - 12s - loss: 0.6192 - accuracy: 0.7417\n",
      "Epoch 37/100\n",
      "417/417 - 12s - loss: 0.6180 - accuracy: 0.7426\n",
      "Epoch 38/100\n",
      "417/417 - 13s - loss: 0.6192 - accuracy: 0.7425\n",
      "Epoch 39/100\n",
      "417/417 - 13s - loss: 0.6183 - accuracy: 0.7435\n",
      "Epoch 40/100\n",
      "417/417 - 13s - loss: 0.6125 - accuracy: 0.7440\n",
      "Epoch 41/100\n",
      "417/417 - 13s - loss: 0.6110 - accuracy: 0.7435\n",
      "Epoch 42/100\n",
      "417/417 - 13s - loss: 0.6106 - accuracy: 0.7450\n",
      "Epoch 43/100\n",
      "417/417 - 13s - loss: 0.6117 - accuracy: 0.7401\n",
      "Epoch 44/100\n",
      "417/417 - 13s - loss: 0.6089 - accuracy: 0.7434\n",
      "Epoch 45/100\n",
      "417/417 - 13s - loss: 0.6056 - accuracy: 0.7459\n",
      "Epoch 46/100\n",
      "417/417 - 13s - loss: 0.6054 - accuracy: 0.7438\n",
      "Epoch 47/100\n",
      "417/417 - 13s - loss: 0.6042 - accuracy: 0.7408\n",
      "Epoch 48/100\n",
      "417/417 - 13s - loss: 0.6019 - accuracy: 0.7450\n",
      "Epoch 49/100\n",
      "417/417 - 13s - loss: 0.5970 - accuracy: 0.7440\n",
      "Epoch 50/100\n",
      "417/417 - 13s - loss: 0.5947 - accuracy: 0.7422\n",
      "Epoch 51/100\n",
      "417/417 - 13s - loss: 0.5933 - accuracy: 0.7477\n",
      "Epoch 52/100\n",
      "417/417 - 13s - loss: 0.5901 - accuracy: 0.7483\n",
      "Epoch 53/100\n",
      "417/417 - 13s - loss: 0.5901 - accuracy: 0.7459\n",
      "Epoch 54/100\n",
      "417/417 - 13s - loss: 0.5858 - accuracy: 0.7450\n",
      "Epoch 55/100\n",
      "417/417 - 13s - loss: 0.5867 - accuracy: 0.7474\n",
      "Epoch 56/100\n",
      "417/417 - 13s - loss: 0.5845 - accuracy: 0.7483\n",
      "Epoch 57/100\n",
      "417/417 - 13s - loss: 0.5830 - accuracy: 0.7490\n",
      "Epoch 58/100\n",
      "417/417 - 13s - loss: 0.5815 - accuracy: 0.7489\n",
      "Epoch 59/100\n",
      "417/417 - 13s - loss: 0.5811 - accuracy: 0.7510\n",
      "Epoch 60/100\n",
      "417/417 - 13s - loss: 0.5803 - accuracy: 0.7486\n",
      "Epoch 61/100\n",
      "417/417 - 13s - loss: 0.5784 - accuracy: 0.7511\n",
      "Epoch 62/100\n",
      "417/417 - 13s - loss: 0.5773 - accuracy: 0.7515\n",
      "Epoch 63/100\n",
      "417/417 - 13s - loss: 0.5766 - accuracy: 0.7522\n",
      "Epoch 64/100\n",
      "417/417 - 13s - loss: 0.5773 - accuracy: 0.7543\n",
      "Epoch 65/100\n",
      "417/417 - 13s - loss: 0.5722 - accuracy: 0.7575\n",
      "Epoch 66/100\n",
      "417/417 - 13s - loss: 0.5727 - accuracy: 0.7535\n",
      "Epoch 67/100\n",
      "417/417 - 13s - loss: 0.5738 - accuracy: 0.7532\n",
      "Epoch 68/100\n",
      "417/417 - 13s - loss: 0.5702 - accuracy: 0.7559\n",
      "Epoch 69/100\n",
      "417/417 - 13s - loss: 0.5711 - accuracy: 0.7571\n",
      "Epoch 70/100\n",
      "417/417 - 13s - loss: 0.5698 - accuracy: 0.7557\n",
      "Epoch 71/100\n",
      "417/417 - 13s - loss: 0.5699 - accuracy: 0.7557\n",
      "Epoch 72/100\n",
      "417/417 - 13s - loss: 0.5697 - accuracy: 0.7538\n",
      "Epoch 73/100\n",
      "417/417 - 13s - loss: 0.5678 - accuracy: 0.7576\n",
      "Epoch 74/100\n",
      "417/417 - 13s - loss: 0.5654 - accuracy: 0.7605\n",
      "Epoch 75/100\n",
      "417/417 - 13s - loss: 0.5652 - accuracy: 0.7547\n",
      "Epoch 76/100\n",
      "417/417 - 13s - loss: 0.5647 - accuracy: 0.7585\n",
      "Epoch 77/100\n",
      "417/417 - 13s - loss: 0.5628 - accuracy: 0.7563\n",
      "Epoch 78/100\n",
      "417/417 - 13s - loss: 0.5626 - accuracy: 0.7589\n",
      "Epoch 79/100\n",
      "417/417 - 13s - loss: 0.5618 - accuracy: 0.7601\n",
      "Epoch 80/100\n",
      "417/417 - 13s - loss: 0.5624 - accuracy: 0.7580\n",
      "Epoch 81/100\n",
      "417/417 - 13s - loss: 0.5603 - accuracy: 0.7589\n",
      "Epoch 82/100\n",
      "417/417 - 13s - loss: 0.5594 - accuracy: 0.7586\n",
      "Epoch 83/100\n",
      "417/417 - 13s - loss: 0.5583 - accuracy: 0.7589\n",
      "Epoch 84/100\n",
      "417/417 - 13s - loss: 0.5574 - accuracy: 0.7586\n",
      "Epoch 85/100\n",
      "417/417 - 13s - loss: 0.5565 - accuracy: 0.7601\n",
      "Epoch 86/100\n",
      "417/417 - 13s - loss: 0.5554 - accuracy: 0.7624\n",
      "Epoch 87/100\n",
      "417/417 - 13s - loss: 0.5565 - accuracy: 0.7607\n",
      "Epoch 88/100\n",
      "417/417 - 13s - loss: 0.5546 - accuracy: 0.7621\n",
      "Epoch 89/100\n",
      "417/417 - 13s - loss: 0.5537 - accuracy: 0.7625\n",
      "Epoch 90/100\n",
      "417/417 - 13s - loss: 0.5533 - accuracy: 0.7615\n",
      "Epoch 91/100\n",
      "417/417 - 13s - loss: 0.5540 - accuracy: 0.7619\n",
      "Epoch 92/100\n",
      "417/417 - 13s - loss: 0.5516 - accuracy: 0.7620\n",
      "Epoch 93/100\n",
      "417/417 - 13s - loss: 0.5511 - accuracy: 0.7632\n",
      "Epoch 94/100\n",
      "417/417 - 13s - loss: 0.5504 - accuracy: 0.7630\n",
      "Epoch 95/100\n",
      "417/417 - 13s - loss: 0.5507 - accuracy: 0.7616\n",
      "Epoch 96/100\n",
      "417/417 - 13s - loss: 0.5490 - accuracy: 0.7625\n",
      "Epoch 97/100\n",
      "417/417 - 13s - loss: 0.5481 - accuracy: 0.7611\n",
      "Epoch 98/100\n",
      "417/417 - 13s - loss: 0.5465 - accuracy: 0.7624\n",
      "Epoch 99/100\n",
      "417/417 - 13s - loss: 0.5464 - accuracy: 0.7631\n",
      "Epoch 100/100\n",
      "417/417 - 13s - loss: 0.5444 - accuracy: 0.7643\n",
      "417/417 - 3s - loss: 0.6013 - accuracy: 0.7474\n",
      "Epoch 1/100\n",
      "833/833 - 27s - loss: 0.7970 - accuracy: 0.7170\n",
      "Epoch 2/100\n",
      "833/833 - 25s - loss: 0.7526 - accuracy: 0.7396\n",
      "Epoch 3/100\n",
      "833/833 - 25s - loss: 0.7504 - accuracy: 0.7396\n",
      "Epoch 4/100\n",
      "833/833 - 25s - loss: 0.7457 - accuracy: 0.7396\n",
      "Epoch 5/100\n",
      "833/833 - 25s - loss: 0.7339 - accuracy: 0.7396\n",
      "Epoch 6/100\n",
      "833/833 - 25s - loss: 0.7084 - accuracy: 0.7396\n",
      "Epoch 7/100\n",
      "833/833 - 25s - loss: 0.6753 - accuracy: 0.7396\n",
      "Epoch 8/100\n",
      "833/833 - 25s - loss: 0.6514 - accuracy: 0.7393\n",
      "Epoch 9/100\n",
      "833/833 - 25s - loss: 0.6408 - accuracy: 0.7396\n",
      "Epoch 10/100\n",
      "833/833 - 25s - loss: 0.6364 - accuracy: 0.7405\n",
      "Epoch 11/100\n",
      "833/833 - 25s - loss: 0.6328 - accuracy: 0.7413\n",
      "Epoch 12/100\n",
      "833/833 - 25s - loss: 0.6296 - accuracy: 0.7420\n",
      "Epoch 13/100\n",
      "833/833 - 25s - loss: 0.6271 - accuracy: 0.7422\n",
      "Epoch 14/100\n",
      "833/833 - 25s - loss: 0.6244 - accuracy: 0.7435\n",
      "Epoch 15/100\n",
      "833/833 - 25s - loss: 0.6215 - accuracy: 0.7443\n",
      "Epoch 16/100\n",
      "833/833 - 25s - loss: 0.6190 - accuracy: 0.7443\n",
      "Epoch 17/100\n",
      "833/833 - 25s - loss: 0.6157 - accuracy: 0.7465\n",
      "Epoch 18/100\n",
      "833/833 - 25s - loss: 0.6128 - accuracy: 0.7462\n",
      "Epoch 19/100\n",
      "833/833 - 25s - loss: 0.6095 - accuracy: 0.7452\n",
      "Epoch 20/100\n",
      "833/833 - 25s - loss: 0.6060 - accuracy: 0.7473\n",
      "Epoch 21/100\n",
      "833/833 - 25s - loss: 0.6022 - accuracy: 0.7467\n",
      "Epoch 22/100\n",
      "833/833 - 25s - loss: 0.5972 - accuracy: 0.7486\n",
      "Epoch 23/100\n",
      "833/833 - 25s - loss: 0.5937 - accuracy: 0.7493\n",
      "Epoch 24/100\n",
      "833/833 - 25s - loss: 0.5901 - accuracy: 0.7499\n",
      "Epoch 25/100\n",
      "833/833 - 25s - loss: 0.5885 - accuracy: 0.7495\n",
      "Epoch 26/100\n",
      "833/833 - 25s - loss: 0.5824 - accuracy: 0.7521\n",
      "Epoch 27/100\n",
      "833/833 - 25s - loss: 0.5798 - accuracy: 0.7517\n",
      "Epoch 28/100\n",
      "833/833 - 25s - loss: 0.5792 - accuracy: 0.7517\n",
      "Epoch 29/100\n",
      "833/833 - 25s - loss: 0.5762 - accuracy: 0.7540\n",
      "Epoch 30/100\n",
      "833/833 - 25s - loss: 0.5732 - accuracy: 0.7527\n",
      "Epoch 31/100\n",
      "833/833 - 25s - loss: 0.5706 - accuracy: 0.7549\n",
      "Epoch 32/100\n",
      "833/833 - 25s - loss: 0.5688 - accuracy: 0.7543\n",
      "Epoch 33/100\n",
      "833/833 - 25s - loss: 0.5682 - accuracy: 0.7538\n",
      "Epoch 34/100\n",
      "833/833 - 25s - loss: 0.5635 - accuracy: 0.7566\n",
      "Epoch 35/100\n",
      "833/833 - 25s - loss: 0.5636 - accuracy: 0.7549\n",
      "Epoch 36/100\n",
      "833/833 - 25s - loss: 0.5618 - accuracy: 0.7562\n",
      "Epoch 37/100\n",
      "833/833 - 25s - loss: 0.5595 - accuracy: 0.7569\n",
      "Epoch 38/100\n",
      "833/833 - 25s - loss: 0.5579 - accuracy: 0.7586\n",
      "Epoch 39/100\n",
      "833/833 - 25s - loss: 0.5567 - accuracy: 0.7587\n",
      "Epoch 40/100\n",
      "833/833 - 25s - loss: 0.5571 - accuracy: 0.7582\n",
      "Epoch 41/100\n",
      "833/833 - 25s - loss: 0.5557 - accuracy: 0.7599\n",
      "Epoch 42/100\n",
      "833/833 - 25s - loss: 0.5539 - accuracy: 0.7590\n",
      "Epoch 43/100\n",
      "833/833 - 25s - loss: 0.5523 - accuracy: 0.7585\n",
      "Epoch 44/100\n",
      "833/833 - 25s - loss: 0.5520 - accuracy: 0.7611\n",
      "Epoch 45/100\n",
      "833/833 - 25s - loss: 0.5488 - accuracy: 0.7598\n",
      "Epoch 46/100\n",
      "833/833 - 25s - loss: 0.5490 - accuracy: 0.7605\n",
      "Epoch 47/100\n",
      "833/833 - 25s - loss: 0.5478 - accuracy: 0.7623\n",
      "Epoch 48/100\n",
      "833/833 - 25s - loss: 0.5468 - accuracy: 0.7631\n",
      "Epoch 49/100\n",
      "833/833 - 25s - loss: 0.5468 - accuracy: 0.7623\n",
      "Epoch 50/100\n",
      "833/833 - 25s - loss: 0.5452 - accuracy: 0.7610\n",
      "Epoch 51/100\n",
      "833/833 - 25s - loss: 0.5449 - accuracy: 0.7622\n",
      "Epoch 52/100\n",
      "833/833 - 25s - loss: 0.5450 - accuracy: 0.7622\n",
      "Epoch 53/100\n",
      "833/833 - 25s - loss: 0.5432 - accuracy: 0.7650\n",
      "Epoch 54/100\n",
      "833/833 - 25s - loss: 0.5431 - accuracy: 0.7637\n",
      "Epoch 55/100\n",
      "833/833 - 25s - loss: 0.5422 - accuracy: 0.7629\n",
      "Epoch 56/100\n",
      "833/833 - 25s - loss: 0.5411 - accuracy: 0.7653\n",
      "Epoch 57/100\n",
      "833/833 - 25s - loss: 0.5404 - accuracy: 0.7645\n",
      "Epoch 58/100\n",
      "833/833 - 25s - loss: 0.5399 - accuracy: 0.7658\n",
      "Epoch 59/100\n",
      "833/833 - 25s - loss: 0.5399 - accuracy: 0.7640\n",
      "Epoch 60/100\n",
      "833/833 - 25s - loss: 0.5384 - accuracy: 0.7651\n",
      "Epoch 61/100\n",
      "833/833 - 25s - loss: 0.5385 - accuracy: 0.7648\n",
      "Epoch 62/100\n",
      "833/833 - 25s - loss: 0.5375 - accuracy: 0.7662\n",
      "Epoch 63/100\n",
      "833/833 - 25s - loss: 0.5385 - accuracy: 0.7652\n",
      "Epoch 64/100\n",
      "833/833 - 25s - loss: 0.5363 - accuracy: 0.7634\n",
      "Epoch 65/100\n",
      "833/833 - 25s - loss: 0.5369 - accuracy: 0.7640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/100\n",
      "833/833 - 25s - loss: 0.5367 - accuracy: 0.7659\n",
      "Epoch 67/100\n",
      "833/833 - 25s - loss: 0.5354 - accuracy: 0.7668\n",
      "Epoch 68/100\n",
      "833/833 - 25s - loss: 0.5348 - accuracy: 0.7657\n",
      "Epoch 69/100\n",
      "833/833 - 25s - loss: 0.5346 - accuracy: 0.7668\n",
      "Epoch 70/100\n",
      "833/833 - 25s - loss: 0.5345 - accuracy: 0.7660\n",
      "Epoch 71/100\n",
      "833/833 - 25s - loss: 0.5343 - accuracy: 0.7655\n",
      "Epoch 72/100\n",
      "833/833 - 25s - loss: 0.5325 - accuracy: 0.7671\n",
      "Epoch 73/100\n",
      "833/833 - 25s - loss: 0.5318 - accuracy: 0.7669\n",
      "Epoch 74/100\n",
      "833/833 - 25s - loss: 0.5314 - accuracy: 0.7685\n",
      "Epoch 75/100\n",
      "833/833 - 25s - loss: 0.5309 - accuracy: 0.7662\n",
      "Epoch 76/100\n",
      "833/833 - 25s - loss: 0.5303 - accuracy: 0.7667\n",
      "Epoch 77/100\n",
      "833/833 - 25s - loss: 0.5300 - accuracy: 0.7689\n",
      "Epoch 78/100\n",
      "833/833 - 25s - loss: 0.5292 - accuracy: 0.7675\n",
      "Epoch 79/100\n",
      "833/833 - 25s - loss: 0.5295 - accuracy: 0.7662\n",
      "Epoch 80/100\n",
      "833/833 - 25s - loss: 0.5295 - accuracy: 0.7705\n",
      "Epoch 81/100\n",
      "833/833 - 25s - loss: 0.5286 - accuracy: 0.7667\n",
      "Epoch 82/100\n",
      "833/833 - 25s - loss: 0.5289 - accuracy: 0.7700\n",
      "Epoch 83/100\n",
      "833/833 - 25s - loss: 0.5282 - accuracy: 0.7683\n",
      "Epoch 84/100\n",
      "833/833 - 25s - loss: 0.5278 - accuracy: 0.7691\n",
      "Epoch 85/100\n",
      "833/833 - 25s - loss: 0.5278 - accuracy: 0.7703\n",
      "Epoch 86/100\n",
      "833/833 - 25s - loss: 0.5266 - accuracy: 0.7696\n",
      "Epoch 87/100\n",
      "833/833 - 25s - loss: 0.5253 - accuracy: 0.7696\n",
      "Epoch 88/100\n",
      "833/833 - 25s - loss: 0.5261 - accuracy: 0.7698\n",
      "Epoch 89/100\n",
      "833/833 - 25s - loss: 0.5251 - accuracy: 0.7704\n",
      "Epoch 90/100\n",
      "833/833 - 25s - loss: 0.5253 - accuracy: 0.7684\n",
      "Epoch 91/100\n",
      "833/833 - 25s - loss: 0.5264 - accuracy: 0.7687\n",
      "Epoch 92/100\n",
      "833/833 - 25s - loss: 0.5247 - accuracy: 0.7712\n",
      "Epoch 93/100\n",
      "833/833 - 25s - loss: 0.5258 - accuracy: 0.7697\n",
      "Epoch 94/100\n",
      "833/833 - 26s - loss: 0.5252 - accuracy: 0.7705\n",
      "Epoch 95/100\n",
      "833/833 - 25s - loss: 0.5236 - accuracy: 0.7704\n",
      "Epoch 96/100\n",
      "833/833 - 25s - loss: 0.5234 - accuracy: 0.7717\n",
      "Epoch 97/100\n",
      "833/833 - 25s - loss: 0.5232 - accuracy: 0.7705\n",
      "Epoch 98/100\n",
      "833/833 - 25s - loss: 0.5222 - accuracy: 0.7702\n",
      "Epoch 99/100\n",
      "833/833 - 25s - loss: 0.5226 - accuracy: 0.7707\n",
      "Epoch 100/100\n",
      "833/833 - 25s - loss: 0.5229 - accuracy: 0.7703\n",
      "417/417 - 3s - loss: 0.5326 - accuracy: 0.7655\n",
      "Epoch 1/100\n",
      "1249/1249 - 38s - loss: 0.7815 - accuracy: 0.7370\n",
      "Epoch 2/100\n",
      "1249/1249 - 38s - loss: 0.7426 - accuracy: 0.7441\n",
      "Epoch 3/100\n",
      "1249/1249 - 38s - loss: 0.7375 - accuracy: 0.7441\n",
      "Epoch 4/100\n",
      "1249/1249 - 38s - loss: 0.7244 - accuracy: 0.7441\n",
      "Epoch 5/100\n",
      "1249/1249 - 37s - loss: 0.6955 - accuracy: 0.7441\n",
      "Epoch 6/100\n",
      "1249/1249 - 37s - loss: 0.6547 - accuracy: 0.7441\n",
      "Epoch 7/100\n",
      "1249/1249 - 38s - loss: 0.6327 - accuracy: 0.7436\n",
      "Epoch 8/100\n",
      "1249/1249 - 38s - loss: 0.6253 - accuracy: 0.7447\n",
      "Epoch 9/100\n",
      "1249/1249 - 38s - loss: 0.6224 - accuracy: 0.7444\n",
      "Epoch 10/100\n",
      "1249/1249 - 38s - loss: 0.6181 - accuracy: 0.7454\n",
      "Epoch 11/100\n",
      "1249/1249 - 38s - loss: 0.6151 - accuracy: 0.7463\n",
      "Epoch 12/100\n",
      "1249/1249 - 38s - loss: 0.6128 - accuracy: 0.7463\n",
      "Epoch 13/100\n",
      "1249/1249 - 38s - loss: 0.6096 - accuracy: 0.7470\n",
      "Epoch 14/100\n",
      "1249/1249 - 38s - loss: 0.6078 - accuracy: 0.7480\n",
      "Epoch 15/100\n",
      "1249/1249 - 38s - loss: 0.6054 - accuracy: 0.7494\n",
      "Epoch 16/100\n",
      "1249/1249 - 38s - loss: 0.6029 - accuracy: 0.7483\n",
      "Epoch 17/100\n",
      "1249/1249 - 38s - loss: 0.6020 - accuracy: 0.7491\n",
      "Epoch 18/100\n",
      "1249/1249 - 38s - loss: 0.5998 - accuracy: 0.7511\n",
      "Epoch 19/100\n",
      "1249/1249 - 38s - loss: 0.5983 - accuracy: 0.7508\n",
      "Epoch 20/100\n",
      "1249/1249 - 38s - loss: 0.5949 - accuracy: 0.7539\n",
      "Epoch 21/100\n",
      "1249/1249 - 38s - loss: 0.5943 - accuracy: 0.7519\n",
      "Epoch 22/100\n",
      "1249/1249 - 38s - loss: 0.5917 - accuracy: 0.7534\n",
      "Epoch 23/100\n",
      "1249/1249 - 38s - loss: 0.5891 - accuracy: 0.7545\n",
      "Epoch 24/100\n",
      "1249/1249 - 38s - loss: 0.5869 - accuracy: 0.7555\n",
      "Epoch 25/100\n",
      "1249/1249 - 38s - loss: 0.5848 - accuracy: 0.7577\n",
      "Epoch 26/100\n",
      "1249/1249 - 38s - loss: 0.5826 - accuracy: 0.7563\n",
      "Epoch 27/100\n",
      "1249/1249 - 38s - loss: 0.5809 - accuracy: 0.7586\n",
      "Epoch 28/100\n",
      "1249/1249 - 38s - loss: 0.5794 - accuracy: 0.7589\n",
      "Epoch 29/100\n",
      "1249/1249 - 38s - loss: 0.5773 - accuracy: 0.7575\n",
      "Epoch 30/100\n",
      "1249/1249 - 38s - loss: 0.5762 - accuracy: 0.7579\n",
      "Epoch 31/100\n",
      "1249/1249 - 38s - loss: 0.5736 - accuracy: 0.7587\n",
      "Epoch 32/100\n",
      "1249/1249 - 38s - loss: 0.5725 - accuracy: 0.7583\n",
      "Epoch 33/100\n",
      "1249/1249 - 38s - loss: 0.5708 - accuracy: 0.7614\n",
      "Epoch 34/100\n",
      "1249/1249 - 38s - loss: 0.5697 - accuracy: 0.7599\n",
      "Epoch 35/100\n",
      "1249/1249 - 38s - loss: 0.5681 - accuracy: 0.7621\n",
      "Epoch 36/100\n",
      "1249/1249 - 38s - loss: 0.5661 - accuracy: 0.7621\n",
      "Epoch 37/100\n",
      "1249/1249 - 38s - loss: 0.5644 - accuracy: 0.7621\n",
      "Epoch 38/100\n",
      "1249/1249 - 38s - loss: 0.5638 - accuracy: 0.7612\n",
      "Epoch 39/100\n",
      "1249/1249 - 38s - loss: 0.5640 - accuracy: 0.7611\n",
      "Epoch 40/100\n",
      "1249/1249 - 39s - loss: 0.5614 - accuracy: 0.7636\n",
      "Epoch 41/100\n",
      "1249/1249 - 39s - loss: 0.5614 - accuracy: 0.7621\n",
      "Epoch 42/100\n",
      "1249/1249 - 38s - loss: 0.5591 - accuracy: 0.7637\n",
      "Epoch 43/100\n",
      "1249/1249 - 38s - loss: 0.5580 - accuracy: 0.7626\n",
      "Epoch 44/100\n",
      "1249/1249 - 38s - loss: 0.5564 - accuracy: 0.7635\n",
      "Epoch 45/100\n",
      "1249/1249 - 38s - loss: 0.5555 - accuracy: 0.7632\n",
      "Epoch 46/100\n",
      "1249/1249 - 38s - loss: 0.5545 - accuracy: 0.7625\n",
      "Epoch 47/100\n",
      "1249/1249 - 37s - loss: 0.5530 - accuracy: 0.7660\n",
      "Epoch 48/100\n",
      "1249/1249 - 37s - loss: 0.5519 - accuracy: 0.7650\n",
      "Epoch 49/100\n",
      "1249/1249 - 38s - loss: 0.5504 - accuracy: 0.7632\n",
      "Epoch 50/100\n",
      "1249/1249 - 38s - loss: 0.5508 - accuracy: 0.7645\n",
      "Epoch 51/100\n",
      "1249/1249 - 37s - loss: 0.5480 - accuracy: 0.7648\n",
      "Epoch 52/100\n",
      "1249/1249 - 37s - loss: 0.5460 - accuracy: 0.7664\n",
      "Epoch 53/100\n",
      "1249/1249 - 37s - loss: 0.5462 - accuracy: 0.7649\n",
      "Epoch 54/100\n",
      "1249/1249 - 38s - loss: 0.5458 - accuracy: 0.7652\n",
      "Epoch 55/100\n",
      "1249/1249 - 38s - loss: 0.5441 - accuracy: 0.7662\n",
      "Epoch 56/100\n",
      "1249/1249 - 38s - loss: 0.5441 - accuracy: 0.7664\n",
      "Epoch 57/100\n",
      "1249/1249 - 38s - loss: 0.5418 - accuracy: 0.7681\n",
      "Epoch 58/100\n",
      "1249/1249 - 38s - loss: 0.5414 - accuracy: 0.7667\n",
      "Epoch 59/100\n",
      "1249/1249 - 38s - loss: 0.5406 - accuracy: 0.7645\n",
      "Epoch 60/100\n",
      "1249/1249 - 38s - loss: 0.5385 - accuracy: 0.7669\n",
      "Epoch 61/100\n",
      "1249/1249 - 38s - loss: 0.5389 - accuracy: 0.7664\n",
      "Epoch 62/100\n",
      "1249/1249 - 38s - loss: 0.5381 - accuracy: 0.7674\n",
      "Epoch 63/100\n",
      "1249/1249 - 38s - loss: 0.5371 - accuracy: 0.7692\n",
      "Epoch 64/100\n",
      "1249/1249 - 38s - loss: 0.5365 - accuracy: 0.7670\n",
      "Epoch 65/100\n",
      "1249/1249 - 38s - loss: 0.5358 - accuracy: 0.7696\n",
      "Epoch 66/100\n",
      "1249/1249 - 38s - loss: 0.5337 - accuracy: 0.7692\n",
      "Epoch 67/100\n",
      "1249/1249 - 38s - loss: 0.5340 - accuracy: 0.7676\n",
      "Epoch 68/100\n",
      "1249/1249 - 38s - loss: 0.5331 - accuracy: 0.7672\n",
      "Epoch 69/100\n",
      "1249/1249 - 38s - loss: 0.5323 - accuracy: 0.7678\n",
      "Epoch 70/100\n",
      "1249/1249 - 38s - loss: 0.5323 - accuracy: 0.7666\n",
      "Epoch 71/100\n",
      "1249/1249 - 38s - loss: 0.5312 - accuracy: 0.7691\n",
      "Epoch 72/100\n",
      "1249/1249 - 38s - loss: 0.5298 - accuracy: 0.7706\n",
      "Epoch 73/100\n",
      "1249/1249 - 38s - loss: 0.5298 - accuracy: 0.7696\n",
      "Epoch 74/100\n",
      "1249/1249 - 38s - loss: 0.5290 - accuracy: 0.7703\n",
      "Epoch 75/100\n",
      "1249/1249 - 38s - loss: 0.5294 - accuracy: 0.7679\n",
      "Epoch 76/100\n",
      "1249/1249 - 38s - loss: 0.5271 - accuracy: 0.7713\n",
      "Epoch 77/100\n",
      "1249/1249 - 38s - loss: 0.5274 - accuracy: 0.7703\n",
      "Epoch 78/100\n",
      "1249/1249 - 38s - loss: 0.5261 - accuracy: 0.7686\n",
      "Epoch 79/100\n",
      "1249/1249 - 38s - loss: 0.5253 - accuracy: 0.7713\n",
      "Epoch 80/100\n",
      "1249/1249 - 38s - loss: 0.5258 - accuracy: 0.7722\n",
      "Epoch 81/100\n",
      "1249/1249 - 38s - loss: 0.5246 - accuracy: 0.7711\n",
      "Epoch 82/100\n",
      "1249/1249 - 38s - loss: 0.5244 - accuracy: 0.7721\n",
      "Epoch 83/100\n",
      "1249/1249 - 38s - loss: 0.5226 - accuracy: 0.7716\n",
      "Epoch 84/100\n",
      "1249/1249 - 38s - loss: 0.5229 - accuracy: 0.7725\n",
      "Epoch 85/100\n",
      "1249/1249 - 38s - loss: 0.5224 - accuracy: 0.7722\n",
      "Epoch 86/100\n",
      "1249/1249 - 38s - loss: 0.5223 - accuracy: 0.7727\n",
      "Epoch 87/100\n",
      "1249/1249 - 38s - loss: 0.5206 - accuracy: 0.7713\n",
      "Epoch 88/100\n",
      "1249/1249 - 38s - loss: 0.5202 - accuracy: 0.7731\n",
      "Epoch 89/100\n",
      "1249/1249 - 38s - loss: 0.5198 - accuracy: 0.7716\n",
      "Epoch 90/100\n",
      "1249/1249 - 38s - loss: 0.5199 - accuracy: 0.7731\n",
      "Epoch 91/100\n",
      "1249/1249 - 38s - loss: 0.5184 - accuracy: 0.7724\n",
      "Epoch 92/100\n",
      "1249/1249 - 38s - loss: 0.5183 - accuracy: 0.7721\n",
      "Epoch 93/100\n",
      "1249/1249 - 38s - loss: 0.5187 - accuracy: 0.7720\n",
      "Epoch 94/100\n",
      "1249/1249 - 38s - loss: 0.5183 - accuracy: 0.7720\n",
      "Epoch 95/100\n",
      "1249/1249 - 38s - loss: 0.5171 - accuracy: 0.7733\n",
      "Epoch 96/100\n",
      "1249/1249 - 38s - loss: 0.5171 - accuracy: 0.7735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100\n",
      "1249/1249 - 38s - loss: 0.5165 - accuracy: 0.7733\n",
      "Epoch 98/100\n",
      "1249/1249 - 38s - loss: 0.5158 - accuracy: 0.7733\n",
      "Epoch 99/100\n",
      "1249/1249 - 38s - loss: 0.5151 - accuracy: 0.7743\n",
      "Epoch 100/100\n",
      "1249/1249 - 38s - loss: 0.5143 - accuracy: 0.7741\n",
      "417/417 - 3s - loss: 0.5798 - accuracy: 0.7427\n",
      "Epoch 1/100\n",
      "1666/1666 - 50s - loss: 0.7666 - accuracy: 0.7412\n",
      "Epoch 2/100\n",
      "1666/1666 - 49s - loss: 0.7432 - accuracy: 0.7413\n",
      "Epoch 3/100\n",
      "1666/1666 - 49s - loss: 0.7287 - accuracy: 0.7413\n",
      "Epoch 4/100\n",
      "1666/1666 - 49s - loss: 0.6953 - accuracy: 0.7413\n",
      "Epoch 5/100\n",
      "1666/1666 - 49s - loss: 0.6602 - accuracy: 0.7405\n",
      "Epoch 6/100\n",
      "1666/1666 - 49s - loss: 0.6454 - accuracy: 0.7412\n",
      "Epoch 7/100\n",
      "1666/1666 - 49s - loss: 0.6348 - accuracy: 0.7418\n",
      "Epoch 8/100\n",
      "1666/1666 - 49s - loss: 0.6279 - accuracy: 0.7432\n",
      "Epoch 9/100\n",
      "1666/1666 - 49s - loss: 0.6226 - accuracy: 0.7447\n",
      "Epoch 10/100\n",
      "1666/1666 - 49s - loss: 0.6179 - accuracy: 0.7449\n",
      "Epoch 11/100\n",
      "1666/1666 - 49s - loss: 0.6140 - accuracy: 0.7465\n",
      "Epoch 12/100\n",
      "1666/1666 - 49s - loss: 0.6118 - accuracy: 0.7456\n",
      "Epoch 13/100\n",
      "1666/1666 - 49s - loss: 0.6075 - accuracy: 0.7458\n",
      "Epoch 14/100\n",
      "1666/1666 - 49s - loss: 0.6022 - accuracy: 0.7459\n",
      "Epoch 15/100\n",
      "1666/1666 - 49s - loss: 0.5976 - accuracy: 0.7461\n",
      "Epoch 16/100\n",
      "1666/1666 - 49s - loss: 0.5930 - accuracy: 0.7476\n",
      "Epoch 17/100\n",
      "1666/1666 - 49s - loss: 0.5874 - accuracy: 0.7487\n",
      "Epoch 18/100\n",
      "1666/1666 - 49s - loss: 0.5836 - accuracy: 0.7485\n",
      "Epoch 19/100\n",
      "1666/1666 - 49s - loss: 0.5801 - accuracy: 0.7495\n",
      "Epoch 20/100\n",
      "1666/1666 - 49s - loss: 0.5756 - accuracy: 0.7513\n",
      "Epoch 21/100\n",
      "1666/1666 - 49s - loss: 0.5721 - accuracy: 0.7500\n",
      "Epoch 22/100\n",
      "1666/1666 - 49s - loss: 0.5699 - accuracy: 0.7506\n",
      "Epoch 23/100\n",
      "1666/1666 - 49s - loss: 0.5652 - accuracy: 0.7524\n",
      "Epoch 24/100\n",
      "1666/1666 - 49s - loss: 0.5617 - accuracy: 0.7548\n",
      "Epoch 25/100\n",
      "1666/1666 - 49s - loss: 0.5594 - accuracy: 0.7526\n",
      "Epoch 26/100\n",
      "1666/1666 - 50s - loss: 0.5569 - accuracy: 0.7537\n",
      "Epoch 27/100\n",
      "1666/1666 - 49s - loss: 0.5547 - accuracy: 0.7562\n",
      "Epoch 28/100\n",
      "1666/1666 - 49s - loss: 0.5526 - accuracy: 0.7556\n",
      "Epoch 29/100\n",
      "1666/1666 - 50s - loss: 0.5507 - accuracy: 0.7577\n",
      "Epoch 30/100\n",
      "1666/1666 - 50s - loss: 0.5494 - accuracy: 0.7569\n",
      "Epoch 31/100\n",
      "1666/1666 - 50s - loss: 0.5478 - accuracy: 0.7565\n",
      "Epoch 32/100\n",
      "1666/1666 - 50s - loss: 0.5460 - accuracy: 0.7568\n",
      "Epoch 33/100\n",
      "1666/1666 - 50s - loss: 0.5444 - accuracy: 0.7594\n",
      "Epoch 34/100\n",
      "1666/1666 - 50s - loss: 0.5432 - accuracy: 0.7571\n",
      "Epoch 35/100\n",
      "1666/1666 - 51s - loss: 0.5421 - accuracy: 0.7574\n",
      "Epoch 36/100\n",
      "1666/1666 - 51s - loss: 0.5415 - accuracy: 0.7582\n",
      "Epoch 37/100\n",
      "1666/1666 - 51s - loss: 0.5415 - accuracy: 0.7569\n",
      "Epoch 38/100\n",
      "1666/1666 - 51s - loss: 0.5400 - accuracy: 0.7590\n",
      "Epoch 39/100\n",
      "1666/1666 - 51s - loss: 0.5385 - accuracy: 0.7590\n",
      "Epoch 40/100\n",
      "1666/1666 - 51s - loss: 0.5374 - accuracy: 0.7594\n",
      "Epoch 41/100\n",
      "1666/1666 - 51s - loss: 0.5373 - accuracy: 0.7601\n",
      "Epoch 42/100\n",
      "1666/1666 - 50s - loss: 0.5359 - accuracy: 0.7598\n",
      "Epoch 43/100\n",
      "1666/1666 - 50s - loss: 0.5357 - accuracy: 0.7594\n",
      "Epoch 44/100\n",
      "1666/1666 - 51s - loss: 0.5351 - accuracy: 0.7606\n",
      "Epoch 45/100\n",
      "1666/1666 - 50s - loss: 0.5351 - accuracy: 0.7598\n",
      "Epoch 46/100\n",
      "1666/1666 - 51s - loss: 0.5336 - accuracy: 0.7607\n",
      "Epoch 47/100\n",
      "1666/1666 - 51s - loss: 0.5338 - accuracy: 0.7594\n",
      "Epoch 48/100\n",
      "1666/1666 - 51s - loss: 0.5331 - accuracy: 0.7608\n",
      "Epoch 49/100\n",
      "1666/1666 - 51s - loss: 0.5315 - accuracy: 0.7625\n",
      "Epoch 50/100\n",
      "1666/1666 - 51s - loss: 0.5313 - accuracy: 0.7619\n",
      "Epoch 51/100\n",
      "1666/1666 - 51s - loss: 0.5305 - accuracy: 0.7621\n",
      "Epoch 52/100\n",
      "1666/1666 - 51s - loss: 0.5304 - accuracy: 0.7628\n",
      "Epoch 53/100\n",
      "1666/1666 - 50s - loss: 0.5295 - accuracy: 0.7623\n",
      "Epoch 54/100\n",
      "1666/1666 - 51s - loss: 0.5288 - accuracy: 0.7634\n",
      "Epoch 55/100\n",
      "1666/1666 - 51s - loss: 0.5284 - accuracy: 0.7646\n",
      "Epoch 56/100\n",
      "1666/1666 - 51s - loss: 0.5278 - accuracy: 0.7640\n",
      "Epoch 57/100\n",
      "1666/1666 - 51s - loss: 0.5275 - accuracy: 0.7639\n",
      "Epoch 58/100\n",
      "1666/1666 - 51s - loss: 0.5265 - accuracy: 0.7657\n",
      "Epoch 59/100\n",
      "1666/1666 - 51s - loss: 0.5270 - accuracy: 0.7636\n",
      "Epoch 60/100\n",
      "1666/1666 - 51s - loss: 0.5263 - accuracy: 0.7637\n",
      "Epoch 61/100\n",
      "1666/1666 - 51s - loss: 0.5259 - accuracy: 0.7634\n",
      "Epoch 62/100\n",
      "1666/1666 - 51s - loss: 0.5252 - accuracy: 0.7636\n",
      "Epoch 63/100\n",
      "1666/1666 - 51s - loss: 0.5248 - accuracy: 0.7641\n",
      "Epoch 64/100\n",
      "1666/1666 - 51s - loss: 0.5246 - accuracy: 0.7644\n",
      "Epoch 65/100\n",
      "1666/1666 - 51s - loss: 0.5243 - accuracy: 0.7639\n",
      "Epoch 66/100\n",
      "1666/1666 - 51s - loss: 0.5237 - accuracy: 0.7659\n",
      "Epoch 67/100\n",
      "1666/1666 - 51s - loss: 0.5227 - accuracy: 0.7652\n",
      "Epoch 68/100\n",
      "1666/1666 - 51s - loss: 0.5229 - accuracy: 0.7641\n",
      "Epoch 69/100\n",
      "1666/1666 - 50s - loss: 0.5225 - accuracy: 0.7646\n",
      "Epoch 70/100\n",
      "1666/1666 - 50s - loss: 0.5216 - accuracy: 0.7664\n",
      "Epoch 71/100\n",
      "1666/1666 - 51s - loss: 0.5213 - accuracy: 0.7672\n",
      "Epoch 72/100\n",
      "1666/1666 - 51s - loss: 0.5213 - accuracy: 0.7653\n",
      "Epoch 73/100\n",
      "1666/1666 - 50s - loss: 0.5208 - accuracy: 0.7657\n",
      "Epoch 74/100\n",
      "1666/1666 - 50s - loss: 0.5206 - accuracy: 0.7670\n",
      "Epoch 75/100\n",
      "1666/1666 - 51s - loss: 0.5192 - accuracy: 0.7667\n",
      "Epoch 76/100\n",
      "1666/1666 - 50s - loss: 0.5199 - accuracy: 0.7668\n",
      "Epoch 77/100\n",
      "1666/1666 - 51s - loss: 0.5188 - accuracy: 0.7666\n",
      "Epoch 78/100\n",
      "1666/1666 - 51s - loss: 0.5185 - accuracy: 0.7658\n",
      "Epoch 79/100\n",
      "1666/1666 - 50s - loss: 0.5182 - accuracy: 0.7652\n",
      "Epoch 80/100\n",
      "1666/1666 - 51s - loss: 0.5185 - accuracy: 0.7649\n",
      "Epoch 81/100\n",
      "1666/1666 - 51s - loss: 0.5170 - accuracy: 0.7678\n",
      "Epoch 82/100\n",
      "1666/1666 - 51s - loss: 0.5176 - accuracy: 0.7658\n",
      "Epoch 83/100\n",
      "1666/1666 - 51s - loss: 0.5171 - accuracy: 0.7675\n",
      "Epoch 84/100\n",
      "1666/1666 - 51s - loss: 0.5158 - accuracy: 0.7668\n",
      "Epoch 85/100\n",
      "1666/1666 - 51s - loss: 0.5150 - accuracy: 0.7667\n",
      "Epoch 86/100\n",
      "1666/1666 - 51s - loss: 0.5158 - accuracy: 0.7666\n",
      "Epoch 87/100\n",
      "1666/1666 - 51s - loss: 0.5164 - accuracy: 0.7663\n",
      "Epoch 88/100\n",
      "1666/1666 - 51s - loss: 0.5149 - accuracy: 0.7675\n",
      "Epoch 89/100\n",
      "1666/1666 - 51s - loss: 0.5152 - accuracy: 0.7668\n",
      "Epoch 90/100\n",
      "1666/1666 - 51s - loss: 0.5144 - accuracy: 0.7684\n",
      "Epoch 91/100\n",
      "1666/1666 - 51s - loss: 0.5148 - accuracy: 0.7664\n",
      "Epoch 92/100\n",
      "1666/1666 - 51s - loss: 0.5150 - accuracy: 0.7660\n",
      "Epoch 93/100\n",
      "1666/1666 - 51s - loss: 0.5134 - accuracy: 0.7673\n",
      "Epoch 94/100\n",
      "1666/1666 - 51s - loss: 0.5131 - accuracy: 0.7673\n",
      "Epoch 95/100\n",
      "1666/1666 - 51s - loss: 0.5125 - accuracy: 0.7691\n",
      "Epoch 96/100\n",
      "1666/1666 - 51s - loss: 0.5120 - accuracy: 0.7688\n",
      "Epoch 97/100\n",
      "1666/1666 - 51s - loss: 0.5127 - accuracy: 0.7687\n",
      "Epoch 98/100\n",
      "1666/1666 - 51s - loss: 0.5145 - accuracy: 0.7678\n",
      "Epoch 99/100\n",
      "1666/1666 - 51s - loss: 0.5122 - accuracy: 0.7691\n",
      "Epoch 100/100\n",
      "1666/1666 - 50s - loss: 0.5111 - accuracy: 0.7671\n",
      "417/417 - 3s - loss: 0.5310 - accuracy: 0.7405\n",
      "Epoch 1/100\n",
      "2082/2082 - 64s - loss: 0.7667 - accuracy: 0.7399\n",
      "Epoch 2/100\n",
      "2082/2082 - 63s - loss: 0.7431 - accuracy: 0.7406\n",
      "Epoch 3/100\n",
      "2082/2082 - 63s - loss: 0.7197 - accuracy: 0.7406\n",
      "Epoch 4/100\n",
      "2082/2082 - 62s - loss: 0.6736 - accuracy: 0.7404\n",
      "Epoch 5/100\n",
      "2082/2082 - 62s - loss: 0.6428 - accuracy: 0.7410\n",
      "Epoch 6/100\n",
      "2082/2082 - 62s - loss: 0.6317 - accuracy: 0.7416\n",
      "Epoch 7/100\n",
      "2082/2082 - 63s - loss: 0.6245 - accuracy: 0.7423\n",
      "Epoch 8/100\n",
      "2082/2082 - 62s - loss: 0.6193 - accuracy: 0.7431\n",
      "Epoch 9/100\n",
      "2082/2082 - 63s - loss: 0.6129 - accuracy: 0.7427\n",
      "Epoch 10/100\n",
      "2082/2082 - 63s - loss: 0.6077 - accuracy: 0.7440\n",
      "Epoch 11/100\n",
      "2082/2082 - 63s - loss: 0.5998 - accuracy: 0.7442\n",
      "Epoch 12/100\n",
      "2082/2082 - 63s - loss: 0.5922 - accuracy: 0.7450\n",
      "Epoch 13/100\n",
      "2082/2082 - 63s - loss: 0.5857 - accuracy: 0.7461\n",
      "Epoch 14/100\n",
      "2082/2082 - 63s - loss: 0.5806 - accuracy: 0.7464\n",
      "Epoch 15/100\n",
      "2082/2082 - 63s - loss: 0.5745 - accuracy: 0.7467\n",
      "Epoch 16/100\n",
      "2082/2082 - 63s - loss: 0.5707 - accuracy: 0.7481\n",
      "Epoch 17/100\n",
      "2082/2082 - 63s - loss: 0.5668 - accuracy: 0.7482\n",
      "Epoch 18/100\n",
      "2082/2082 - 63s - loss: 0.5634 - accuracy: 0.7501\n",
      "Epoch 19/100\n",
      "2082/2082 - 63s - loss: 0.5614 - accuracy: 0.7490\n",
      "Epoch 20/100\n",
      "2082/2082 - 63s - loss: 0.5580 - accuracy: 0.7514\n",
      "Epoch 21/100\n",
      "2082/2082 - 63s - loss: 0.5561 - accuracy: 0.7516\n",
      "Epoch 22/100\n",
      "2082/2082 - 63s - loss: 0.5525 - accuracy: 0.7527\n",
      "Epoch 23/100\n",
      "2082/2082 - 63s - loss: 0.5509 - accuracy: 0.7534\n",
      "Epoch 24/100\n",
      "2082/2082 - 63s - loss: 0.5490 - accuracy: 0.7538\n",
      "Epoch 25/100\n",
      "2082/2082 - 63s - loss: 0.5475 - accuracy: 0.7548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100\n",
      "2082/2082 - 63s - loss: 0.5459 - accuracy: 0.7529\n",
      "Epoch 27/100\n",
      "2082/2082 - 63s - loss: 0.5443 - accuracy: 0.7549\n",
      "Epoch 28/100\n",
      "2082/2082 - 63s - loss: 0.5423 - accuracy: 0.7567\n",
      "Epoch 29/100\n",
      "2082/2082 - 63s - loss: 0.5420 - accuracy: 0.7566\n",
      "Epoch 30/100\n",
      "2082/2082 - 63s - loss: 0.5404 - accuracy: 0.7560\n",
      "Epoch 31/100\n",
      "2082/2082 - 63s - loss: 0.5390 - accuracy: 0.7580\n",
      "Epoch 32/100\n",
      "2082/2082 - 63s - loss: 0.5378 - accuracy: 0.7572\n",
      "Epoch 33/100\n",
      "2082/2082 - 63s - loss: 0.5365 - accuracy: 0.7582\n",
      "Epoch 34/100\n",
      "2082/2082 - 63s - loss: 0.5355 - accuracy: 0.7585\n",
      "Epoch 35/100\n",
      "2082/2082 - 63s - loss: 0.5353 - accuracy: 0.7577\n",
      "Epoch 36/100\n",
      "2082/2082 - 63s - loss: 0.5341 - accuracy: 0.7591\n",
      "Epoch 37/100\n",
      "2082/2082 - 63s - loss: 0.5328 - accuracy: 0.7601\n",
      "Epoch 38/100\n",
      "2082/2082 - 63s - loss: 0.5312 - accuracy: 0.7588\n",
      "Epoch 39/100\n",
      "2082/2082 - 63s - loss: 0.5312 - accuracy: 0.7600\n",
      "Epoch 40/100\n",
      "2082/2082 - 63s - loss: 0.5300 - accuracy: 0.7610\n",
      "Epoch 41/100\n",
      "2082/2082 - 63s - loss: 0.5300 - accuracy: 0.7593\n",
      "Epoch 42/100\n",
      "2082/2082 - 63s - loss: 0.5296 - accuracy: 0.7615\n",
      "Epoch 43/100\n",
      "2082/2082 - 63s - loss: 0.5276 - accuracy: 0.7608\n",
      "Epoch 44/100\n",
      "2082/2082 - 63s - loss: 0.5274 - accuracy: 0.7618\n",
      "Epoch 45/100\n",
      "2082/2082 - 66s - loss: 0.5264 - accuracy: 0.7621\n",
      "Epoch 46/100\n",
      "2082/2082 - 63s - loss: 0.5258 - accuracy: 0.7611\n",
      "Epoch 47/100\n",
      "2082/2082 - 63s - loss: 0.5257 - accuracy: 0.7606\n",
      "Epoch 48/100\n",
      "2082/2082 - 63s - loss: 0.5248 - accuracy: 0.7619\n",
      "Epoch 49/100\n",
      "2082/2082 - 63s - loss: 0.5245 - accuracy: 0.7626\n",
      "Epoch 50/100\n",
      "2082/2082 - 63s - loss: 0.5245 - accuracy: 0.7613\n",
      "Epoch 51/100\n",
      "2082/2082 - 65s - loss: 0.5240 - accuracy: 0.7625\n",
      "Epoch 52/100\n",
      "2082/2082 - 63s - loss: 0.5229 - accuracy: 0.7619\n",
      "Epoch 53/100\n",
      "2082/2082 - 63s - loss: 0.5220 - accuracy: 0.7629\n",
      "Epoch 54/100\n",
      "2082/2082 - 63s - loss: 0.5219 - accuracy: 0.7638\n",
      "Epoch 55/100\n",
      "2082/2082 - 63s - loss: 0.5222 - accuracy: 0.7630\n",
      "Epoch 56/100\n",
      "2082/2082 - 63s - loss: 0.5220 - accuracy: 0.7626\n",
      "Epoch 57/100\n",
      "2082/2082 - 63s - loss: 0.5203 - accuracy: 0.7636\n",
      "Epoch 58/100\n",
      "2082/2082 - 62s - loss: 0.5201 - accuracy: 0.7638\n",
      "Epoch 59/100\n",
      "2082/2082 - 62s - loss: 0.5195 - accuracy: 0.7635\n",
      "Epoch 60/100\n",
      "2082/2082 - 62s - loss: 0.5194 - accuracy: 0.7629\n",
      "Epoch 61/100\n",
      "2082/2082 - 62s - loss: 0.5186 - accuracy: 0.7631\n",
      "Epoch 62/100\n",
      "2082/2082 - 62s - loss: 0.5187 - accuracy: 0.7656\n",
      "Epoch 63/100\n",
      "2082/2082 - 62s - loss: 0.5172 - accuracy: 0.7643\n",
      "Epoch 64/100\n",
      "2082/2082 - 62s - loss: 0.5174 - accuracy: 0.7642\n",
      "Epoch 65/100\n",
      "2082/2082 - 62s - loss: 0.5170 - accuracy: 0.7670\n",
      "Epoch 66/100\n",
      "2082/2082 - 62s - loss: 0.5171 - accuracy: 0.7644\n",
      "Epoch 67/100\n",
      "2082/2082 - 62s - loss: 0.5165 - accuracy: 0.7647\n",
      "Epoch 68/100\n",
      "2082/2082 - 62s - loss: 0.5156 - accuracy: 0.7663\n",
      "Epoch 69/100\n",
      "2082/2082 - 62s - loss: 0.5148 - accuracy: 0.7653\n",
      "Epoch 70/100\n",
      "2082/2082 - 63s - loss: 0.5160 - accuracy: 0.7645\n",
      "Epoch 71/100\n",
      "2082/2082 - 62s - loss: 0.5147 - accuracy: 0.7650\n",
      "Epoch 72/100\n",
      "2082/2082 - 63s - loss: 0.5151 - accuracy: 0.7660\n",
      "Epoch 73/100\n",
      "2082/2082 - 62s - loss: 0.5148 - accuracy: 0.7654\n",
      "Epoch 74/100\n",
      "2082/2082 - 62s - loss: 0.5141 - accuracy: 0.7671\n",
      "Epoch 75/100\n",
      "2082/2082 - 62s - loss: 0.5131 - accuracy: 0.7668\n",
      "Epoch 76/100\n",
      "2082/2082 - 62s - loss: 0.5127 - accuracy: 0.7654\n",
      "Epoch 77/100\n",
      "2082/2082 - 62s - loss: 0.5125 - accuracy: 0.7673\n",
      "Epoch 78/100\n",
      "2082/2082 - 62s - loss: 0.5124 - accuracy: 0.7669\n",
      "Epoch 79/100\n",
      "2082/2082 - 62s - loss: 0.5123 - accuracy: 0.7656\n",
      "Epoch 80/100\n",
      "2082/2082 - 63s - loss: 0.5114 - accuracy: 0.7672\n",
      "Epoch 81/100\n",
      "2082/2082 - 62s - loss: 0.5118 - accuracy: 0.7664\n",
      "Epoch 82/100\n",
      "2082/2082 - 62s - loss: 0.5109 - accuracy: 0.7676\n",
      "Epoch 83/100\n",
      "2082/2082 - 62s - loss: 0.5113 - accuracy: 0.7674\n",
      "Epoch 84/100\n",
      "2082/2082 - 62s - loss: 0.5103 - accuracy: 0.7688\n",
      "Epoch 85/100\n",
      "2082/2082 - 62s - loss: 0.5100 - accuracy: 0.7684\n",
      "Epoch 86/100\n",
      "2082/2082 - 63s - loss: 0.5102 - accuracy: 0.7674\n",
      "Epoch 87/100\n",
      "2082/2082 - 63s - loss: 0.5094 - accuracy: 0.7672\n",
      "Epoch 88/100\n",
      "2082/2082 - 63s - loss: 0.5089 - accuracy: 0.7693\n",
      "Epoch 89/100\n",
      "2082/2082 - 63s - loss: 0.5095 - accuracy: 0.7682\n",
      "Epoch 90/100\n",
      "2082/2082 - 63s - loss: 0.5089 - accuracy: 0.7686\n",
      "Epoch 91/100\n",
      "2082/2082 - 63s - loss: 0.5088 - accuracy: 0.7695\n",
      "Epoch 92/100\n",
      "2082/2082 - 63s - loss: 0.5085 - accuracy: 0.7680\n",
      "Epoch 93/100\n",
      "2082/2082 - 63s - loss: 0.5083 - accuracy: 0.7684\n",
      "Epoch 94/100\n",
      "2082/2082 - 63s - loss: 0.5080 - accuracy: 0.7691\n",
      "Epoch 95/100\n",
      "2082/2082 - 63s - loss: 0.5074 - accuracy: 0.7701\n",
      "Epoch 96/100\n",
      "2082/2082 - 63s - loss: 0.5071 - accuracy: 0.7689\n",
      "Epoch 97/100\n",
      "2082/2082 - 63s - loss: 0.5065 - accuracy: 0.7703\n",
      "Epoch 98/100\n",
      "2082/2082 - 63s - loss: 0.5062 - accuracy: 0.7709\n",
      "Epoch 99/100\n",
      "2082/2082 - 63s - loss: 0.5066 - accuracy: 0.7694\n",
      "Epoch 100/100\n",
      "2082/2082 - 63s - loss: 0.5060 - accuracy: 0.7708\n",
      "417/417 - 3s - loss: 0.6137 - accuracy: 0.7399\n",
      "Epoch 1/100\n",
      "417/417 - 13s - loss: 0.8399 - accuracy: 0.7071\n",
      "Epoch 2/100\n",
      "417/417 - 13s - loss: 0.7550 - accuracy: 0.7389\n",
      "Epoch 3/100\n",
      "417/417 - 13s - loss: 0.7547 - accuracy: 0.7389\n",
      "Epoch 4/100\n",
      "417/417 - 13s - loss: 0.7543 - accuracy: 0.7389\n",
      "Epoch 5/100\n",
      "417/417 - 13s - loss: 0.7539 - accuracy: 0.7389\n",
      "Epoch 6/100\n",
      "417/417 - 13s - loss: 0.7533 - accuracy: 0.7389\n",
      "Epoch 7/100\n",
      "417/417 - 13s - loss: 0.7524 - accuracy: 0.7389\n",
      "Epoch 8/100\n",
      "417/417 - 13s - loss: 0.7512 - accuracy: 0.7389\n",
      "Epoch 9/100\n",
      "417/417 - 13s - loss: 0.7501 - accuracy: 0.7389\n",
      "Epoch 10/100\n",
      "417/417 - 13s - loss: 0.7480 - accuracy: 0.7389\n",
      "Epoch 11/100\n",
      "417/417 - 13s - loss: 0.7452 - accuracy: 0.7389\n",
      "Epoch 12/100\n",
      "417/417 - 13s - loss: 0.7417 - accuracy: 0.7389\n",
      "Epoch 13/100\n",
      "417/417 - 13s - loss: 0.7378 - accuracy: 0.7389\n",
      "Epoch 14/100\n",
      "417/417 - 15s - loss: 0.7315 - accuracy: 0.7389\n",
      "Epoch 15/100\n",
      "417/417 - 13s - loss: 0.7248 - accuracy: 0.7389\n",
      "Epoch 16/100\n",
      "417/417 - 13s - loss: 0.7164 - accuracy: 0.7389\n",
      "Epoch 17/100\n",
      "417/417 - 13s - loss: 0.7075 - accuracy: 0.7389\n",
      "Epoch 18/100\n",
      "417/417 - 13s - loss: 0.6987 - accuracy: 0.7389\n",
      "Epoch 19/100\n",
      "417/417 - 13s - loss: 0.6913 - accuracy: 0.7389\n",
      "Epoch 20/100\n",
      "417/417 - 13s - loss: 0.6838 - accuracy: 0.7389\n",
      "Epoch 21/100\n",
      "417/417 - 13s - loss: 0.6776 - accuracy: 0.7388\n",
      "Epoch 22/100\n",
      "417/417 - 13s - loss: 0.6712 - accuracy: 0.7386\n",
      "Epoch 23/100\n",
      "417/417 - 13s - loss: 0.6672 - accuracy: 0.7388\n",
      "Epoch 24/100\n",
      "417/417 - 13s - loss: 0.6629 - accuracy: 0.7391\n",
      "Epoch 25/100\n",
      "417/417 - 13s - loss: 0.6584 - accuracy: 0.7395\n",
      "Epoch 26/100\n",
      "417/417 - 13s - loss: 0.6550 - accuracy: 0.7396\n",
      "Epoch 27/100\n",
      "417/417 - 13s - loss: 0.6515 - accuracy: 0.7415\n",
      "Epoch 28/100\n",
      "417/417 - 13s - loss: 0.6470 - accuracy: 0.7412\n",
      "Epoch 29/100\n",
      "417/417 - 13s - loss: 0.6444 - accuracy: 0.7414\n",
      "Epoch 30/100\n",
      "417/417 - 13s - loss: 0.6429 - accuracy: 0.7407\n",
      "Epoch 31/100\n",
      "417/417 - 13s - loss: 0.6397 - accuracy: 0.7417\n",
      "Epoch 32/100\n",
      "417/417 - 13s - loss: 0.6369 - accuracy: 0.7453\n",
      "Epoch 33/100\n",
      "417/417 - 13s - loss: 0.6349 - accuracy: 0.7437\n",
      "Epoch 34/100\n",
      "417/417 - 13s - loss: 0.6318 - accuracy: 0.7428\n",
      "Epoch 35/100\n",
      "417/417 - 13s - loss: 0.6325 - accuracy: 0.7441\n",
      "Epoch 36/100\n",
      "417/417 - 13s - loss: 0.6277 - accuracy: 0.7437\n",
      "Epoch 37/100\n",
      "417/417 - 13s - loss: 0.6278 - accuracy: 0.7438\n",
      "Epoch 38/100\n",
      "417/417 - 13s - loss: 0.6252 - accuracy: 0.7439\n",
      "Epoch 39/100\n",
      "417/417 - 13s - loss: 0.6229 - accuracy: 0.7423\n",
      "Epoch 40/100\n",
      "417/417 - 13s - loss: 0.6226 - accuracy: 0.7424\n",
      "Epoch 41/100\n",
      "417/417 - 13s - loss: 0.6201 - accuracy: 0.7427\n",
      "Epoch 42/100\n",
      "417/417 - 13s - loss: 0.6196 - accuracy: 0.7430\n",
      "Epoch 43/100\n",
      "417/417 - 13s - loss: 0.6186 - accuracy: 0.7436\n",
      "Epoch 44/100\n",
      "417/417 - 13s - loss: 0.6179 - accuracy: 0.7449\n",
      "Epoch 45/100\n",
      "417/417 - 13s - loss: 0.6158 - accuracy: 0.7450\n",
      "Epoch 46/100\n",
      "417/417 - 13s - loss: 0.6147 - accuracy: 0.7441\n",
      "Epoch 47/100\n",
      "417/417 - 13s - loss: 0.6127 - accuracy: 0.7437\n",
      "Epoch 48/100\n",
      "417/417 - 13s - loss: 0.6126 - accuracy: 0.7432\n",
      "Epoch 49/100\n",
      "417/417 - 13s - loss: 0.6115 - accuracy: 0.7469\n",
      "Epoch 50/100\n",
      "417/417 - 13s - loss: 0.6105 - accuracy: 0.7450\n",
      "Epoch 51/100\n",
      "417/417 - 13s - loss: 0.6085 - accuracy: 0.7449\n",
      "Epoch 52/100\n",
      "417/417 - 13s - loss: 0.6080 - accuracy: 0.7452\n",
      "Epoch 53/100\n",
      "417/417 - 13s - loss: 0.6071 - accuracy: 0.7459\n",
      "Epoch 54/100\n",
      "417/417 - 13s - loss: 0.6059 - accuracy: 0.7473\n",
      "Epoch 55/100\n",
      "417/417 - 13s - loss: 0.6036 - accuracy: 0.7434\n",
      "Epoch 56/100\n",
      "417/417 - 13s - loss: 0.6035 - accuracy: 0.7454\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417/417 - 13s - loss: 0.6011 - accuracy: 0.7483\n",
      "Epoch 58/100\n",
      "417/417 - 13s - loss: 0.6010 - accuracy: 0.7453\n",
      "Epoch 59/100\n",
      "417/417 - 13s - loss: 0.5997 - accuracy: 0.7465\n",
      "Epoch 60/100\n",
      "417/417 - 13s - loss: 0.5988 - accuracy: 0.7477\n",
      "Epoch 61/100\n",
      "417/417 - 13s - loss: 0.5987 - accuracy: 0.7474\n",
      "Epoch 62/100\n",
      "417/417 - 13s - loss: 0.5954 - accuracy: 0.7465\n",
      "Epoch 63/100\n",
      "417/417 - 13s - loss: 0.5954 - accuracy: 0.7481\n",
      "Epoch 64/100\n",
      "417/417 - 13s - loss: 0.5935 - accuracy: 0.7475\n",
      "Epoch 65/100\n",
      "417/417 - 13s - loss: 0.5925 - accuracy: 0.7483\n",
      "Epoch 66/100\n",
      "417/417 - 13s - loss: 0.5921 - accuracy: 0.7474\n",
      "Epoch 67/100\n",
      "417/417 - 13s - loss: 0.5899 - accuracy: 0.7509\n",
      "Epoch 68/100\n",
      "417/417 - 13s - loss: 0.5890 - accuracy: 0.7536\n",
      "Epoch 69/100\n",
      "417/417 - 13s - loss: 0.5896 - accuracy: 0.7498\n",
      "Epoch 70/100\n",
      "417/417 - 13s - loss: 0.5864 - accuracy: 0.7527\n",
      "Epoch 71/100\n",
      "417/417 - 13s - loss: 0.5866 - accuracy: 0.7513\n",
      "Epoch 72/100\n",
      "417/417 - 13s - loss: 0.5837 - accuracy: 0.7523\n",
      "Epoch 73/100\n",
      "417/417 - 13s - loss: 0.5844 - accuracy: 0.7494\n",
      "Epoch 74/100\n",
      "417/417 - 13s - loss: 0.5847 - accuracy: 0.7532\n",
      "Epoch 75/100\n",
      "417/417 - 13s - loss: 0.5819 - accuracy: 0.7544\n",
      "Epoch 76/100\n",
      "417/417 - 13s - loss: 0.5812 - accuracy: 0.7532\n",
      "Epoch 77/100\n",
      "417/417 - 13s - loss: 0.5795 - accuracy: 0.7564\n",
      "Epoch 78/100\n",
      "417/417 - 13s - loss: 0.5794 - accuracy: 0.7541\n",
      "Epoch 79/100\n",
      "417/417 - 13s - loss: 0.5774 - accuracy: 0.7550\n",
      "Epoch 80/100\n",
      "417/417 - 13s - loss: 0.5776 - accuracy: 0.7542\n",
      "Epoch 81/100\n",
      "417/417 - 13s - loss: 0.5754 - accuracy: 0.7581\n",
      "Epoch 82/100\n",
      "417/417 - 13s - loss: 0.5743 - accuracy: 0.7550\n",
      "Epoch 83/100\n",
      "417/417 - 13s - loss: 0.5743 - accuracy: 0.7549\n",
      "Epoch 84/100\n",
      "417/417 - 13s - loss: 0.5735 - accuracy: 0.7558\n",
      "Epoch 85/100\n",
      "417/417 - 13s - loss: 0.5722 - accuracy: 0.7577\n",
      "Epoch 86/100\n",
      "417/417 - 13s - loss: 0.5707 - accuracy: 0.7579\n",
      "Epoch 87/100\n",
      "417/417 - 13s - loss: 0.5692 - accuracy: 0.7583\n",
      "Epoch 88/100\n",
      "417/417 - 13s - loss: 0.5699 - accuracy: 0.7586\n",
      "Epoch 89/100\n",
      "417/417 - 13s - loss: 0.5672 - accuracy: 0.7604\n",
      "Epoch 90/100\n",
      "417/417 - 13s - loss: 0.5674 - accuracy: 0.7576\n",
      "Epoch 91/100\n",
      "417/417 - 13s - loss: 0.5636 - accuracy: 0.7610\n",
      "Epoch 92/100\n",
      "417/417 - 13s - loss: 0.5659 - accuracy: 0.7587\n",
      "Epoch 93/100\n",
      "417/417 - 13s - loss: 0.5649 - accuracy: 0.7586\n",
      "Epoch 94/100\n",
      "417/417 - 13s - loss: 0.5625 - accuracy: 0.7637\n",
      "Epoch 95/100\n",
      "417/417 - 13s - loss: 0.5631 - accuracy: 0.7619\n",
      "Epoch 96/100\n",
      "417/417 - 13s - loss: 0.5624 - accuracy: 0.7609\n",
      "Epoch 97/100\n",
      "417/417 - 13s - loss: 0.5597 - accuracy: 0.7607\n",
      "Epoch 98/100\n",
      "417/417 - 13s - loss: 0.5610 - accuracy: 0.7595\n",
      "Epoch 99/100\n",
      "417/417 - 13s - loss: 0.5589 - accuracy: 0.7616\n",
      "Epoch 100/100\n",
      "417/417 - 13s - loss: 0.5584 - accuracy: 0.7631\n",
      "417/417 - 3s - loss: 0.5859 - accuracy: 0.7507\n",
      "Epoch 1/100\n",
      "833/833 - 25s - loss: 0.7945 - accuracy: 0.7255\n",
      "Epoch 2/100\n",
      "833/833 - 25s - loss: 0.7512 - accuracy: 0.7396\n",
      "Epoch 3/100\n",
      "833/833 - 25s - loss: 0.7486 - accuracy: 0.7396\n",
      "Epoch 4/100\n",
      "833/833 - 25s - loss: 0.7443 - accuracy: 0.7396\n",
      "Epoch 5/100\n",
      "833/833 - 24s - loss: 0.7365 - accuracy: 0.7396\n",
      "Epoch 6/100\n",
      "833/833 - 25s - loss: 0.7214 - accuracy: 0.7396\n",
      "Epoch 7/100\n",
      "833/833 - 25s - loss: 0.7022 - accuracy: 0.7396\n",
      "Epoch 8/100\n",
      "833/833 - 25s - loss: 0.6812 - accuracy: 0.7396\n",
      "Epoch 9/100\n",
      "833/833 - 24s - loss: 0.6618 - accuracy: 0.7396\n",
      "Epoch 10/100\n",
      "833/833 - 24s - loss: 0.6492 - accuracy: 0.7391\n",
      "Epoch 11/100\n",
      "833/833 - 24s - loss: 0.6405 - accuracy: 0.7395\n",
      "Epoch 12/100\n",
      "833/833 - 24s - loss: 0.6346 - accuracy: 0.7402\n",
      "Epoch 13/100\n",
      "833/833 - 25s - loss: 0.6301 - accuracy: 0.7403\n",
      "Epoch 14/100\n",
      "833/833 - 24s - loss: 0.6275 - accuracy: 0.7404\n",
      "Epoch 15/100\n",
      "833/833 - 24s - loss: 0.6255 - accuracy: 0.7410\n",
      "Epoch 16/100\n",
      "833/833 - 25s - loss: 0.6223 - accuracy: 0.7410\n",
      "Epoch 17/100\n",
      "833/833 - 25s - loss: 0.6204 - accuracy: 0.7431\n",
      "Epoch 18/100\n",
      "833/833 - 25s - loss: 0.6180 - accuracy: 0.7433\n",
      "Epoch 19/100\n",
      "833/833 - 25s - loss: 0.6168 - accuracy: 0.7429\n",
      "Epoch 20/100\n",
      "833/833 - 24s - loss: 0.6155 - accuracy: 0.7441\n",
      "Epoch 21/100\n",
      "833/833 - 24s - loss: 0.6121 - accuracy: 0.7434\n",
      "Epoch 22/100\n",
      "833/833 - 24s - loss: 0.6107 - accuracy: 0.7449\n",
      "Epoch 23/100\n",
      "833/833 - 24s - loss: 0.6090 - accuracy: 0.7455\n",
      "Epoch 24/100\n",
      "833/833 - 25s - loss: 0.6079 - accuracy: 0.7458\n",
      "Epoch 25/100\n",
      "833/833 - 25s - loss: 0.6060 - accuracy: 0.7459\n",
      "Epoch 26/100\n",
      "833/833 - 25s - loss: 0.6050 - accuracy: 0.7468\n",
      "Epoch 27/100\n",
      "833/833 - 25s - loss: 0.6025 - accuracy: 0.7483\n",
      "Epoch 28/100\n",
      "833/833 - 25s - loss: 0.6014 - accuracy: 0.7490\n",
      "Epoch 29/100\n",
      "833/833 - 25s - loss: 0.5988 - accuracy: 0.7486\n",
      "Epoch 30/100\n",
      "833/833 - 25s - loss: 0.5987 - accuracy: 0.7495\n",
      "Epoch 31/100\n",
      "833/833 - 25s - loss: 0.5963 - accuracy: 0.7488\n",
      "Epoch 32/100\n",
      "833/833 - 25s - loss: 0.5940 - accuracy: 0.7508\n",
      "Epoch 33/100\n",
      "833/833 - 25s - loss: 0.5918 - accuracy: 0.7505\n",
      "Epoch 34/100\n",
      "833/833 - 25s - loss: 0.5914 - accuracy: 0.7529\n",
      "Epoch 35/100\n",
      "833/833 - 25s - loss: 0.5896 - accuracy: 0.7525\n",
      "Epoch 36/100\n",
      "833/833 - 25s - loss: 0.5886 - accuracy: 0.7539\n",
      "Epoch 37/100\n",
      "833/833 - 25s - loss: 0.5875 - accuracy: 0.7510\n",
      "Epoch 38/100\n",
      "833/833 - 25s - loss: 0.5861 - accuracy: 0.7520\n",
      "Epoch 39/100\n",
      "833/833 - 25s - loss: 0.5852 - accuracy: 0.7539\n",
      "Epoch 40/100\n",
      "833/833 - 25s - loss: 0.5835 - accuracy: 0.7550\n",
      "Epoch 41/100\n",
      "833/833 - 25s - loss: 0.5828 - accuracy: 0.7536\n",
      "Epoch 42/100\n",
      "833/833 - 25s - loss: 0.5808 - accuracy: 0.7551\n",
      "Epoch 43/100\n",
      "833/833 - 25s - loss: 0.5802 - accuracy: 0.7553\n",
      "Epoch 44/100\n",
      "833/833 - 25s - loss: 0.5782 - accuracy: 0.7566\n",
      "Epoch 45/100\n",
      "833/833 - 25s - loss: 0.5772 - accuracy: 0.7570\n",
      "Epoch 46/100\n",
      "833/833 - 25s - loss: 0.5756 - accuracy: 0.7551\n",
      "Epoch 47/100\n",
      "833/833 - 25s - loss: 0.5757 - accuracy: 0.7565\n",
      "Epoch 48/100\n",
      "833/833 - 25s - loss: 0.5758 - accuracy: 0.7539\n",
      "Epoch 49/100\n",
      "833/833 - 25s - loss: 0.5730 - accuracy: 0.7568\n",
      "Epoch 50/100\n",
      "833/833 - 25s - loss: 0.5745 - accuracy: 0.7569\n",
      "Epoch 51/100\n",
      "833/833 - 25s - loss: 0.5723 - accuracy: 0.7553\n",
      "Epoch 52/100\n",
      "833/833 - 25s - loss: 0.5723 - accuracy: 0.7562\n",
      "Epoch 53/100\n",
      "833/833 - 25s - loss: 0.5706 - accuracy: 0.7581\n",
      "Epoch 54/100\n",
      "833/833 - 25s - loss: 0.5692 - accuracy: 0.7558\n",
      "Epoch 55/100\n",
      "833/833 - 25s - loss: 0.5683 - accuracy: 0.7570\n",
      "Epoch 56/100\n",
      "833/833 - 25s - loss: 0.5675 - accuracy: 0.7577\n",
      "Epoch 57/100\n",
      "833/833 - 25s - loss: 0.5671 - accuracy: 0.7586\n",
      "Epoch 58/100\n",
      "833/833 - 25s - loss: 0.5661 - accuracy: 0.7570\n",
      "Epoch 59/100\n",
      "833/833 - 25s - loss: 0.5651 - accuracy: 0.7569\n",
      "Epoch 60/100\n",
      "833/833 - 25s - loss: 0.5638 - accuracy: 0.7578\n",
      "Epoch 61/100\n",
      "833/833 - 25s - loss: 0.5619 - accuracy: 0.7601\n",
      "Epoch 62/100\n",
      "833/833 - 25s - loss: 0.5619 - accuracy: 0.7592\n",
      "Epoch 63/100\n",
      "833/833 - 25s - loss: 0.5608 - accuracy: 0.7599\n",
      "Epoch 64/100\n",
      "833/833 - 25s - loss: 0.5605 - accuracy: 0.7598\n",
      "Epoch 65/100\n",
      "833/833 - 25s - loss: 0.5595 - accuracy: 0.7599\n",
      "Epoch 66/100\n",
      "833/833 - 25s - loss: 0.5583 - accuracy: 0.7596\n",
      "Epoch 67/100\n",
      "833/833 - 25s - loss: 0.5585 - accuracy: 0.7605\n",
      "Epoch 68/100\n",
      "833/833 - 25s - loss: 0.5559 - accuracy: 0.7623\n",
      "Epoch 69/100\n",
      "833/833 - 25s - loss: 0.5568 - accuracy: 0.7613\n",
      "Epoch 70/100\n",
      "833/833 - 25s - loss: 0.5547 - accuracy: 0.7610\n",
      "Epoch 71/100\n",
      "833/833 - 25s - loss: 0.5539 - accuracy: 0.7623\n",
      "Epoch 72/100\n",
      "833/833 - 25s - loss: 0.5549 - accuracy: 0.7619\n",
      "Epoch 73/100\n",
      "833/833 - 25s - loss: 0.5516 - accuracy: 0.7623\n",
      "Epoch 74/100\n",
      "833/833 - 25s - loss: 0.5521 - accuracy: 0.7621\n",
      "Epoch 75/100\n",
      "833/833 - 25s - loss: 0.5515 - accuracy: 0.7619\n",
      "Epoch 76/100\n",
      "833/833 - 25s - loss: 0.5508 - accuracy: 0.7637\n",
      "Epoch 77/100\n",
      "833/833 - 25s - loss: 0.5498 - accuracy: 0.7626\n",
      "Epoch 78/100\n",
      "833/833 - 25s - loss: 0.5493 - accuracy: 0.7638\n",
      "Epoch 79/100\n",
      "833/833 - 25s - loss: 0.5481 - accuracy: 0.7620\n",
      "Epoch 80/100\n",
      "833/833 - 25s - loss: 0.5479 - accuracy: 0.7612\n",
      "Epoch 81/100\n",
      "833/833 - 25s - loss: 0.5453 - accuracy: 0.7640\n",
      "Epoch 82/100\n",
      "833/833 - 25s - loss: 0.5459 - accuracy: 0.7633\n",
      "Epoch 83/100\n",
      "833/833 - 25s - loss: 0.5456 - accuracy: 0.7649\n",
      "Epoch 84/100\n",
      "833/833 - 25s - loss: 0.5448 - accuracy: 0.7676\n",
      "Epoch 85/100\n",
      "833/833 - 25s - loss: 0.5448 - accuracy: 0.7648\n",
      "Epoch 86/100\n",
      "833/833 - 25s - loss: 0.5445 - accuracy: 0.7649\n",
      "Epoch 87/100\n",
      "833/833 - 25s - loss: 0.5427 - accuracy: 0.7658\n",
      "Epoch 88/100\n",
      "833/833 - 25s - loss: 0.5421 - accuracy: 0.7641\n",
      "Epoch 89/100\n",
      "833/833 - 25s - loss: 0.5417 - accuracy: 0.7660\n",
      "Epoch 90/100\n",
      "833/833 - 25s - loss: 0.5410 - accuracy: 0.7655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/100\n",
      "833/833 - 28s - loss: 0.5421 - accuracy: 0.7644\n",
      "Epoch 92/100\n",
      "833/833 - 25s - loss: 0.5396 - accuracy: 0.7697\n",
      "Epoch 93/100\n",
      "833/833 - 25s - loss: 0.5394 - accuracy: 0.7661\n",
      "Epoch 94/100\n",
      "833/833 - 25s - loss: 0.5395 - accuracy: 0.7681\n",
      "Epoch 95/100\n",
      "833/833 - 25s - loss: 0.5378 - accuracy: 0.7659\n",
      "Epoch 96/100\n",
      "833/833 - 25s - loss: 0.5380 - accuracy: 0.7661\n",
      "Epoch 97/100\n",
      "833/833 - 25s - loss: 0.5366 - accuracy: 0.7676\n",
      "Epoch 98/100\n",
      "833/833 - 25s - loss: 0.5371 - accuracy: 0.7673\n",
      "Epoch 99/100\n",
      "833/833 - 25s - loss: 0.5353 - accuracy: 0.7677\n",
      "Epoch 100/100\n",
      "833/833 - 25s - loss: 0.5344 - accuracy: 0.7677\n",
      "417/417 - 3s - loss: 0.5369 - accuracy: 0.7613\n",
      "Epoch 1/100\n",
      "1249/1249 - 38s - loss: 0.7742 - accuracy: 0.7356\n",
      "Epoch 2/100\n",
      "1249/1249 - 37s - loss: 0.7388 - accuracy: 0.7441\n",
      "Epoch 3/100\n",
      "1249/1249 - 37s - loss: 0.7271 - accuracy: 0.7441\n",
      "Epoch 4/100\n",
      "1249/1249 - 37s - loss: 0.7030 - accuracy: 0.7441\n",
      "Epoch 5/100\n",
      "1249/1249 - 37s - loss: 0.6691 - accuracy: 0.7440\n",
      "Epoch 6/100\n",
      "1249/1249 - 37s - loss: 0.6433 - accuracy: 0.7437\n",
      "Epoch 7/100\n",
      "1249/1249 - 37s - loss: 0.6325 - accuracy: 0.7451\n",
      "Epoch 8/100\n",
      "1249/1249 - 37s - loss: 0.6253 - accuracy: 0.7462\n",
      "Epoch 9/100\n",
      "1249/1249 - 37s - loss: 0.6210 - accuracy: 0.7477\n",
      "Epoch 10/100\n",
      "1249/1249 - 37s - loss: 0.6173 - accuracy: 0.7481\n",
      "Epoch 11/100\n",
      "1249/1249 - 37s - loss: 0.6132 - accuracy: 0.7487\n",
      "Epoch 12/100\n",
      "1249/1249 - 37s - loss: 0.6104 - accuracy: 0.7499\n",
      "Epoch 13/100\n",
      "1249/1249 - 37s - loss: 0.6064 - accuracy: 0.7508\n",
      "Epoch 14/100\n",
      "1249/1249 - 37s - loss: 0.6037 - accuracy: 0.7514\n",
      "Epoch 15/100\n",
      "1249/1249 - 37s - loss: 0.6009 - accuracy: 0.7528\n",
      "Epoch 16/100\n",
      "1249/1249 - 37s - loss: 0.5988 - accuracy: 0.7523\n",
      "Epoch 17/100\n",
      "1249/1249 - 37s - loss: 0.5963 - accuracy: 0.7527\n",
      "Epoch 18/100\n",
      "1249/1249 - 37s - loss: 0.5938 - accuracy: 0.7529\n",
      "Epoch 19/100\n",
      "1249/1249 - 37s - loss: 0.5908 - accuracy: 0.7540\n",
      "Epoch 20/100\n",
      "1249/1249 - 37s - loss: 0.5882 - accuracy: 0.7543\n",
      "Epoch 21/100\n",
      "1249/1249 - 37s - loss: 0.5859 - accuracy: 0.7554\n",
      "Epoch 22/100\n",
      "1249/1249 - 37s - loss: 0.5836 - accuracy: 0.7550\n",
      "Epoch 23/100\n",
      "1249/1249 - 38s - loss: 0.5812 - accuracy: 0.7556\n",
      "Epoch 24/100\n",
      "1249/1249 - 38s - loss: 0.5794 - accuracy: 0.7553\n",
      "Epoch 25/100\n",
      "1249/1249 - 38s - loss: 0.5780 - accuracy: 0.7564\n",
      "Epoch 26/100\n",
      "1249/1249 - 38s - loss: 0.5741 - accuracy: 0.7563\n",
      "Epoch 27/100\n",
      "1249/1249 - 37s - loss: 0.5703 - accuracy: 0.7571\n",
      "Epoch 28/100\n",
      "1249/1249 - 38s - loss: 0.5693 - accuracy: 0.7583\n",
      "Epoch 29/100\n",
      "1249/1249 - 38s - loss: 0.5655 - accuracy: 0.7579\n",
      "Epoch 30/100\n",
      "1249/1249 - 38s - loss: 0.5636 - accuracy: 0.7593\n",
      "Epoch 31/100\n",
      "1249/1249 - 38s - loss: 0.5623 - accuracy: 0.7593\n",
      "Epoch 32/100\n",
      "1249/1249 - 38s - loss: 0.5613 - accuracy: 0.7582\n",
      "Epoch 33/100\n",
      "1249/1249 - 38s - loss: 0.5584 - accuracy: 0.7610\n",
      "Epoch 34/100\n",
      "1249/1249 - 38s - loss: 0.5566 - accuracy: 0.7604\n",
      "Epoch 35/100\n",
      "1249/1249 - 38s - loss: 0.5542 - accuracy: 0.7616\n",
      "Epoch 36/100\n",
      "1249/1249 - 38s - loss: 0.5535 - accuracy: 0.7612\n",
      "Epoch 37/100\n",
      "1249/1249 - 38s - loss: 0.5520 - accuracy: 0.7603\n",
      "Epoch 38/100\n",
      "1249/1249 - 38s - loss: 0.5497 - accuracy: 0.7625\n",
      "Epoch 39/100\n",
      "1249/1249 - 38s - loss: 0.5489 - accuracy: 0.7618\n",
      "Epoch 40/100\n",
      "1249/1249 - 38s - loss: 0.5462 - accuracy: 0.7653\n",
      "Epoch 41/100\n",
      "1249/1249 - 38s - loss: 0.5459 - accuracy: 0.7622\n",
      "Epoch 42/100\n",
      "1249/1249 - 38s - loss: 0.5455 - accuracy: 0.7639\n",
      "Epoch 43/100\n",
      "1249/1249 - 38s - loss: 0.5442 - accuracy: 0.7642\n",
      "Epoch 44/100\n",
      "1249/1249 - 38s - loss: 0.5439 - accuracy: 0.7646\n",
      "Epoch 45/100\n",
      "1249/1249 - 38s - loss: 0.5420 - accuracy: 0.7647\n",
      "Epoch 46/100\n",
      "1249/1249 - 38s - loss: 0.5419 - accuracy: 0.7635\n",
      "Epoch 47/100\n",
      "1249/1249 - 38s - loss: 0.5409 - accuracy: 0.7661\n",
      "Epoch 48/100\n",
      "1249/1249 - 38s - loss: 0.5395 - accuracy: 0.7649\n",
      "Epoch 49/100\n",
      "1249/1249 - 38s - loss: 0.5393 - accuracy: 0.7645\n",
      "Epoch 50/100\n",
      "1249/1249 - 38s - loss: 0.5382 - accuracy: 0.7663\n",
      "Epoch 51/100\n",
      "1249/1249 - 38s - loss: 0.5382 - accuracy: 0.7655\n",
      "Epoch 52/100\n",
      "1249/1249 - 38s - loss: 0.5379 - accuracy: 0.7665\n",
      "Epoch 53/100\n",
      "1249/1249 - 38s - loss: 0.5369 - accuracy: 0.7657\n",
      "Epoch 54/100\n",
      "1249/1249 - 38s - loss: 0.5378 - accuracy: 0.7657\n",
      "Epoch 55/100\n",
      "1249/1249 - 38s - loss: 0.5362 - accuracy: 0.7675\n",
      "Epoch 56/100\n",
      "1249/1249 - 38s - loss: 0.5363 - accuracy: 0.7665\n",
      "Epoch 57/100\n",
      "1249/1249 - 38s - loss: 0.5366 - accuracy: 0.7663\n",
      "Epoch 58/100\n",
      "1249/1249 - 38s - loss: 0.5357 - accuracy: 0.7659\n",
      "Epoch 59/100\n",
      "1249/1249 - 38s - loss: 0.5363 - accuracy: 0.7673\n",
      "Epoch 60/100\n",
      "1249/1249 - 38s - loss: 0.5353 - accuracy: 0.7677\n",
      "Epoch 61/100\n",
      "1249/1249 - 38s - loss: 0.5334 - accuracy: 0.7675\n",
      "Epoch 62/100\n",
      "1249/1249 - 38s - loss: 0.5347 - accuracy: 0.7659\n",
      "Epoch 63/100\n",
      "1249/1249 - 38s - loss: 0.5328 - accuracy: 0.7673\n",
      "Epoch 64/100\n",
      "1249/1249 - 38s - loss: 0.5326 - accuracy: 0.7680\n",
      "Epoch 65/100\n",
      "1249/1249 - 38s - loss: 0.5331 - accuracy: 0.7678\n",
      "Epoch 66/100\n",
      "1249/1249 - 38s - loss: 0.5327 - accuracy: 0.7686\n",
      "Epoch 67/100\n",
      "1249/1249 - 38s - loss: 0.5326 - accuracy: 0.7684\n",
      "Epoch 68/100\n",
      "1249/1249 - 38s - loss: 0.5313 - accuracy: 0.7664\n",
      "Epoch 69/100\n",
      "1249/1249 - 38s - loss: 0.5306 - accuracy: 0.7675\n",
      "Epoch 70/100\n",
      "1249/1249 - 38s - loss: 0.5304 - accuracy: 0.7671\n",
      "Epoch 71/100\n",
      "1249/1249 - 38s - loss: 0.5291 - accuracy: 0.7702\n",
      "Epoch 72/100\n",
      "1249/1249 - 38s - loss: 0.5282 - accuracy: 0.7687\n",
      "Epoch 73/100\n",
      "1249/1249 - 38s - loss: 0.5282 - accuracy: 0.7682\n",
      "Epoch 74/100\n",
      "1249/1249 - 38s - loss: 0.5278 - accuracy: 0.7691\n",
      "Epoch 75/100\n",
      "1249/1249 - 38s - loss: 0.5279 - accuracy: 0.7690\n",
      "Epoch 76/100\n",
      "1249/1249 - 38s - loss: 0.5286 - accuracy: 0.7688\n",
      "Epoch 77/100\n",
      "1249/1249 - 38s - loss: 0.5265 - accuracy: 0.7680\n",
      "Epoch 78/100\n",
      "1249/1249 - 38s - loss: 0.5268 - accuracy: 0.7705\n",
      "Epoch 79/100\n",
      "1249/1249 - 38s - loss: 0.5261 - accuracy: 0.7693\n",
      "Epoch 80/100\n",
      "1249/1249 - 38s - loss: 0.5254 - accuracy: 0.7697\n",
      "Epoch 81/100\n",
      "1249/1249 - 38s - loss: 0.5254 - accuracy: 0.7687\n",
      "Epoch 82/100\n",
      "1249/1249 - 38s - loss: 0.5250 - accuracy: 0.7716\n",
      "Epoch 83/100\n",
      "1249/1249 - 38s - loss: 0.5236 - accuracy: 0.7710\n",
      "Epoch 84/100\n",
      "1249/1249 - 38s - loss: 0.5244 - accuracy: 0.7705\n",
      "Epoch 85/100\n",
      "1249/1249 - 38s - loss: 0.5241 - accuracy: 0.7706\n",
      "Epoch 86/100\n",
      "1249/1249 - 38s - loss: 0.5231 - accuracy: 0.7710\n",
      "Epoch 87/100\n",
      "1249/1249 - 38s - loss: 0.5224 - accuracy: 0.7723\n",
      "Epoch 88/100\n",
      "1249/1249 - 38s - loss: 0.5234 - accuracy: 0.7712\n",
      "Epoch 89/100\n",
      "1249/1249 - 38s - loss: 0.5217 - accuracy: 0.7698\n",
      "Epoch 90/100\n",
      "1249/1249 - 38s - loss: 0.5220 - accuracy: 0.7707\n",
      "Epoch 91/100\n",
      "1249/1249 - 38s - loss: 0.5211 - accuracy: 0.7733\n",
      "Epoch 92/100\n",
      "1249/1249 - 38s - loss: 0.5217 - accuracy: 0.7726\n",
      "Epoch 93/100\n",
      "1249/1249 - 38s - loss: 0.5206 - accuracy: 0.7731\n",
      "Epoch 94/100\n",
      "1249/1249 - 38s - loss: 0.5209 - accuracy: 0.7713\n",
      "Epoch 95/100\n",
      "1249/1249 - 38s - loss: 0.5208 - accuracy: 0.7708\n",
      "Epoch 96/100\n",
      "1249/1249 - 38s - loss: 0.5203 - accuracy: 0.7726\n",
      "Epoch 97/100\n",
      "1249/1249 - 38s - loss: 0.5203 - accuracy: 0.7718\n",
      "Epoch 98/100\n",
      "1249/1249 - 38s - loss: 0.5199 - accuracy: 0.7722\n",
      "Epoch 99/100\n",
      "1249/1249 - 38s - loss: 0.5188 - accuracy: 0.7734\n",
      "Epoch 100/100\n",
      "1249/1249 - 38s - loss: 0.5186 - accuracy: 0.7743\n",
      "417/417 - 3s - loss: 0.5788 - accuracy: 0.7333\n",
      "Epoch 1/100\n",
      "1666/1666 - 52s - loss: 0.7804 - accuracy: 0.7346\n",
      "Epoch 2/100\n",
      "1666/1666 - 50s - loss: 0.7479 - accuracy: 0.7413\n",
      "Epoch 3/100\n",
      "1666/1666 - 50s - loss: 0.7421 - accuracy: 0.7413\n",
      "Epoch 4/100\n",
      "1666/1666 - 50s - loss: 0.7240 - accuracy: 0.7413\n",
      "Epoch 5/100\n",
      "1666/1666 - 50s - loss: 0.6932 - accuracy: 0.7413\n",
      "Epoch 6/100\n",
      "1666/1666 - 49s - loss: 0.6662 - accuracy: 0.7412\n",
      "Epoch 7/100\n",
      "1666/1666 - 50s - loss: 0.6493 - accuracy: 0.7417\n",
      "Epoch 8/100\n",
      "1666/1666 - 49s - loss: 0.6403 - accuracy: 0.7419\n",
      "Epoch 9/100\n",
      "1666/1666 - 50s - loss: 0.6335 - accuracy: 0.7430\n",
      "Epoch 10/100\n",
      "1666/1666 - 50s - loss: 0.6284 - accuracy: 0.7438\n",
      "Epoch 11/100\n",
      "1666/1666 - 50s - loss: 0.6236 - accuracy: 0.7444\n",
      "Epoch 12/100\n",
      "1666/1666 - 51s - loss: 0.6191 - accuracy: 0.7454\n",
      "Epoch 13/100\n",
      "1666/1666 - 51s - loss: 0.6147 - accuracy: 0.7449\n",
      "Epoch 14/100\n",
      "1666/1666 - 50s - loss: 0.6084 - accuracy: 0.7456\n",
      "Epoch 15/100\n",
      "1666/1666 - 50s - loss: 0.6010 - accuracy: 0.7461\n",
      "Epoch 16/100\n",
      "1666/1666 - 51s - loss: 0.5961 - accuracy: 0.7460\n",
      "Epoch 17/100\n",
      "1666/1666 - 51s - loss: 0.5914 - accuracy: 0.7481\n",
      "Epoch 18/100\n",
      "1666/1666 - 51s - loss: 0.5879 - accuracy: 0.7473\n",
      "Epoch 19/100\n",
      "1666/1666 - 51s - loss: 0.5833 - accuracy: 0.7492\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1666/1666 - 51s - loss: 0.5795 - accuracy: 0.7498\n",
      "Epoch 21/100\n",
      "1666/1666 - 51s - loss: 0.5753 - accuracy: 0.7508\n",
      "Epoch 22/100\n",
      "1666/1666 - 51s - loss: 0.5716 - accuracy: 0.7506\n",
      "Epoch 23/100\n",
      "1666/1666 - 51s - loss: 0.5685 - accuracy: 0.7520\n",
      "Epoch 24/100\n",
      "1666/1666 - 51s - loss: 0.5667 - accuracy: 0.7522\n",
      "Epoch 25/100\n",
      "1666/1666 - 51s - loss: 0.5630 - accuracy: 0.7536\n",
      "Epoch 26/100\n",
      "1666/1666 - 51s - loss: 0.5607 - accuracy: 0.7535\n",
      "Epoch 27/100\n",
      "1666/1666 - 51s - loss: 0.5585 - accuracy: 0.7557\n",
      "Epoch 28/100\n",
      "1666/1666 - 51s - loss: 0.5566 - accuracy: 0.7564\n",
      "Epoch 29/100\n",
      "1666/1666 - 51s - loss: 0.5556 - accuracy: 0.7553\n",
      "Epoch 30/100\n",
      "1666/1666 - 51s - loss: 0.5531 - accuracy: 0.7554\n",
      "Epoch 31/100\n",
      "1666/1666 - 51s - loss: 0.5510 - accuracy: 0.7580\n",
      "Epoch 32/100\n",
      "1666/1666 - 51s - loss: 0.5495 - accuracy: 0.7584\n",
      "Epoch 33/100\n",
      "1666/1666 - 51s - loss: 0.5490 - accuracy: 0.7578\n",
      "Epoch 34/100\n",
      "1666/1666 - 51s - loss: 0.5476 - accuracy: 0.7577\n",
      "Epoch 35/100\n",
      "1666/1666 - 51s - loss: 0.5466 - accuracy: 0.7588\n",
      "Epoch 36/100\n",
      "1666/1666 - 51s - loss: 0.5455 - accuracy: 0.7583\n",
      "Epoch 37/100\n",
      "1666/1666 - 51s - loss: 0.5430 - accuracy: 0.7609\n",
      "Epoch 38/100\n",
      "1666/1666 - 51s - loss: 0.5424 - accuracy: 0.7601\n",
      "Epoch 39/100\n",
      "1666/1666 - 51s - loss: 0.5422 - accuracy: 0.7609\n",
      "Epoch 40/100\n",
      "1666/1666 - 51s - loss: 0.5408 - accuracy: 0.7614\n",
      "Epoch 41/100\n",
      "1666/1666 - 51s - loss: 0.5399 - accuracy: 0.7615\n",
      "Epoch 42/100\n",
      "1666/1666 - 51s - loss: 0.5392 - accuracy: 0.7607\n",
      "Epoch 43/100\n",
      "1666/1666 - 51s - loss: 0.5382 - accuracy: 0.7622\n",
      "Epoch 44/100\n",
      "1666/1666 - 51s - loss: 0.5377 - accuracy: 0.7618\n",
      "Epoch 45/100\n",
      "1666/1666 - 51s - loss: 0.5364 - accuracy: 0.7620\n",
      "Epoch 46/100\n",
      "1666/1666 - 51s - loss: 0.5355 - accuracy: 0.7635\n",
      "Epoch 47/100\n",
      "1666/1666 - 51s - loss: 0.5352 - accuracy: 0.7637\n",
      "Epoch 48/100\n",
      "1666/1666 - 51s - loss: 0.5334 - accuracy: 0.7643\n",
      "Epoch 49/100\n",
      "1666/1666 - 51s - loss: 0.5334 - accuracy: 0.7643\n",
      "Epoch 50/100\n",
      "1666/1666 - 51s - loss: 0.5319 - accuracy: 0.7628\n",
      "Epoch 51/100\n",
      "1666/1666 - 51s - loss: 0.5325 - accuracy: 0.7628\n",
      "Epoch 52/100\n",
      "1666/1666 - 51s - loss: 0.5316 - accuracy: 0.7633\n",
      "Epoch 53/100\n",
      "1666/1666 - 51s - loss: 0.5307 - accuracy: 0.7654\n",
      "Epoch 54/100\n",
      "1666/1666 - 51s - loss: 0.5304 - accuracy: 0.7646\n",
      "Epoch 55/100\n",
      "1666/1666 - 50s - loss: 0.5297 - accuracy: 0.7663\n",
      "Epoch 56/100\n",
      "1666/1666 - 50s - loss: 0.5293 - accuracy: 0.7648\n",
      "Epoch 57/100\n",
      "1666/1666 - 50s - loss: 0.5291 - accuracy: 0.7657\n",
      "Epoch 58/100\n",
      "1666/1666 - 51s - loss: 0.5284 - accuracy: 0.7651\n",
      "Epoch 59/100\n",
      "1666/1666 - 50s - loss: 0.5279 - accuracy: 0.7654\n",
      "Epoch 60/100\n",
      "1666/1666 - 50s - loss: 0.5269 - accuracy: 0.7671\n",
      "Epoch 61/100\n",
      "1666/1666 - 50s - loss: 0.5271 - accuracy: 0.7672\n",
      "Epoch 62/100\n",
      "1666/1666 - 50s - loss: 0.5253 - accuracy: 0.7662\n",
      "Epoch 63/100\n",
      "1666/1666 - 50s - loss: 0.5260 - accuracy: 0.7661\n",
      "Epoch 64/100\n",
      "1666/1666 - 50s - loss: 0.5265 - accuracy: 0.7663\n",
      "Epoch 65/100\n",
      "1666/1666 - 50s - loss: 0.5256 - accuracy: 0.7663\n",
      "Epoch 66/100\n",
      "1666/1666 - 50s - loss: 0.5245 - accuracy: 0.7675\n",
      "Epoch 67/100\n",
      "1666/1666 - 50s - loss: 0.5244 - accuracy: 0.7670\n",
      "Epoch 68/100\n",
      "1666/1666 - 50s - loss: 0.5242 - accuracy: 0.7678\n",
      "Epoch 69/100\n",
      "1666/1666 - 50s - loss: 0.5236 - accuracy: 0.7678\n",
      "Epoch 70/100\n",
      "1666/1666 - 50s - loss: 0.5239 - accuracy: 0.7667\n",
      "Epoch 71/100\n",
      "1666/1666 - 50s - loss: 0.5234 - accuracy: 0.7674\n",
      "Epoch 72/100\n",
      "1666/1666 - 50s - loss: 0.5227 - accuracy: 0.7656\n",
      "Epoch 73/100\n",
      "1666/1666 - 50s - loss: 0.5216 - accuracy: 0.7688\n",
      "Epoch 74/100\n",
      "1666/1666 - 50s - loss: 0.5229 - accuracy: 0.7673\n",
      "Epoch 75/100\n",
      "1666/1666 - 50s - loss: 0.5214 - accuracy: 0.7681\n",
      "Epoch 76/100\n",
      "1666/1666 - 50s - loss: 0.5212 - accuracy: 0.7681\n",
      "Epoch 77/100\n",
      "1666/1666 - 50s - loss: 0.5208 - accuracy: 0.7694\n",
      "Epoch 78/100\n",
      "1666/1666 - 50s - loss: 0.5204 - accuracy: 0.7704\n",
      "Epoch 79/100\n",
      "1666/1666 - 50s - loss: 0.5192 - accuracy: 0.7689\n",
      "Epoch 80/100\n",
      "1666/1666 - 50s - loss: 0.5193 - accuracy: 0.7701\n",
      "Epoch 81/100\n",
      "1666/1666 - 50s - loss: 0.5195 - accuracy: 0.7684\n",
      "Epoch 82/100\n",
      "1666/1666 - 50s - loss: 0.5195 - accuracy: 0.7685\n",
      "Epoch 83/100\n",
      "1666/1666 - 51s - loss: 0.5187 - accuracy: 0.7690\n",
      "Epoch 84/100\n",
      "1666/1666 - 51s - loss: 0.5175 - accuracy: 0.7706\n",
      "Epoch 85/100\n",
      "1666/1666 - 51s - loss: 0.5180 - accuracy: 0.7709\n",
      "Epoch 86/100\n",
      "1666/1666 - 51s - loss: 0.5179 - accuracy: 0.7701\n",
      "Epoch 87/100\n",
      "1666/1666 - 51s - loss: 0.5177 - accuracy: 0.7696\n",
      "Epoch 88/100\n",
      "1666/1666 - 51s - loss: 0.5172 - accuracy: 0.7711\n",
      "Epoch 89/100\n",
      "1666/1666 - 51s - loss: 0.5166 - accuracy: 0.7691\n",
      "Epoch 90/100\n",
      "1666/1666 - 51s - loss: 0.5159 - accuracy: 0.7701\n",
      "Epoch 91/100\n",
      "1666/1666 - 51s - loss: 0.5159 - accuracy: 0.7703\n",
      "Epoch 92/100\n",
      "1666/1666 - 51s - loss: 0.5161 - accuracy: 0.7712\n",
      "Epoch 93/100\n",
      "1666/1666 - 51s - loss: 0.5156 - accuracy: 0.7695\n",
      "Epoch 94/100\n",
      "1666/1666 - 51s - loss: 0.5158 - accuracy: 0.7697\n",
      "Epoch 95/100\n",
      "1666/1666 - 51s - loss: 0.5147 - accuracy: 0.7714\n",
      "Epoch 96/100\n",
      "1666/1666 - 51s - loss: 0.5146 - accuracy: 0.7711\n",
      "Epoch 97/100\n",
      "1666/1666 - 51s - loss: 0.5149 - accuracy: 0.7705\n",
      "Epoch 98/100\n",
      "1666/1666 - 51s - loss: 0.5142 - accuracy: 0.7713\n",
      "Epoch 99/100\n",
      "1666/1666 - 51s - loss: 0.5137 - accuracy: 0.7725\n",
      "Epoch 100/100\n",
      "1666/1666 - 51s - loss: 0.5143 - accuracy: 0.7718\n",
      "417/417 - 3s - loss: 0.5266 - accuracy: 0.7477\n",
      "Epoch 1/100\n",
      "2082/2082 - 63s - loss: 0.7729 - accuracy: 0.7331\n",
      "Epoch 2/100\n",
      "2082/2082 - 63s - loss: 0.7419 - accuracy: 0.7406\n",
      "Epoch 3/100\n",
      "2082/2082 - 63s - loss: 0.7078 - accuracy: 0.7406\n",
      "Epoch 4/100\n",
      "2082/2082 - 63s - loss: 0.6578 - accuracy: 0.7403\n",
      "Epoch 5/100\n",
      "2082/2082 - 63s - loss: 0.6380 - accuracy: 0.7396\n",
      "Epoch 6/100\n",
      "2082/2082 - 63s - loss: 0.6278 - accuracy: 0.7419\n",
      "Epoch 7/100\n",
      "2082/2082 - 63s - loss: 0.6192 - accuracy: 0.7435\n",
      "Epoch 8/100\n",
      "2082/2082 - 64s - loss: 0.6103 - accuracy: 0.7429\n",
      "Epoch 9/100\n",
      "2082/2082 - 63s - loss: 0.6016 - accuracy: 0.7441\n",
      "Epoch 10/100\n",
      "2082/2082 - 63s - loss: 0.5959 - accuracy: 0.7436\n",
      "Epoch 11/100\n",
      "2082/2082 - 64s - loss: 0.5912 - accuracy: 0.7440\n",
      "Epoch 12/100\n",
      "2082/2082 - 63s - loss: 0.5868 - accuracy: 0.7452\n",
      "Epoch 13/100\n",
      "2082/2082 - 63s - loss: 0.5829 - accuracy: 0.7455\n",
      "Epoch 14/100\n",
      "2082/2082 - 63s - loss: 0.5786 - accuracy: 0.7467\n",
      "Epoch 15/100\n",
      "2082/2082 - 63s - loss: 0.5756 - accuracy: 0.7473\n",
      "Epoch 16/100\n",
      "2082/2082 - 63s - loss: 0.5707 - accuracy: 0.7470\n",
      "Epoch 17/100\n",
      "2082/2082 - 63s - loss: 0.5668 - accuracy: 0.7482\n",
      "Epoch 18/100\n",
      "2082/2082 - 63s - loss: 0.5640 - accuracy: 0.7482\n",
      "Epoch 19/100\n",
      "2082/2082 - 62s - loss: 0.5603 - accuracy: 0.7494\n",
      "Epoch 20/100\n",
      "2082/2082 - 63s - loss: 0.5565 - accuracy: 0.7520\n",
      "Epoch 21/100\n",
      "2082/2082 - 62s - loss: 0.5529 - accuracy: 0.7528\n",
      "Epoch 22/100\n",
      "2082/2082 - 62s - loss: 0.5512 - accuracy: 0.7526\n",
      "Epoch 23/100\n",
      "2082/2082 - 63s - loss: 0.5483 - accuracy: 0.7541\n",
      "Epoch 24/100\n",
      "2082/2082 - 63s - loss: 0.5468 - accuracy: 0.7536\n",
      "Epoch 25/100\n",
      "2082/2082 - 63s - loss: 0.5454 - accuracy: 0.7542\n",
      "Epoch 26/100\n",
      "2082/2082 - 63s - loss: 0.5422 - accuracy: 0.7556\n",
      "Epoch 27/100\n",
      "2082/2082 - 63s - loss: 0.5416 - accuracy: 0.7570\n",
      "Epoch 28/100\n",
      "2082/2082 - 63s - loss: 0.5407 - accuracy: 0.7562\n",
      "Epoch 29/100\n",
      "2082/2082 - 63s - loss: 0.5397 - accuracy: 0.7564\n",
      "Epoch 30/100\n",
      "2082/2082 - 63s - loss: 0.5374 - accuracy: 0.7589\n",
      "Epoch 31/100\n",
      "2082/2082 - 63s - loss: 0.5360 - accuracy: 0.7586\n",
      "Epoch 32/100\n",
      "2082/2082 - 63s - loss: 0.5354 - accuracy: 0.7572\n",
      "Epoch 33/100\n",
      "2082/2082 - 63s - loss: 0.5352 - accuracy: 0.7577\n",
      "Epoch 34/100\n",
      "2082/2082 - 63s - loss: 0.5338 - accuracy: 0.7584\n",
      "Epoch 35/100\n",
      "2082/2082 - 63s - loss: 0.5335 - accuracy: 0.7568\n",
      "Epoch 36/100\n",
      "2082/2082 - 63s - loss: 0.5322 - accuracy: 0.7601\n",
      "Epoch 37/100\n",
      "2082/2082 - 63s - loss: 0.5316 - accuracy: 0.7587\n",
      "Epoch 38/100\n",
      "2082/2082 - 63s - loss: 0.5303 - accuracy: 0.7604\n",
      "Epoch 39/100\n",
      "2082/2082 - 63s - loss: 0.5299 - accuracy: 0.7603\n",
      "Epoch 40/100\n",
      "2082/2082 - 63s - loss: 0.5293 - accuracy: 0.7614\n",
      "Epoch 41/100\n",
      "2082/2082 - 63s - loss: 0.5288 - accuracy: 0.7614\n",
      "Epoch 42/100\n",
      "2082/2082 - 63s - loss: 0.5277 - accuracy: 0.7597\n",
      "Epoch 43/100\n",
      "2082/2082 - 63s - loss: 0.5276 - accuracy: 0.7602\n",
      "Epoch 44/100\n",
      "2082/2082 - 64s - loss: 0.5267 - accuracy: 0.7614\n",
      "Epoch 45/100\n",
      "2082/2082 - 64s - loss: 0.5259 - accuracy: 0.7625\n",
      "Epoch 46/100\n",
      "2082/2082 - 64s - loss: 0.5269 - accuracy: 0.7606\n",
      "Epoch 47/100\n",
      "2082/2082 - 64s - loss: 0.5253 - accuracy: 0.7613\n",
      "Epoch 48/100\n",
      "2082/2082 - 64s - loss: 0.5251 - accuracy: 0.7624\n",
      "Epoch 49/100\n",
      "2082/2082 - 64s - loss: 0.5247 - accuracy: 0.7618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100\n",
      "2082/2082 - 64s - loss: 0.5239 - accuracy: 0.7634\n",
      "Epoch 51/100\n",
      "2082/2082 - 64s - loss: 0.5238 - accuracy: 0.7617\n",
      "Epoch 52/100\n",
      "2082/2082 - 64s - loss: 0.5234 - accuracy: 0.7623\n",
      "Epoch 53/100\n",
      "2082/2082 - 64s - loss: 0.5227 - accuracy: 0.7625\n",
      "Epoch 54/100\n",
      "2082/2082 - 64s - loss: 0.5219 - accuracy: 0.7644\n",
      "Epoch 55/100\n",
      "2082/2082 - 64s - loss: 0.5213 - accuracy: 0.7630\n",
      "Epoch 56/100\n",
      "2082/2082 - 64s - loss: 0.5212 - accuracy: 0.7634\n",
      "Epoch 57/100\n",
      "2082/2082 - 64s - loss: 0.5216 - accuracy: 0.7631\n",
      "Epoch 58/100\n",
      "2082/2082 - 64s - loss: 0.5215 - accuracy: 0.7635\n",
      "Epoch 59/100\n",
      "2082/2082 - 64s - loss: 0.5201 - accuracy: 0.7640\n",
      "Epoch 60/100\n",
      "2082/2082 - 64s - loss: 0.5197 - accuracy: 0.7651\n",
      "Epoch 61/100\n",
      "2082/2082 - 64s - loss: 0.5191 - accuracy: 0.7639\n",
      "Epoch 62/100\n",
      "2082/2082 - 64s - loss: 0.5186 - accuracy: 0.7643\n",
      "Epoch 63/100\n",
      "2082/2082 - 64s - loss: 0.5184 - accuracy: 0.7639\n",
      "Epoch 64/100\n",
      "2082/2082 - 64s - loss: 0.5180 - accuracy: 0.7643\n",
      "Epoch 65/100\n",
      "2082/2082 - 64s - loss: 0.5172 - accuracy: 0.7660\n",
      "Epoch 66/100\n",
      "2082/2082 - 64s - loss: 0.5172 - accuracy: 0.7655\n",
      "Epoch 67/100\n",
      "2082/2082 - 64s - loss: 0.5162 - accuracy: 0.7654\n",
      "Epoch 68/100\n",
      "2082/2082 - 64s - loss: 0.5161 - accuracy: 0.7663\n",
      "Epoch 69/100\n",
      "2082/2082 - 64s - loss: 0.5155 - accuracy: 0.7664\n",
      "Epoch 70/100\n",
      "2082/2082 - 64s - loss: 0.5149 - accuracy: 0.7664\n",
      "Epoch 71/100\n",
      "2082/2082 - 64s - loss: 0.5149 - accuracy: 0.7670\n",
      "Epoch 72/100\n",
      "2082/2082 - 63s - loss: 0.5141 - accuracy: 0.7671\n",
      "Epoch 73/100\n",
      "2082/2082 - 62s - loss: 0.5139 - accuracy: 0.7670\n",
      "Epoch 74/100\n",
      "2082/2082 - 63s - loss: 0.5140 - accuracy: 0.7656\n",
      "Epoch 75/100\n",
      "2082/2082 - 63s - loss: 0.5138 - accuracy: 0.7668\n",
      "Epoch 76/100\n",
      "2082/2082 - 63s - loss: 0.5131 - accuracy: 0.7667\n",
      "Epoch 77/100\n",
      "2082/2082 - 62s - loss: 0.5130 - accuracy: 0.7673\n",
      "Epoch 78/100\n",
      "2082/2082 - 62s - loss: 0.5124 - accuracy: 0.7679\n",
      "Epoch 79/100\n",
      "2082/2082 - 63s - loss: 0.5121 - accuracy: 0.7665\n",
      "Epoch 80/100\n",
      "2082/2082 - 62s - loss: 0.5116 - accuracy: 0.7669\n",
      "Epoch 81/100\n",
      "2082/2082 - 61s - loss: 0.5109 - accuracy: 0.7676\n",
      "Epoch 82/100\n",
      "2082/2082 - 61s - loss: 0.5104 - accuracy: 0.7691\n",
      "Epoch 83/100\n",
      "2082/2082 - 61s - loss: 0.5112 - accuracy: 0.7675\n",
      "Epoch 84/100\n",
      "2082/2082 - 62s - loss: 0.5110 - accuracy: 0.7678\n",
      "Epoch 85/100\n",
      "2082/2082 - 61s - loss: 0.5099 - accuracy: 0.7683\n",
      "Epoch 86/100\n",
      "2082/2082 - 61s - loss: 0.5096 - accuracy: 0.7682\n",
      "Epoch 87/100\n",
      "2082/2082 - 61s - loss: 0.5094 - accuracy: 0.7696\n",
      "Epoch 88/100\n",
      "2082/2082 - 61s - loss: 0.5087 - accuracy: 0.7677\n",
      "Epoch 89/100\n",
      "2082/2082 - 61s - loss: 0.5089 - accuracy: 0.7704\n",
      "Epoch 90/100\n",
      "2082/2082 - 61s - loss: 0.5077 - accuracy: 0.7701\n",
      "Epoch 91/100\n",
      "2082/2082 - 61s - loss: 0.5083 - accuracy: 0.7686\n",
      "Epoch 92/100\n",
      "2082/2082 - 61s - loss: 0.5072 - accuracy: 0.7704\n",
      "Epoch 93/100\n",
      "2082/2082 - 61s - loss: 0.5077 - accuracy: 0.7698\n",
      "Epoch 94/100\n",
      "2082/2082 - 61s - loss: 0.5068 - accuracy: 0.7704\n",
      "Epoch 95/100\n",
      "2082/2082 - 61s - loss: 0.5068 - accuracy: 0.7698\n",
      "Epoch 96/100\n",
      "2082/2082 - 61s - loss: 0.5058 - accuracy: 0.7710\n",
      "Epoch 97/100\n",
      "2082/2082 - 61s - loss: 0.5058 - accuracy: 0.7711\n",
      "Epoch 98/100\n",
      "2082/2082 - 61s - loss: 0.5054 - accuracy: 0.7712\n",
      "Epoch 99/100\n",
      "2082/2082 - 62s - loss: 0.5051 - accuracy: 0.7705\n",
      "Epoch 100/100\n",
      "2082/2082 - 62s - loss: 0.5055 - accuracy: 0.7700\n",
      "417/417 - 3s - loss: 0.6205 - accuracy: 0.7332\n",
      "Epoch 1/100\n",
      "417/417 - 13s - loss: 0.8566 - accuracy: 0.7113\n",
      "Epoch 2/100\n",
      "417/417 - 12s - loss: 0.7536 - accuracy: 0.7389\n",
      "Epoch 3/100\n",
      "417/417 - 12s - loss: 0.7527 - accuracy: 0.7389\n",
      "Epoch 4/100\n",
      "417/417 - 12s - loss: 0.7517 - accuracy: 0.7389\n",
      "Epoch 5/100\n",
      "417/417 - 13s - loss: 0.7500 - accuracy: 0.7389\n",
      "Epoch 6/100\n",
      "417/417 - 13s - loss: 0.7481 - accuracy: 0.7389\n",
      "Epoch 7/100\n",
      "417/417 - 12s - loss: 0.7455 - accuracy: 0.7389\n",
      "Epoch 8/100\n",
      "417/417 - 12s - loss: 0.7416 - accuracy: 0.7389\n",
      "Epoch 9/100\n",
      "417/417 - 12s - loss: 0.7362 - accuracy: 0.7389\n",
      "Epoch 10/100\n",
      "417/417 - 12s - loss: 0.7295 - accuracy: 0.7389\n",
      "Epoch 11/100\n",
      "417/417 - 12s - loss: 0.7210 - accuracy: 0.7389\n",
      "Epoch 12/100\n",
      "417/417 - 12s - loss: 0.7098 - accuracy: 0.7389\n",
      "Epoch 13/100\n",
      "417/417 - 12s - loss: 0.6985 - accuracy: 0.7389\n",
      "Epoch 14/100\n",
      "417/417 - 12s - loss: 0.6873 - accuracy: 0.7389\n",
      "Epoch 15/100\n",
      "417/417 - 12s - loss: 0.6772 - accuracy: 0.7390\n",
      "Epoch 16/100\n",
      "417/417 - 12s - loss: 0.6690 - accuracy: 0.7386\n",
      "Epoch 17/100\n",
      "417/417 - 12s - loss: 0.6616 - accuracy: 0.7382\n",
      "Epoch 18/100\n",
      "417/417 - 13s - loss: 0.6574 - accuracy: 0.7384\n",
      "Epoch 19/100\n",
      "417/417 - 12s - loss: 0.6523 - accuracy: 0.7387\n",
      "Epoch 20/100\n",
      "417/417 - 12s - loss: 0.6479 - accuracy: 0.7381\n",
      "Epoch 21/100\n",
      "417/417 - 12s - loss: 0.6472 - accuracy: 0.7389\n",
      "Epoch 22/100\n",
      "417/417 - 12s - loss: 0.6433 - accuracy: 0.7385\n",
      "Epoch 23/100\n",
      "417/417 - 12s - loss: 0.6406 - accuracy: 0.7406\n",
      "Epoch 24/100\n",
      "417/417 - 13s - loss: 0.6390 - accuracy: 0.7396\n",
      "Epoch 25/100\n",
      "417/417 - 12s - loss: 0.6388 - accuracy: 0.7412\n",
      "Epoch 26/100\n",
      "417/417 - 12s - loss: 0.6354 - accuracy: 0.7395\n",
      "Epoch 27/100\n",
      "417/417 - 12s - loss: 0.6332 - accuracy: 0.7406\n",
      "Epoch 28/100\n",
      "417/417 - 12s - loss: 0.6325 - accuracy: 0.7392\n",
      "Epoch 29/100\n",
      "417/417 - 12s - loss: 0.6299 - accuracy: 0.7402\n",
      "Epoch 30/100\n",
      "417/417 - 13s - loss: 0.6259 - accuracy: 0.7418\n",
      "Epoch 31/100\n",
      "417/417 - 12s - loss: 0.6256 - accuracy: 0.7414\n",
      "Epoch 32/100\n",
      "417/417 - 12s - loss: 0.6243 - accuracy: 0.7410\n",
      "Epoch 33/100\n",
      "417/417 - 12s - loss: 0.6202 - accuracy: 0.7403\n",
      "Epoch 34/100\n",
      "417/417 - 12s - loss: 0.6211 - accuracy: 0.7399\n",
      "Epoch 35/100\n",
      "417/417 - 12s - loss: 0.6182 - accuracy: 0.7432\n",
      "Epoch 36/100\n",
      "417/417 - 12s - loss: 0.6167 - accuracy: 0.7409\n",
      "Epoch 37/100\n",
      "417/417 - 12s - loss: 0.6158 - accuracy: 0.7422\n",
      "Epoch 38/100\n",
      "417/417 - 13s - loss: 0.6142 - accuracy: 0.7447\n",
      "Epoch 39/100\n",
      "417/417 - 12s - loss: 0.6133 - accuracy: 0.7455\n",
      "Epoch 40/100\n",
      "417/417 - 13s - loss: 0.6115 - accuracy: 0.7441\n",
      "Epoch 41/100\n",
      "417/417 - 12s - loss: 0.6116 - accuracy: 0.7446\n",
      "Epoch 42/100\n",
      "417/417 - 12s - loss: 0.6124 - accuracy: 0.7437\n",
      "Epoch 43/100\n",
      "417/417 - 12s - loss: 0.6108 - accuracy: 0.7426\n",
      "Epoch 44/100\n",
      "417/417 - 12s - loss: 0.6109 - accuracy: 0.7448\n",
      "Epoch 45/100\n",
      "417/417 - 12s - loss: 0.6098 - accuracy: 0.7452\n",
      "Epoch 46/100\n",
      "417/417 - 12s - loss: 0.6063 - accuracy: 0.7440\n",
      "Epoch 47/100\n",
      "417/417 - 12s - loss: 0.6046 - accuracy: 0.7471\n",
      "Epoch 48/100\n",
      "417/417 - 12s - loss: 0.6050 - accuracy: 0.7468\n",
      "Epoch 49/100\n",
      "417/417 - 12s - loss: 0.6030 - accuracy: 0.7477\n",
      "Epoch 50/100\n",
      "417/417 - 13s - loss: 0.6026 - accuracy: 0.7443\n",
      "Epoch 51/100\n",
      "417/417 - 12s - loss: 0.6036 - accuracy: 0.7476\n",
      "Epoch 52/100\n",
      "417/417 - 12s - loss: 0.6016 - accuracy: 0.7475\n",
      "Epoch 53/100\n",
      "417/417 - 13s - loss: 0.5994 - accuracy: 0.7471\n",
      "Epoch 54/100\n",
      "417/417 - 12s - loss: 0.5989 - accuracy: 0.7492\n",
      "Epoch 55/100\n",
      "417/417 - 12s - loss: 0.6002 - accuracy: 0.7457\n",
      "Epoch 56/100\n",
      "417/417 - 12s - loss: 0.5995 - accuracy: 0.7479\n",
      "Epoch 57/100\n",
      "417/417 - 13s - loss: 0.5955 - accuracy: 0.7503\n",
      "Epoch 58/100\n",
      "417/417 - 12s - loss: 0.5954 - accuracy: 0.7479\n",
      "Epoch 59/100\n",
      "417/417 - 13s - loss: 0.5940 - accuracy: 0.7498\n",
      "Epoch 60/100\n",
      "417/417 - 12s - loss: 0.5948 - accuracy: 0.7473\n",
      "Epoch 61/100\n",
      "417/417 - 13s - loss: 0.5919 - accuracy: 0.7514\n",
      "Epoch 62/100\n",
      "417/417 - 13s - loss: 0.5915 - accuracy: 0.7508\n",
      "Epoch 63/100\n",
      "417/417 - 13s - loss: 0.5917 - accuracy: 0.7507\n",
      "Epoch 64/100\n",
      "417/417 - 13s - loss: 0.5891 - accuracy: 0.7497\n",
      "Epoch 65/100\n",
      "417/417 - 13s - loss: 0.5903 - accuracy: 0.7503\n",
      "Epoch 66/100\n",
      "417/417 - 13s - loss: 0.5883 - accuracy: 0.7517\n",
      "Epoch 67/100\n",
      "417/417 - 13s - loss: 0.5888 - accuracy: 0.7495\n",
      "Epoch 68/100\n",
      "417/417 - 13s - loss: 0.5877 - accuracy: 0.7502\n",
      "Epoch 69/100\n",
      "417/417 - 13s - loss: 0.5867 - accuracy: 0.7526\n",
      "Epoch 70/100\n",
      "417/417 - 13s - loss: 0.5862 - accuracy: 0.7518\n",
      "Epoch 71/100\n",
      "417/417 - 12s - loss: 0.5863 - accuracy: 0.7519\n",
      "Epoch 72/100\n",
      "417/417 - 13s - loss: 0.5837 - accuracy: 0.7541\n",
      "Epoch 73/100\n",
      "417/417 - 13s - loss: 0.5830 - accuracy: 0.7546\n",
      "Epoch 74/100\n",
      "417/417 - 13s - loss: 0.5799 - accuracy: 0.7558\n",
      "Epoch 75/100\n",
      "417/417 - 13s - loss: 0.5837 - accuracy: 0.7506\n",
      "Epoch 76/100\n",
      "417/417 - 13s - loss: 0.5790 - accuracy: 0.7556\n",
      "Epoch 77/100\n",
      "417/417 - 13s - loss: 0.5824 - accuracy: 0.7513\n",
      "Epoch 78/100\n",
      "417/417 - 13s - loss: 0.5785 - accuracy: 0.7535\n",
      "Epoch 79/100\n",
      "417/417 - 13s - loss: 0.5770 - accuracy: 0.7559\n",
      "Epoch 80/100\n",
      "417/417 - 13s - loss: 0.5773 - accuracy: 0.7565\n",
      "Epoch 81/100\n",
      "417/417 - 13s - loss: 0.5756 - accuracy: 0.7565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100\n",
      "417/417 - 13s - loss: 0.5756 - accuracy: 0.7571\n",
      "Epoch 83/100\n",
      "417/417 - 13s - loss: 0.5734 - accuracy: 0.7567\n",
      "Epoch 84/100\n",
      "417/417 - 13s - loss: 0.5723 - accuracy: 0.7559\n",
      "Epoch 85/100\n",
      "417/417 - 13s - loss: 0.5715 - accuracy: 0.7572\n",
      "Epoch 86/100\n",
      "417/417 - 13s - loss: 0.5726 - accuracy: 0.7582\n",
      "Epoch 87/100\n",
      "417/417 - 13s - loss: 0.5675 - accuracy: 0.7599\n",
      "Epoch 88/100\n",
      "417/417 - 13s - loss: 0.5692 - accuracy: 0.7562\n",
      "Epoch 89/100\n",
      "417/417 - 13s - loss: 0.5692 - accuracy: 0.7589\n",
      "Epoch 90/100\n",
      "417/417 - 13s - loss: 0.5665 - accuracy: 0.7591\n",
      "Epoch 91/100\n",
      "417/417 - 13s - loss: 0.5671 - accuracy: 0.7587\n",
      "Epoch 92/100\n",
      "417/417 - 13s - loss: 0.5661 - accuracy: 0.7590\n",
      "Epoch 93/100\n",
      "417/417 - 13s - loss: 0.5641 - accuracy: 0.7601\n",
      "Epoch 94/100\n",
      "417/417 - 13s - loss: 0.5624 - accuracy: 0.7616\n",
      "Epoch 95/100\n",
      "417/417 - 13s - loss: 0.5649 - accuracy: 0.7592\n",
      "Epoch 96/100\n",
      "417/417 - 13s - loss: 0.5630 - accuracy: 0.7604\n",
      "Epoch 97/100\n",
      "417/417 - 13s - loss: 0.5621 - accuracy: 0.7590\n",
      "Epoch 98/100\n",
      "417/417 - 13s - loss: 0.5629 - accuracy: 0.7604\n",
      "Epoch 99/100\n",
      "417/417 - 13s - loss: 0.5600 - accuracy: 0.7613\n",
      "Epoch 100/100\n",
      "417/417 - 13s - loss: 0.5588 - accuracy: 0.7599\n",
      "417/417 - 3s - loss: 0.6085 - accuracy: 0.7431\n",
      "Epoch 1/100\n",
      "833/833 - 26s - loss: 0.8001 - accuracy: 0.7215\n",
      "Epoch 2/100\n",
      "833/833 - 25s - loss: 0.7492 - accuracy: 0.7396\n",
      "Epoch 3/100\n",
      "833/833 - 25s - loss: 0.7450 - accuracy: 0.7396\n",
      "Epoch 4/100\n",
      "833/833 - 25s - loss: 0.7386 - accuracy: 0.7396\n",
      "Epoch 5/100\n",
      "833/833 - 25s - loss: 0.7277 - accuracy: 0.7396\n",
      "Epoch 6/100\n",
      "833/833 - 25s - loss: 0.7108 - accuracy: 0.7396\n",
      "Epoch 7/100\n",
      "833/833 - 25s - loss: 0.6874 - accuracy: 0.7396\n",
      "Epoch 8/100\n",
      "833/833 - 25s - loss: 0.6626 - accuracy: 0.7396\n",
      "Epoch 9/100\n",
      "833/833 - 25s - loss: 0.6490 - accuracy: 0.7407\n",
      "Epoch 10/100\n",
      "833/833 - 25s - loss: 0.6414 - accuracy: 0.7405\n",
      "Epoch 11/100\n",
      "833/833 - 25s - loss: 0.6373 - accuracy: 0.7411\n",
      "Epoch 12/100\n",
      "833/833 - 25s - loss: 0.6338 - accuracy: 0.7415\n",
      "Epoch 13/100\n",
      "833/833 - 25s - loss: 0.6301 - accuracy: 0.7416\n",
      "Epoch 14/100\n",
      "833/833 - 25s - loss: 0.6280 - accuracy: 0.7425\n",
      "Epoch 15/100\n",
      "833/833 - 25s - loss: 0.6267 - accuracy: 0.7413\n",
      "Epoch 16/100\n",
      "833/833 - 25s - loss: 0.6251 - accuracy: 0.7420\n",
      "Epoch 17/100\n",
      "833/833 - 25s - loss: 0.6238 - accuracy: 0.7435\n",
      "Epoch 18/100\n",
      "833/833 - 25s - loss: 0.6218 - accuracy: 0.7431\n",
      "Epoch 19/100\n",
      "833/833 - 25s - loss: 0.6205 - accuracy: 0.7426\n",
      "Epoch 20/100\n",
      "833/833 - 25s - loss: 0.6190 - accuracy: 0.7435\n",
      "Epoch 21/100\n",
      "833/833 - 25s - loss: 0.6185 - accuracy: 0.7432\n",
      "Epoch 22/100\n",
      "833/833 - 25s - loss: 0.6168 - accuracy: 0.7440\n",
      "Epoch 23/100\n",
      "833/833 - 25s - loss: 0.6146 - accuracy: 0.7440\n",
      "Epoch 24/100\n",
      "833/833 - 25s - loss: 0.6147 - accuracy: 0.7438\n",
      "Epoch 25/100\n",
      "833/833 - 25s - loss: 0.6124 - accuracy: 0.7435\n",
      "Epoch 26/100\n",
      "833/833 - 25s - loss: 0.6117 - accuracy: 0.7443\n",
      "Epoch 27/100\n",
      "833/833 - 25s - loss: 0.6095 - accuracy: 0.7452\n",
      "Epoch 28/100\n",
      "833/833 - 25s - loss: 0.6078 - accuracy: 0.7455\n",
      "Epoch 29/100\n",
      "833/833 - 25s - loss: 0.6071 - accuracy: 0.7448\n",
      "Epoch 30/100\n",
      "833/833 - 25s - loss: 0.6058 - accuracy: 0.7453\n",
      "Epoch 31/100\n",
      "833/833 - 25s - loss: 0.6050 - accuracy: 0.7476\n",
      "Epoch 32/100\n",
      "833/833 - 25s - loss: 0.6019 - accuracy: 0.7485\n",
      "Epoch 33/100\n",
      "833/833 - 25s - loss: 0.6003 - accuracy: 0.7477\n",
      "Epoch 34/100\n",
      "833/833 - 25s - loss: 0.5991 - accuracy: 0.7489\n",
      "Epoch 35/100\n",
      "833/833 - 25s - loss: 0.5967 - accuracy: 0.7477\n",
      "Epoch 36/100\n",
      "833/833 - 25s - loss: 0.5956 - accuracy: 0.7517\n",
      "Epoch 37/100\n",
      "833/833 - 25s - loss: 0.5935 - accuracy: 0.7511\n",
      "Epoch 38/100\n",
      "833/833 - 25s - loss: 0.5923 - accuracy: 0.7504\n",
      "Epoch 39/100\n",
      "833/833 - 25s - loss: 0.5896 - accuracy: 0.7488\n",
      "Epoch 40/100\n",
      "833/833 - 25s - loss: 0.5867 - accuracy: 0.7508\n",
      "Epoch 41/100\n",
      "833/833 - 25s - loss: 0.5849 - accuracy: 0.7538\n",
      "Epoch 42/100\n",
      "833/833 - 25s - loss: 0.5829 - accuracy: 0.7530\n",
      "Epoch 43/100\n",
      "833/833 - 25s - loss: 0.5822 - accuracy: 0.7524\n",
      "Epoch 44/100\n",
      "833/833 - 25s - loss: 0.5794 - accuracy: 0.7516\n",
      "Epoch 45/100\n",
      "833/833 - 25s - loss: 0.5758 - accuracy: 0.7540\n",
      "Epoch 46/100\n",
      "833/833 - 25s - loss: 0.5740 - accuracy: 0.7552\n",
      "Epoch 47/100\n",
      "833/833 - 25s - loss: 0.5743 - accuracy: 0.7530\n",
      "Epoch 48/100\n",
      "833/833 - 25s - loss: 0.5729 - accuracy: 0.7538\n",
      "Epoch 49/100\n",
      "833/833 - 25s - loss: 0.5691 - accuracy: 0.7550\n",
      "Epoch 50/100\n",
      "833/833 - 25s - loss: 0.5681 - accuracy: 0.7565\n",
      "Epoch 51/100\n",
      "833/833 - 26s - loss: 0.5672 - accuracy: 0.7556\n",
      "Epoch 52/100\n",
      "833/833 - 25s - loss: 0.5663 - accuracy: 0.7584\n",
      "Epoch 53/100\n",
      "833/833 - 25s - loss: 0.5647 - accuracy: 0.7567\n",
      "Epoch 54/100\n",
      "833/833 - 25s - loss: 0.5625 - accuracy: 0.7581\n",
      "Epoch 55/100\n",
      "833/833 - 25s - loss: 0.5640 - accuracy: 0.7568\n",
      "Epoch 56/100\n",
      "833/833 - 25s - loss: 0.5615 - accuracy: 0.7607\n",
      "Epoch 57/100\n",
      "833/833 - 25s - loss: 0.5586 - accuracy: 0.7611\n",
      "Epoch 58/100\n",
      "833/833 - 25s - loss: 0.5575 - accuracy: 0.7615\n",
      "Epoch 59/100\n",
      "833/833 - 25s - loss: 0.5570 - accuracy: 0.7611\n",
      "Epoch 60/100\n",
      "833/833 - 25s - loss: 0.5562 - accuracy: 0.7622\n",
      "Epoch 61/100\n",
      "833/833 - 25s - loss: 0.5543 - accuracy: 0.7644\n",
      "Epoch 62/100\n",
      "833/833 - 25s - loss: 0.5529 - accuracy: 0.7610\n",
      "Epoch 63/100\n",
      "833/833 - 25s - loss: 0.5521 - accuracy: 0.7620\n",
      "Epoch 64/100\n",
      "833/833 - 25s - loss: 0.5521 - accuracy: 0.7638\n",
      "Epoch 65/100\n",
      "833/833 - 25s - loss: 0.5497 - accuracy: 0.7648\n",
      "Epoch 66/100\n",
      "833/833 - 25s - loss: 0.5492 - accuracy: 0.7653\n",
      "Epoch 67/100\n",
      "833/833 - 25s - loss: 0.5479 - accuracy: 0.7655\n",
      "Epoch 68/100\n",
      "833/833 - 25s - loss: 0.5485 - accuracy: 0.7625\n",
      "Epoch 69/100\n",
      "833/833 - 25s - loss: 0.5478 - accuracy: 0.7645\n",
      "Epoch 70/100\n",
      "833/833 - 25s - loss: 0.5472 - accuracy: 0.7661\n",
      "Epoch 71/100\n",
      "833/833 - 25s - loss: 0.5453 - accuracy: 0.7661\n",
      "Epoch 72/100\n",
      "833/833 - 25s - loss: 0.5444 - accuracy: 0.7650\n",
      "Epoch 73/100\n",
      "833/833 - 25s - loss: 0.5442 - accuracy: 0.7666\n",
      "Epoch 74/100\n",
      "833/833 - 25s - loss: 0.5441 - accuracy: 0.7650\n",
      "Epoch 75/100\n",
      "833/833 - 25s - loss: 0.5429 - accuracy: 0.7675\n",
      "Epoch 76/100\n",
      "833/833 - 25s - loss: 0.5411 - accuracy: 0.7676\n",
      "Epoch 77/100\n",
      "833/833 - 25s - loss: 0.5404 - accuracy: 0.7647\n",
      "Epoch 78/100\n",
      "833/833 - 25s - loss: 0.5397 - accuracy: 0.7691\n",
      "Epoch 79/100\n",
      "833/833 - 25s - loss: 0.5402 - accuracy: 0.7674\n",
      "Epoch 80/100\n",
      "833/833 - 25s - loss: 0.5390 - accuracy: 0.7657\n",
      "Epoch 81/100\n",
      "833/833 - 25s - loss: 0.5384 - accuracy: 0.7678\n",
      "Epoch 82/100\n",
      "833/833 - 25s - loss: 0.5387 - accuracy: 0.7660\n",
      "Epoch 83/100\n",
      "833/833 - 25s - loss: 0.5354 - accuracy: 0.7689\n",
      "Epoch 84/100\n",
      "833/833 - 25s - loss: 0.5361 - accuracy: 0.7670\n",
      "Epoch 85/100\n",
      "833/833 - 25s - loss: 0.5353 - accuracy: 0.7692\n",
      "Epoch 86/100\n",
      "833/833 - 25s - loss: 0.5338 - accuracy: 0.7690\n",
      "Epoch 87/100\n",
      "833/833 - 25s - loss: 0.5361 - accuracy: 0.7685\n",
      "Epoch 88/100\n",
      "833/833 - 25s - loss: 0.5345 - accuracy: 0.7689\n",
      "Epoch 89/100\n",
      "833/833 - 25s - loss: 0.5334 - accuracy: 0.7685\n",
      "Epoch 90/100\n",
      "833/833 - 25s - loss: 0.5335 - accuracy: 0.7693\n",
      "Epoch 91/100\n",
      "833/833 - 25s - loss: 0.5319 - accuracy: 0.7693\n",
      "Epoch 92/100\n",
      "833/833 - 25s - loss: 0.5326 - accuracy: 0.7687\n",
      "Epoch 93/100\n",
      "833/833 - 25s - loss: 0.5318 - accuracy: 0.7700\n",
      "Epoch 94/100\n",
      "833/833 - 25s - loss: 0.5317 - accuracy: 0.7702\n",
      "Epoch 95/100\n",
      "833/833 - 25s - loss: 0.5313 - accuracy: 0.7698\n",
      "Epoch 96/100\n",
      "833/833 - 25s - loss: 0.5307 - accuracy: 0.7708\n",
      "Epoch 97/100\n",
      "833/833 - 25s - loss: 0.5310 - accuracy: 0.7697\n",
      "Epoch 98/100\n",
      "833/833 - 25s - loss: 0.5307 - accuracy: 0.7695\n",
      "Epoch 99/100\n",
      "833/833 - 25s - loss: 0.5311 - accuracy: 0.7697\n",
      "Epoch 100/100\n",
      "833/833 - 25s - loss: 0.5291 - accuracy: 0.7712\n",
      "417/417 - 3s - loss: 0.5361 - accuracy: 0.7624\n",
      "Epoch 1/100\n",
      "1249/1249 - 38s - loss: 0.7677 - accuracy: 0.7439\n",
      "Epoch 2/100\n",
      "1249/1249 - 37s - loss: 0.7371 - accuracy: 0.7441\n",
      "Epoch 3/100\n",
      "1249/1249 - 37s - loss: 0.7244 - accuracy: 0.7441\n",
      "Epoch 4/100\n",
      "1249/1249 - 37s - loss: 0.7008 - accuracy: 0.7441\n",
      "Epoch 5/100\n",
      "1249/1249 - 37s - loss: 0.6683 - accuracy: 0.7440\n",
      "Epoch 6/100\n",
      "1249/1249 - 37s - loss: 0.6408 - accuracy: 0.7435\n",
      "Epoch 7/100\n",
      "1249/1249 - 37s - loss: 0.6287 - accuracy: 0.7441\n",
      "Epoch 8/100\n",
      "1249/1249 - 37s - loss: 0.6241 - accuracy: 0.7447\n",
      "Epoch 9/100\n",
      "1249/1249 - 37s - loss: 0.6207 - accuracy: 0.7443\n",
      "Epoch 10/100\n",
      "1249/1249 - 37s - loss: 0.6168 - accuracy: 0.7444\n",
      "Epoch 11/100\n",
      "1249/1249 - 37s - loss: 0.6140 - accuracy: 0.7448\n",
      "Epoch 12/100\n",
      "1249/1249 - 37s - loss: 0.6118 - accuracy: 0.7460\n",
      "Epoch 13/100\n",
      "1249/1249 - 37s - loss: 0.6090 - accuracy: 0.7467\n",
      "Epoch 14/100\n",
      "1249/1249 - 37s - loss: 0.6074 - accuracy: 0.7467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n",
      "1249/1249 - 37s - loss: 0.6054 - accuracy: 0.7469\n",
      "Epoch 16/100\n",
      "1249/1249 - 37s - loss: 0.6027 - accuracy: 0.7483\n",
      "Epoch 17/100\n",
      "1249/1249 - 37s - loss: 0.6005 - accuracy: 0.7493\n",
      "Epoch 18/100\n",
      "1249/1249 - 37s - loss: 0.5978 - accuracy: 0.7491\n",
      "Epoch 19/100\n",
      "1249/1249 - 37s - loss: 0.5945 - accuracy: 0.7496\n",
      "Epoch 20/100\n",
      "1249/1249 - 37s - loss: 0.5925 - accuracy: 0.7498\n",
      "Epoch 21/100\n",
      "1249/1249 - 37s - loss: 0.5911 - accuracy: 0.7506\n",
      "Epoch 22/100\n",
      "1249/1249 - 37s - loss: 0.5888 - accuracy: 0.7518\n",
      "Epoch 23/100\n",
      "1249/1249 - 37s - loss: 0.5859 - accuracy: 0.7514\n",
      "Epoch 24/100\n",
      "1249/1249 - 37s - loss: 0.5829 - accuracy: 0.7528\n",
      "Epoch 25/100\n",
      "1249/1249 - 37s - loss: 0.5794 - accuracy: 0.7525\n",
      "Epoch 26/100\n",
      "1249/1249 - 37s - loss: 0.5770 - accuracy: 0.7527\n",
      "Epoch 27/100\n",
      "1249/1249 - 37s - loss: 0.5746 - accuracy: 0.7560\n",
      "Epoch 28/100\n",
      "1249/1249 - 37s - loss: 0.5717 - accuracy: 0.7555\n",
      "Epoch 29/100\n",
      "1249/1249 - 37s - loss: 0.5742 - accuracy: 0.7559\n",
      "Epoch 30/100\n",
      "1249/1249 - 37s - loss: 0.5674 - accuracy: 0.7564\n",
      "Epoch 31/100\n",
      "1249/1249 - 37s - loss: 0.5650 - accuracy: 0.7581\n",
      "Epoch 32/100\n",
      "1249/1249 - 37s - loss: 0.5631 - accuracy: 0.7567\n",
      "Epoch 33/100\n",
      "1249/1249 - 37s - loss: 0.5616 - accuracy: 0.7588\n",
      "Epoch 34/100\n",
      "1249/1249 - 37s - loss: 0.5591 - accuracy: 0.7584\n",
      "Epoch 35/100\n",
      "1249/1249 - 37s - loss: 0.5572 - accuracy: 0.7582\n",
      "Epoch 36/100\n",
      "1249/1249 - 37s - loss: 0.5559 - accuracy: 0.7596\n",
      "Epoch 37/100\n",
      "1249/1249 - 37s - loss: 0.5541 - accuracy: 0.7614\n",
      "Epoch 38/100\n",
      "1249/1249 - 37s - loss: 0.5534 - accuracy: 0.7605\n",
      "Epoch 39/100\n",
      "1249/1249 - 37s - loss: 0.5520 - accuracy: 0.7640\n",
      "Epoch 40/100\n",
      "1249/1249 - 37s - loss: 0.5496 - accuracy: 0.7642\n",
      "Epoch 41/100\n",
      "1249/1249 - 37s - loss: 0.5487 - accuracy: 0.7638\n",
      "Epoch 42/100\n",
      "1249/1249 - 37s - loss: 0.5484 - accuracy: 0.7635\n",
      "Epoch 43/100\n",
      "1249/1249 - 37s - loss: 0.5456 - accuracy: 0.7639\n",
      "Epoch 44/100\n",
      "1249/1249 - 37s - loss: 0.5436 - accuracy: 0.7639\n",
      "Epoch 45/100\n",
      "1249/1249 - 37s - loss: 0.5436 - accuracy: 0.7650\n",
      "Epoch 46/100\n",
      "1249/1249 - 37s - loss: 0.5416 - accuracy: 0.7637\n",
      "Epoch 47/100\n",
      "1249/1249 - 37s - loss: 0.5386 - accuracy: 0.7675\n",
      "Epoch 48/100\n",
      "1249/1249 - 37s - loss: 0.5395 - accuracy: 0.7664\n",
      "Epoch 49/100\n",
      "1249/1249 - 37s - loss: 0.5377 - accuracy: 0.7658\n",
      "Epoch 50/100\n",
      "1249/1249 - 37s - loss: 0.5360 - accuracy: 0.7674\n",
      "Epoch 51/100\n",
      "1249/1249 - 37s - loss: 0.5356 - accuracy: 0.7673\n",
      "Epoch 52/100\n",
      "1249/1249 - 37s - loss: 0.5348 - accuracy: 0.7690\n",
      "Epoch 53/100\n",
      "1249/1249 - 37s - loss: 0.5349 - accuracy: 0.7665\n",
      "Epoch 54/100\n",
      "1249/1249 - 37s - loss: 0.5332 - accuracy: 0.7682\n",
      "Epoch 55/100\n",
      "1249/1249 - 37s - loss: 0.5321 - accuracy: 0.7674\n",
      "Epoch 56/100\n",
      "1249/1249 - 37s - loss: 0.5315 - accuracy: 0.7692\n",
      "Epoch 57/100\n",
      "1249/1249 - 37s - loss: 0.5316 - accuracy: 0.7697\n",
      "Epoch 58/100\n",
      "1249/1249 - 37s - loss: 0.5305 - accuracy: 0.7688\n",
      "Epoch 59/100\n",
      "1249/1249 - 37s - loss: 0.5287 - accuracy: 0.7685\n",
      "Epoch 60/100\n",
      "1249/1249 - 37s - loss: 0.5300 - accuracy: 0.7691\n",
      "Epoch 61/100\n",
      "1249/1249 - 37s - loss: 0.5282 - accuracy: 0.7688\n",
      "Epoch 62/100\n",
      "1249/1249 - 37s - loss: 0.5287 - accuracy: 0.7692\n",
      "Epoch 63/100\n",
      "1249/1249 - 37s - loss: 0.5275 - accuracy: 0.7694\n",
      "Epoch 64/100\n",
      "1249/1249 - 37s - loss: 0.5277 - accuracy: 0.7696\n",
      "Epoch 65/100\n",
      "1249/1249 - 37s - loss: 0.5263 - accuracy: 0.7703\n",
      "Epoch 66/100\n",
      "1249/1249 - 37s - loss: 0.5259 - accuracy: 0.7698\n",
      "Epoch 67/100\n",
      "1249/1249 - 37s - loss: 0.5261 - accuracy: 0.7704\n",
      "Epoch 68/100\n",
      "1249/1249 - 37s - loss: 0.5247 - accuracy: 0.7705\n",
      "Epoch 69/100\n",
      "1249/1249 - 37s - loss: 0.5259 - accuracy: 0.7705\n",
      "Epoch 70/100\n",
      "1249/1249 - 37s - loss: 0.5242 - accuracy: 0.7713\n",
      "Epoch 71/100\n",
      "1249/1249 - 37s - loss: 0.5241 - accuracy: 0.7702\n",
      "Epoch 72/100\n",
      "1249/1249 - 37s - loss: 0.5237 - accuracy: 0.7702\n",
      "Epoch 73/100\n",
      "1249/1249 - 37s - loss: 0.5223 - accuracy: 0.7718\n",
      "Epoch 74/100\n",
      "1249/1249 - 37s - loss: 0.5223 - accuracy: 0.7709\n",
      "Epoch 75/100\n",
      "1249/1249 - 37s - loss: 0.5227 - accuracy: 0.7722\n",
      "Epoch 76/100\n",
      "1249/1249 - 37s - loss: 0.5225 - accuracy: 0.7716\n",
      "Epoch 77/100\n",
      "1249/1249 - 37s - loss: 0.5211 - accuracy: 0.7727\n",
      "Epoch 78/100\n",
      "1249/1249 - 37s - loss: 0.5220 - accuracy: 0.7714\n",
      "Epoch 79/100\n",
      "1249/1249 - 37s - loss: 0.5195 - accuracy: 0.7725\n",
      "Epoch 80/100\n",
      "1249/1249 - 37s - loss: 0.5189 - accuracy: 0.7725\n",
      "Epoch 81/100\n",
      "1249/1249 - 37s - loss: 0.5185 - accuracy: 0.7729\n",
      "Epoch 82/100\n",
      "1249/1249 - 37s - loss: 0.5192 - accuracy: 0.7742\n",
      "Epoch 83/100\n",
      "1249/1249 - 37s - loss: 0.5180 - accuracy: 0.7724\n",
      "Epoch 84/100\n",
      "1249/1249 - 37s - loss: 0.5185 - accuracy: 0.7736\n",
      "Epoch 85/100\n",
      "1249/1249 - 37s - loss: 0.5173 - accuracy: 0.7746\n",
      "Epoch 86/100\n",
      "1249/1249 - 37s - loss: 0.5174 - accuracy: 0.7749\n",
      "Epoch 87/100\n",
      "1249/1249 - 37s - loss: 0.5167 - accuracy: 0.7741\n",
      "Epoch 88/100\n",
      "1249/1249 - 37s - loss: 0.5171 - accuracy: 0.7729\n",
      "Epoch 89/100\n",
      "1249/1249 - 37s - loss: 0.5174 - accuracy: 0.7742\n",
      "Epoch 90/100\n",
      "1249/1249 - 37s - loss: 0.5158 - accuracy: 0.7739\n",
      "Epoch 91/100\n",
      "1249/1249 - 37s - loss: 0.5153 - accuracy: 0.7744\n",
      "Epoch 92/100\n",
      "1249/1249 - 37s - loss: 0.5160 - accuracy: 0.7731\n",
      "Epoch 93/100\n",
      "1249/1249 - 38s - loss: 0.5148 - accuracy: 0.7747\n",
      "Epoch 94/100\n",
      "1249/1249 - 38s - loss: 0.5149 - accuracy: 0.7752\n",
      "Epoch 95/100\n",
      "1249/1249 - 38s - loss: 0.5144 - accuracy: 0.7736\n",
      "Epoch 96/100\n",
      "1249/1249 - 38s - loss: 0.5144 - accuracy: 0.7762\n",
      "Epoch 97/100\n",
      "1249/1249 - 38s - loss: 0.5138 - accuracy: 0.7756\n",
      "Epoch 98/100\n",
      "1249/1249 - 38s - loss: 0.5144 - accuracy: 0.7738\n",
      "Epoch 99/100\n",
      "1249/1249 - 38s - loss: 0.5137 - accuracy: 0.7754\n",
      "Epoch 100/100\n",
      "1249/1249 - 38s - loss: 0.5133 - accuracy: 0.7751\n",
      "417/417 - 3s - loss: 0.5638 - accuracy: 0.7408\n",
      "Epoch 1/100\n",
      "1666/1666 - 52s - loss: 0.7666 - accuracy: 0.7413\n",
      "Epoch 2/100\n",
      "1666/1666 - 51s - loss: 0.7385 - accuracy: 0.7413\n",
      "Epoch 3/100\n",
      "1666/1666 - 51s - loss: 0.7103 - accuracy: 0.7413\n",
      "Epoch 4/100\n",
      "1666/1666 - 51s - loss: 0.6668 - accuracy: 0.7410\n",
      "Epoch 5/100\n",
      "1666/1666 - 50s - loss: 0.6497 - accuracy: 0.7409\n",
      "Epoch 6/100\n",
      "1666/1666 - 50s - loss: 0.6432 - accuracy: 0.7417\n",
      "Epoch 7/100\n",
      "1666/1666 - 50s - loss: 0.6382 - accuracy: 0.7412\n",
      "Epoch 8/100\n",
      "1666/1666 - 51s - loss: 0.6344 - accuracy: 0.7427\n",
      "Epoch 9/100\n",
      "1666/1666 - 51s - loss: 0.6297 - accuracy: 0.7430\n",
      "Epoch 10/100\n",
      "1666/1666 - 51s - loss: 0.6265 - accuracy: 0.7440\n",
      "Epoch 11/100\n",
      "1666/1666 - 51s - loss: 0.6231 - accuracy: 0.7430\n",
      "Epoch 12/100\n",
      "1666/1666 - 51s - loss: 0.6199 - accuracy: 0.7445\n",
      "Epoch 13/100\n",
      "1666/1666 - 51s - loss: 0.6176 - accuracy: 0.7436\n",
      "Epoch 14/100\n",
      "1666/1666 - 51s - loss: 0.6137 - accuracy: 0.7448\n",
      "Epoch 15/100\n",
      "1666/1666 - 51s - loss: 0.6107 - accuracy: 0.7449\n",
      "Epoch 16/100\n",
      "1666/1666 - 51s - loss: 0.6081 - accuracy: 0.7467\n",
      "Epoch 17/100\n",
      "1666/1666 - 51s - loss: 0.6040 - accuracy: 0.7466\n",
      "Epoch 18/100\n",
      "1666/1666 - 51s - loss: 0.6007 - accuracy: 0.7471\n",
      "Epoch 19/100\n",
      "1666/1666 - 51s - loss: 0.5955 - accuracy: 0.7470\n",
      "Epoch 20/100\n",
      "1666/1666 - 51s - loss: 0.5918 - accuracy: 0.7475\n",
      "Epoch 21/100\n",
      "1666/1666 - 51s - loss: 0.5887 - accuracy: 0.7493\n",
      "Epoch 22/100\n",
      "1666/1666 - 51s - loss: 0.5859 - accuracy: 0.7480\n",
      "Epoch 23/100\n",
      "1666/1666 - 51s - loss: 0.5823 - accuracy: 0.7501\n",
      "Epoch 24/100\n",
      "1666/1666 - 51s - loss: 0.5801 - accuracy: 0.7507\n",
      "Epoch 25/100\n",
      "1666/1666 - 51s - loss: 0.5762 - accuracy: 0.7517\n",
      "Epoch 26/100\n",
      "1666/1666 - 51s - loss: 0.5737 - accuracy: 0.7542\n",
      "Epoch 27/100\n",
      "1666/1666 - 51s - loss: 0.5702 - accuracy: 0.7534\n",
      "Epoch 28/100\n",
      "1666/1666 - 51s - loss: 0.5678 - accuracy: 0.7555\n",
      "Epoch 29/100\n",
      "1666/1666 - 51s - loss: 0.5655 - accuracy: 0.7559\n",
      "Epoch 30/100\n",
      "1666/1666 - 51s - loss: 0.5630 - accuracy: 0.7585\n",
      "Epoch 31/100\n",
      "1666/1666 - 51s - loss: 0.5609 - accuracy: 0.7565\n",
      "Epoch 32/100\n",
      "1666/1666 - 50s - loss: 0.5595 - accuracy: 0.7570\n",
      "Epoch 33/100\n",
      "1666/1666 - 50s - loss: 0.5578 - accuracy: 0.7568\n",
      "Epoch 34/100\n",
      "1666/1666 - 50s - loss: 0.5548 - accuracy: 0.7581\n",
      "Epoch 35/100\n",
      "1666/1666 - 50s - loss: 0.5533 - accuracy: 0.7587\n",
      "Epoch 36/100\n",
      "1666/1666 - 50s - loss: 0.5525 - accuracy: 0.7598\n",
      "Epoch 37/100\n",
      "1666/1666 - 50s - loss: 0.5500 - accuracy: 0.7590\n",
      "Epoch 38/100\n",
      "1666/1666 - 50s - loss: 0.5489 - accuracy: 0.7591\n",
      "Epoch 39/100\n",
      "1666/1666 - 50s - loss: 0.5481 - accuracy: 0.7576\n",
      "Epoch 40/100\n",
      "1666/1666 - 50s - loss: 0.5478 - accuracy: 0.7584\n",
      "Epoch 41/100\n",
      "1666/1666 - 50s - loss: 0.5463 - accuracy: 0.7602\n",
      "Epoch 42/100\n",
      "1666/1666 - 50s - loss: 0.5452 - accuracy: 0.7592\n",
      "Epoch 43/100\n",
      "1666/1666 - 50s - loss: 0.5444 - accuracy: 0.7597\n",
      "Epoch 44/100\n",
      "1666/1666 - 50s - loss: 0.5435 - accuracy: 0.7601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100\n",
      "1666/1666 - 50s - loss: 0.5428 - accuracy: 0.7601\n",
      "Epoch 46/100\n",
      "1666/1666 - 50s - loss: 0.5418 - accuracy: 0.7601\n",
      "Epoch 47/100\n",
      "1666/1666 - 50s - loss: 0.5409 - accuracy: 0.7610\n",
      "Epoch 48/100\n",
      "1666/1666 - 50s - loss: 0.5405 - accuracy: 0.7603\n",
      "Epoch 49/100\n",
      "1666/1666 - 50s - loss: 0.5408 - accuracy: 0.7598\n",
      "Epoch 50/100\n",
      "1666/1666 - 50s - loss: 0.5386 - accuracy: 0.7617\n",
      "Epoch 51/100\n",
      "1666/1666 - 50s - loss: 0.5395 - accuracy: 0.7612\n",
      "Epoch 52/100\n",
      "1666/1666 - 50s - loss: 0.5385 - accuracy: 0.7612\n",
      "Epoch 53/100\n",
      "1666/1666 - 50s - loss: 0.5377 - accuracy: 0.7625\n",
      "Epoch 54/100\n",
      "1666/1666 - 50s - loss: 0.5377 - accuracy: 0.7616\n",
      "Epoch 55/100\n",
      "1666/1666 - 50s - loss: 0.5367 - accuracy: 0.7625\n",
      "Epoch 56/100\n",
      "1666/1666 - 50s - loss: 0.5361 - accuracy: 0.7617\n",
      "Epoch 57/100\n",
      "1666/1666 - 50s - loss: 0.5356 - accuracy: 0.7618\n",
      "Epoch 58/100\n",
      "1666/1666 - 50s - loss: 0.5356 - accuracy: 0.7626\n",
      "Epoch 59/100\n",
      "1666/1666 - 50s - loss: 0.5355 - accuracy: 0.7617\n",
      "Epoch 60/100\n",
      "1666/1666 - 50s - loss: 0.5351 - accuracy: 0.7643\n",
      "Epoch 61/100\n",
      "1666/1666 - 50s - loss: 0.5341 - accuracy: 0.7620\n",
      "Epoch 62/100\n",
      "1666/1666 - 50s - loss: 0.5327 - accuracy: 0.7623\n",
      "Epoch 63/100\n",
      "1666/1666 - 50s - loss: 0.5335 - accuracy: 0.7617\n",
      "Epoch 64/100\n",
      "1666/1666 - 50s - loss: 0.5324 - accuracy: 0.7640\n",
      "Epoch 65/100\n",
      "1666/1666 - 50s - loss: 0.5329 - accuracy: 0.7630\n",
      "Epoch 66/100\n",
      "1666/1666 - 50s - loss: 0.5314 - accuracy: 0.7635\n",
      "Epoch 67/100\n",
      "1666/1666 - 50s - loss: 0.5318 - accuracy: 0.7647\n",
      "Epoch 68/100\n",
      "1666/1666 - 50s - loss: 0.5306 - accuracy: 0.7649\n",
      "Epoch 69/100\n",
      "1666/1666 - 50s - loss: 0.5307 - accuracy: 0.7643\n",
      "Epoch 70/100\n",
      "1666/1666 - 50s - loss: 0.5295 - accuracy: 0.7648\n",
      "Epoch 71/100\n",
      "1666/1666 - 50s - loss: 0.5293 - accuracy: 0.7640\n",
      "Epoch 72/100\n",
      "1666/1666 - 50s - loss: 0.5298 - accuracy: 0.7643\n",
      "Epoch 73/100\n",
      "1666/1666 - 50s - loss: 0.5292 - accuracy: 0.7655\n",
      "Epoch 74/100\n",
      "1666/1666 - 51s - loss: 0.5283 - accuracy: 0.7653\n",
      "Epoch 75/100\n",
      "1666/1666 - 50s - loss: 0.5280 - accuracy: 0.7645\n",
      "Epoch 76/100\n",
      "1666/1666 - 51s - loss: 0.5285 - accuracy: 0.7652\n",
      "Epoch 77/100\n",
      "1666/1666 - 50s - loss: 0.5287 - accuracy: 0.7647\n",
      "Epoch 78/100\n",
      "1666/1666 - 50s - loss: 0.5280 - accuracy: 0.7655\n",
      "Epoch 79/100\n",
      "1666/1666 - 50s - loss: 0.5274 - accuracy: 0.7663\n",
      "Epoch 80/100\n",
      "1666/1666 - 51s - loss: 0.5270 - accuracy: 0.7653\n",
      "Epoch 81/100\n",
      "1666/1666 - 51s - loss: 0.5278 - accuracy: 0.7645\n",
      "Epoch 82/100\n",
      "1666/1666 - 51s - loss: 0.5264 - accuracy: 0.7645\n",
      "Epoch 83/100\n",
      "1666/1666 - 51s - loss: 0.5263 - accuracy: 0.7644\n",
      "Epoch 84/100\n",
      "1666/1666 - 51s - loss: 0.5259 - accuracy: 0.7663\n",
      "Epoch 85/100\n",
      "1666/1666 - 51s - loss: 0.5261 - accuracy: 0.7652\n",
      "Epoch 86/100\n",
      "1666/1666 - 51s - loss: 0.5254 - accuracy: 0.7656\n",
      "Epoch 87/100\n",
      "1666/1666 - 50s - loss: 0.5251 - accuracy: 0.7658\n",
      "Epoch 88/100\n",
      "1666/1666 - 50s - loss: 0.5248 - accuracy: 0.7660\n",
      "Epoch 89/100\n",
      "1666/1666 - 51s - loss: 0.5256 - accuracy: 0.7651\n",
      "Epoch 90/100\n",
      "1666/1666 - 51s - loss: 0.5246 - accuracy: 0.7659\n",
      "Epoch 91/100\n",
      "1666/1666 - 51s - loss: 0.5237 - accuracy: 0.7674\n",
      "Epoch 92/100\n",
      "1666/1666 - 50s - loss: 0.5231 - accuracy: 0.7676\n",
      "Epoch 93/100\n",
      "1666/1666 - 51s - loss: 0.5235 - accuracy: 0.7657\n",
      "Epoch 94/100\n",
      "1666/1666 - 51s - loss: 0.5234 - accuracy: 0.7676\n",
      "Epoch 95/100\n",
      "1666/1666 - 51s - loss: 0.5230 - accuracy: 0.7669\n",
      "Epoch 96/100\n",
      "1666/1666 - 51s - loss: 0.5240 - accuracy: 0.7647\n",
      "Epoch 97/100\n",
      "1666/1666 - 51s - loss: 0.5225 - accuracy: 0.7655\n",
      "Epoch 98/100\n",
      "1666/1666 - 51s - loss: 0.5230 - accuracy: 0.7657\n",
      "Epoch 99/100\n",
      "1666/1666 - 51s - loss: 0.5226 - accuracy: 0.7664\n",
      "Epoch 100/100\n",
      "1666/1666 - 51s - loss: 0.5208 - accuracy: 0.7682\n",
      "417/417 - 3s - loss: 0.5250 - accuracy: 0.7514\n",
      "Epoch 1/100\n",
      "2082/2082 - 63s - loss: 0.7638 - accuracy: 0.7403\n",
      "Epoch 2/100\n",
      "2082/2082 - 62s - loss: 0.7407 - accuracy: 0.7406\n",
      "Epoch 3/100\n",
      "2082/2082 - 62s - loss: 0.7084 - accuracy: 0.7406\n",
      "Epoch 4/100\n",
      "2082/2082 - 62s - loss: 0.6639 - accuracy: 0.7405\n",
      "Epoch 5/100\n",
      "2082/2082 - 62s - loss: 0.6441 - accuracy: 0.7397\n",
      "Epoch 6/100\n",
      "2082/2082 - 62s - loss: 0.6372 - accuracy: 0.7400\n",
      "Epoch 7/100\n",
      "2082/2082 - 62s - loss: 0.6316 - accuracy: 0.7401\n",
      "Epoch 8/100\n",
      "2082/2082 - 62s - loss: 0.6282 - accuracy: 0.7412\n",
      "Epoch 9/100\n",
      "2082/2082 - 62s - loss: 0.6239 - accuracy: 0.7425\n",
      "Epoch 10/100\n",
      "2082/2082 - 62s - loss: 0.6193 - accuracy: 0.7420\n",
      "Epoch 11/100\n",
      "2082/2082 - 62s - loss: 0.6159 - accuracy: 0.7438\n",
      "Epoch 12/100\n",
      "2082/2082 - 62s - loss: 0.6111 - accuracy: 0.7439\n",
      "Epoch 13/100\n",
      "2082/2082 - 62s - loss: 0.6071 - accuracy: 0.7446\n",
      "Epoch 14/100\n",
      "2082/2082 - 62s - loss: 0.6041 - accuracy: 0.7455\n",
      "Epoch 15/100\n",
      "2082/2082 - 62s - loss: 0.6013 - accuracy: 0.7452\n",
      "Epoch 16/100\n",
      "2082/2082 - 62s - loss: 0.5971 - accuracy: 0.7464\n",
      "Epoch 17/100\n",
      "2082/2082 - 62s - loss: 0.5945 - accuracy: 0.7460\n",
      "Epoch 18/100\n",
      "2082/2082 - 62s - loss: 0.5895 - accuracy: 0.7484\n",
      "Epoch 19/100\n",
      "2082/2082 - 62s - loss: 0.5856 - accuracy: 0.7481\n",
      "Epoch 20/100\n",
      "2082/2082 - 62s - loss: 0.5797 - accuracy: 0.7479\n",
      "Epoch 21/100\n",
      "2082/2082 - 62s - loss: 0.5756 - accuracy: 0.7480\n",
      "Epoch 22/100\n",
      "2082/2082 - 62s - loss: 0.5729 - accuracy: 0.7489\n",
      "Epoch 23/100\n",
      "2082/2082 - 62s - loss: 0.5703 - accuracy: 0.7485\n",
      "Epoch 24/100\n",
      "2082/2082 - 62s - loss: 0.5668 - accuracy: 0.7506\n",
      "Epoch 25/100\n",
      "2082/2082 - 62s - loss: 0.5640 - accuracy: 0.7513\n",
      "Epoch 26/100\n",
      "2082/2082 - 62s - loss: 0.5623 - accuracy: 0.7524\n",
      "Epoch 27/100\n",
      "2082/2082 - 62s - loss: 0.5601 - accuracy: 0.7521\n",
      "Epoch 28/100\n",
      "2082/2082 - 62s - loss: 0.5584 - accuracy: 0.7519\n",
      "Epoch 29/100\n",
      "2082/2082 - 62s - loss: 0.5560 - accuracy: 0.7548\n",
      "Epoch 30/100\n",
      "2082/2082 - 62s - loss: 0.5542 - accuracy: 0.7549\n",
      "Epoch 31/100\n",
      "2082/2082 - 63s - loss: 0.5531 - accuracy: 0.7542\n",
      "Epoch 32/100\n",
      "2082/2082 - 63s - loss: 0.5509 - accuracy: 0.7552\n",
      "Epoch 33/100\n",
      "2082/2082 - 63s - loss: 0.5491 - accuracy: 0.7555\n",
      "Epoch 34/100\n",
      "2082/2082 - 63s - loss: 0.5490 - accuracy: 0.7549\n",
      "Epoch 35/100\n",
      "2082/2082 - 63s - loss: 0.5467 - accuracy: 0.7572\n",
      "Epoch 36/100\n",
      "2082/2082 - 63s - loss: 0.5460 - accuracy: 0.7564\n",
      "Epoch 37/100\n",
      "2082/2082 - 63s - loss: 0.5445 - accuracy: 0.7568\n",
      "Epoch 38/100\n",
      "2082/2082 - 63s - loss: 0.5431 - accuracy: 0.7570\n",
      "Epoch 39/100\n",
      "2082/2082 - 63s - loss: 0.5416 - accuracy: 0.7572\n",
      "Epoch 40/100\n",
      "2082/2082 - 63s - loss: 0.5411 - accuracy: 0.7564\n",
      "Epoch 41/100\n",
      "2082/2082 - 63s - loss: 0.5395 - accuracy: 0.7567\n",
      "Epoch 42/100\n",
      "2082/2082 - 63s - loss: 0.5400 - accuracy: 0.7559\n",
      "Epoch 43/100\n",
      "2082/2082 - 63s - loss: 0.5386 - accuracy: 0.7581\n",
      "Epoch 44/100\n",
      "2082/2082 - 63s - loss: 0.5370 - accuracy: 0.7578\n",
      "Epoch 45/100\n",
      "2082/2082 - 63s - loss: 0.5359 - accuracy: 0.7584\n",
      "Epoch 46/100\n",
      "2082/2082 - 63s - loss: 0.5360 - accuracy: 0.7594\n",
      "Epoch 47/100\n",
      "2082/2082 - 63s - loss: 0.5350 - accuracy: 0.7583\n",
      "Epoch 48/100\n",
      "2082/2082 - 63s - loss: 0.5354 - accuracy: 0.7578\n",
      "Epoch 49/100\n",
      "2082/2082 - 63s - loss: 0.5342 - accuracy: 0.7575\n",
      "Epoch 50/100\n",
      "2082/2082 - 63s - loss: 0.5330 - accuracy: 0.7575\n",
      "Epoch 51/100\n",
      "2082/2082 - 63s - loss: 0.5323 - accuracy: 0.7587\n",
      "Epoch 52/100\n",
      "2082/2082 - 63s - loss: 0.5309 - accuracy: 0.7586\n",
      "Epoch 53/100\n",
      "2082/2082 - 63s - loss: 0.5315 - accuracy: 0.7586\n",
      "Epoch 54/100\n",
      "2082/2082 - 63s - loss: 0.5313 - accuracy: 0.7601\n",
      "Epoch 55/100\n",
      "2082/2082 - 63s - loss: 0.5299 - accuracy: 0.7603\n",
      "Epoch 56/100\n",
      "2082/2082 - 63s - loss: 0.5299 - accuracy: 0.7594\n",
      "Epoch 57/100\n",
      "2082/2082 - 63s - loss: 0.5283 - accuracy: 0.7592\n",
      "Epoch 58/100\n",
      "2082/2082 - 63s - loss: 0.5283 - accuracy: 0.7593\n",
      "Epoch 59/100\n",
      "2082/2082 - 63s - loss: 0.5278 - accuracy: 0.7599\n",
      "Epoch 60/100\n",
      "2082/2082 - 63s - loss: 0.5273 - accuracy: 0.7608\n",
      "Epoch 61/100\n",
      "2082/2082 - 63s - loss: 0.5271 - accuracy: 0.7622\n",
      "Epoch 62/100\n",
      "2082/2082 - 63s - loss: 0.5267 - accuracy: 0.7615\n",
      "Epoch 63/100\n",
      "2082/2082 - 63s - loss: 0.5254 - accuracy: 0.7605\n",
      "Epoch 64/100\n",
      "2082/2082 - 63s - loss: 0.5255 - accuracy: 0.7614\n",
      "Epoch 65/100\n",
      "2082/2082 - 63s - loss: 0.5257 - accuracy: 0.7607\n",
      "Epoch 66/100\n",
      "2082/2082 - 63s - loss: 0.5254 - accuracy: 0.7618\n",
      "Epoch 67/100\n",
      "2082/2082 - 63s - loss: 0.5245 - accuracy: 0.7625\n",
      "Epoch 68/100\n",
      "2082/2082 - 63s - loss: 0.5241 - accuracy: 0.7621\n",
      "Epoch 69/100\n",
      "2082/2082 - 63s - loss: 0.5237 - accuracy: 0.7633\n",
      "Epoch 70/100\n",
      "2082/2082 - 63s - loss: 0.5240 - accuracy: 0.7612\n",
      "Epoch 71/100\n",
      "2082/2082 - 63s - loss: 0.5232 - accuracy: 0.7641\n",
      "Epoch 72/100\n",
      "2082/2082 - 63s - loss: 0.5232 - accuracy: 0.7621\n",
      "Epoch 73/100\n",
      "2082/2082 - 63s - loss: 0.5226 - accuracy: 0.7628\n",
      "Epoch 74/100\n",
      "2082/2082 - 63s - loss: 0.5223 - accuracy: 0.7635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/100\n",
      "2082/2082 - 63s - loss: 0.5225 - accuracy: 0.7627\n",
      "Epoch 76/100\n",
      "2082/2082 - 63s - loss: 0.5213 - accuracy: 0.7625\n",
      "Epoch 77/100\n",
      "2082/2082 - 63s - loss: 0.5216 - accuracy: 0.7622\n",
      "Epoch 78/100\n",
      "2082/2082 - 63s - loss: 0.5209 - accuracy: 0.7626\n",
      "Epoch 79/100\n",
      "2082/2082 - 63s - loss: 0.5211 - accuracy: 0.7626\n",
      "Epoch 80/100\n",
      "2082/2082 - 63s - loss: 0.5203 - accuracy: 0.7629\n",
      "Epoch 81/100\n",
      "2082/2082 - 63s - loss: 0.5207 - accuracy: 0.7633\n",
      "Epoch 82/100\n",
      "2082/2082 - 63s - loss: 0.5196 - accuracy: 0.7653\n",
      "Epoch 83/100\n",
      "2082/2082 - 63s - loss: 0.5191 - accuracy: 0.7650\n",
      "Epoch 84/100\n",
      "2082/2082 - 63s - loss: 0.5196 - accuracy: 0.7641\n",
      "Epoch 85/100\n",
      "2082/2082 - 63s - loss: 0.5189 - accuracy: 0.7637\n",
      "Epoch 86/100\n",
      "2082/2082 - 63s - loss: 0.5192 - accuracy: 0.7648\n",
      "Epoch 87/100\n",
      "2082/2082 - 63s - loss: 0.5185 - accuracy: 0.7647\n",
      "Epoch 88/100\n",
      "2082/2082 - 63s - loss: 0.5183 - accuracy: 0.7654\n",
      "Epoch 89/100\n",
      "2082/2082 - 64s - loss: 0.5183 - accuracy: 0.7650\n",
      "Epoch 90/100\n",
      "2082/2082 - 64s - loss: 0.5174 - accuracy: 0.7651\n",
      "Epoch 91/100\n",
      "2082/2082 - 64s - loss: 0.5175 - accuracy: 0.7656\n",
      "Epoch 92/100\n",
      "2082/2082 - 64s - loss: 0.5173 - accuracy: 0.7633\n",
      "Epoch 93/100\n",
      "2082/2082 - 64s - loss: 0.5170 - accuracy: 0.7654\n",
      "Epoch 94/100\n",
      "2082/2082 - 64s - loss: 0.5170 - accuracy: 0.7661\n",
      "Epoch 95/100\n",
      "2082/2082 - 64s - loss: 0.5166 - accuracy: 0.7663\n",
      "Epoch 96/100\n",
      "2082/2082 - 64s - loss: 0.5163 - accuracy: 0.7657\n",
      "Epoch 97/100\n",
      "2082/2082 - 64s - loss: 0.5161 - accuracy: 0.7659\n",
      "Epoch 98/100\n",
      "2082/2082 - 64s - loss: 0.5154 - accuracy: 0.7661\n",
      "Epoch 99/100\n",
      "2082/2082 - 64s - loss: 0.5155 - accuracy: 0.7657\n",
      "Epoch 100/100\n",
      "2082/2082 - 64s - loss: 0.5161 - accuracy: 0.7652\n",
      "417/417 - 3s - loss: 0.6138 - accuracy: 0.7331\n",
      "Epoch 1/100\n",
      "417/417 - 14s - loss: 0.8596 - accuracy: 0.6921\n",
      "Epoch 2/100\n",
      "417/417 - 13s - loss: 0.7551 - accuracy: 0.7389\n",
      "Epoch 3/100\n",
      "417/417 - 13s - loss: 0.7546 - accuracy: 0.7389\n",
      "Epoch 4/100\n",
      "417/417 - 13s - loss: 0.7538 - accuracy: 0.7389\n",
      "Epoch 5/100\n",
      "417/417 - 13s - loss: 0.7530 - accuracy: 0.7389\n",
      "Epoch 6/100\n",
      "417/417 - 13s - loss: 0.7528 - accuracy: 0.7389\n",
      "Epoch 7/100\n",
      "417/417 - 13s - loss: 0.7511 - accuracy: 0.7389\n",
      "Epoch 8/100\n",
      "417/417 - 13s - loss: 0.7495 - accuracy: 0.7389\n",
      "Epoch 9/100\n",
      "417/417 - 13s - loss: 0.7481 - accuracy: 0.7389\n",
      "Epoch 10/100\n",
      "417/417 - 13s - loss: 0.7454 - accuracy: 0.7389\n",
      "Epoch 11/100\n",
      "417/417 - 13s - loss: 0.7414 - accuracy: 0.7389\n",
      "Epoch 12/100\n",
      "417/417 - 13s - loss: 0.7363 - accuracy: 0.7389\n",
      "Epoch 13/100\n",
      "417/417 - 13s - loss: 0.7295 - accuracy: 0.7389\n",
      "Epoch 14/100\n",
      "417/417 - 13s - loss: 0.7205 - accuracy: 0.7389\n",
      "Epoch 15/100\n",
      "417/417 - 13s - loss: 0.7104 - accuracy: 0.7389\n",
      "Epoch 16/100\n",
      "417/417 - 13s - loss: 0.6986 - accuracy: 0.7389\n",
      "Epoch 17/100\n",
      "417/417 - 13s - loss: 0.6863 - accuracy: 0.7389\n",
      "Epoch 18/100\n",
      "417/417 - 13s - loss: 0.6770 - accuracy: 0.7388\n",
      "Epoch 19/100\n",
      "417/417 - 13s - loss: 0.6685 - accuracy: 0.7386\n",
      "Epoch 20/100\n",
      "417/417 - 13s - loss: 0.6624 - accuracy: 0.7389\n",
      "Epoch 21/100\n",
      "417/417 - 13s - loss: 0.6562 - accuracy: 0.7398\n",
      "Epoch 22/100\n",
      "417/417 - 13s - loss: 0.6536 - accuracy: 0.7394\n",
      "Epoch 23/100\n",
      "417/417 - 13s - loss: 0.6495 - accuracy: 0.7402\n",
      "Epoch 24/100\n",
      "417/417 - 13s - loss: 0.6509 - accuracy: 0.7399\n",
      "Epoch 25/100\n",
      "417/417 - 13s - loss: 0.6464 - accuracy: 0.7401\n",
      "Epoch 26/100\n",
      "417/417 - 13s - loss: 0.6445 - accuracy: 0.7399\n",
      "Epoch 27/100\n",
      "417/417 - 13s - loss: 0.6406 - accuracy: 0.7390\n",
      "Epoch 28/100\n",
      "417/417 - 13s - loss: 0.6397 - accuracy: 0.7404\n",
      "Epoch 29/100\n",
      "417/417 - 13s - loss: 0.6361 - accuracy: 0.7404\n",
      "Epoch 30/100\n",
      "417/417 - 13s - loss: 0.6353 - accuracy: 0.7416\n",
      "Epoch 31/100\n",
      "417/417 - 13s - loss: 0.6331 - accuracy: 0.7417\n",
      "Epoch 32/100\n",
      "417/417 - 13s - loss: 0.6321 - accuracy: 0.7404\n",
      "Epoch 33/100\n",
      "417/417 - 13s - loss: 0.6317 - accuracy: 0.7423\n",
      "Epoch 34/100\n",
      "417/417 - 13s - loss: 0.6301 - accuracy: 0.7426\n",
      "Epoch 35/100\n",
      "417/417 - 13s - loss: 0.6269 - accuracy: 0.7411\n",
      "Epoch 36/100\n",
      "417/417 - 13s - loss: 0.6259 - accuracy: 0.7415\n",
      "Epoch 37/100\n",
      "417/417 - 13s - loss: 0.6231 - accuracy: 0.7434\n",
      "Epoch 38/100\n",
      "417/417 - 13s - loss: 0.6232 - accuracy: 0.7429\n",
      "Epoch 39/100\n",
      "417/417 - 13s - loss: 0.6202 - accuracy: 0.7420\n",
      "Epoch 40/100\n",
      "417/417 - 13s - loss: 0.6194 - accuracy: 0.7432\n",
      "Epoch 41/100\n",
      "417/417 - 13s - loss: 0.6180 - accuracy: 0.7452\n",
      "Epoch 42/100\n",
      "417/417 - 13s - loss: 0.6163 - accuracy: 0.7425\n",
      "Epoch 43/100\n",
      "417/417 - 13s - loss: 0.6153 - accuracy: 0.7429\n",
      "Epoch 44/100\n",
      "417/417 - 13s - loss: 0.6142 - accuracy: 0.7439\n",
      "Epoch 45/100\n",
      "417/417 - 13s - loss: 0.6112 - accuracy: 0.7452\n",
      "Epoch 46/100\n",
      "417/417 - 13s - loss: 0.6111 - accuracy: 0.7476\n",
      "Epoch 47/100\n",
      "417/417 - 13s - loss: 0.6086 - accuracy: 0.7456\n",
      "Epoch 48/100\n",
      "417/417 - 13s - loss: 0.6071 - accuracy: 0.7462\n",
      "Epoch 49/100\n",
      "417/417 - 13s - loss: 0.6064 - accuracy: 0.7447\n",
      "Epoch 50/100\n",
      "417/417 - 13s - loss: 0.6047 - accuracy: 0.7470\n",
      "Epoch 51/100\n",
      "417/417 - 13s - loss: 0.6045 - accuracy: 0.7463\n",
      "Epoch 52/100\n",
      "417/417 - 13s - loss: 0.6039 - accuracy: 0.7465\n",
      "Epoch 53/100\n",
      "417/417 - 13s - loss: 0.6046 - accuracy: 0.7433\n",
      "Epoch 54/100\n",
      "417/417 - 13s - loss: 0.5996 - accuracy: 0.7474\n",
      "Epoch 55/100\n",
      "417/417 - 13s - loss: 0.5960 - accuracy: 0.7486\n",
      "Epoch 56/100\n",
      "417/417 - 13s - loss: 0.5982 - accuracy: 0.7504\n",
      "Epoch 57/100\n",
      "417/417 - 13s - loss: 0.5937 - accuracy: 0.7465\n",
      "Epoch 58/100\n",
      "417/417 - 13s - loss: 0.5906 - accuracy: 0.7491\n",
      "Epoch 59/100\n",
      "417/417 - 13s - loss: 0.5892 - accuracy: 0.7486\n",
      "Epoch 60/100\n",
      "417/417 - 13s - loss: 0.5903 - accuracy: 0.7518\n",
      "Epoch 61/100\n",
      "417/417 - 13s - loss: 0.5876 - accuracy: 0.7482\n",
      "Epoch 62/100\n",
      "417/417 - 13s - loss: 0.5880 - accuracy: 0.7503\n",
      "Epoch 63/100\n",
      "417/417 - 13s - loss: 0.5858 - accuracy: 0.7479\n",
      "Epoch 64/100\n",
      "417/417 - 13s - loss: 0.5832 - accuracy: 0.7505\n",
      "Epoch 65/100\n",
      "417/417 - 13s - loss: 0.5834 - accuracy: 0.7522\n",
      "Epoch 66/100\n",
      "417/417 - 13s - loss: 0.5827 - accuracy: 0.7522\n",
      "Epoch 67/100\n",
      "417/417 - 13s - loss: 0.5812 - accuracy: 0.7520\n",
      "Epoch 68/100\n",
      "417/417 - 13s - loss: 0.5796 - accuracy: 0.7504\n",
      "Epoch 69/100\n",
      "417/417 - 13s - loss: 0.5777 - accuracy: 0.7500\n",
      "Epoch 70/100\n",
      "417/417 - 13s - loss: 0.5785 - accuracy: 0.7482\n",
      "Epoch 71/100\n",
      "417/417 - 13s - loss: 0.5759 - accuracy: 0.7523\n",
      "Epoch 72/100\n",
      "417/417 - 13s - loss: 0.5750 - accuracy: 0.7538\n",
      "Epoch 73/100\n",
      "417/417 - 13s - loss: 0.5742 - accuracy: 0.7554\n",
      "Epoch 74/100\n",
      "417/417 - 12s - loss: 0.5747 - accuracy: 0.7526\n",
      "Epoch 75/100\n",
      "417/417 - 13s - loss: 0.5750 - accuracy: 0.7534\n",
      "Epoch 76/100\n",
      "417/417 - 13s - loss: 0.5730 - accuracy: 0.7556\n",
      "Epoch 77/100\n",
      "417/417 - 13s - loss: 0.5723 - accuracy: 0.7562\n",
      "Epoch 78/100\n",
      "417/417 - 13s - loss: 0.5721 - accuracy: 0.7534\n",
      "Epoch 79/100\n",
      "417/417 - 13s - loss: 0.5706 - accuracy: 0.7538\n",
      "Epoch 80/100\n",
      "417/417 - 13s - loss: 0.5678 - accuracy: 0.7569\n",
      "Epoch 81/100\n",
      "417/417 - 13s - loss: 0.5698 - accuracy: 0.7542\n",
      "Epoch 82/100\n",
      "417/417 - 13s - loss: 0.5683 - accuracy: 0.7562\n",
      "Epoch 83/100\n",
      "417/417 - 13s - loss: 0.5683 - accuracy: 0.7572\n",
      "Epoch 84/100\n",
      "417/417 - 13s - loss: 0.5678 - accuracy: 0.7548\n",
      "Epoch 85/100\n",
      "417/417 - 13s - loss: 0.5641 - accuracy: 0.7593\n",
      "Epoch 86/100\n",
      "417/417 - 13s - loss: 0.5656 - accuracy: 0.7568\n",
      "Epoch 87/100\n",
      "417/417 - 13s - loss: 0.5647 - accuracy: 0.7590\n",
      "Epoch 88/100\n",
      "417/417 - 13s - loss: 0.5627 - accuracy: 0.7618\n",
      "Epoch 89/100\n",
      "417/417 - 13s - loss: 0.5640 - accuracy: 0.7599\n",
      "Epoch 90/100\n",
      "417/417 - 13s - loss: 0.5636 - accuracy: 0.7593\n",
      "Epoch 91/100\n",
      "417/417 - 13s - loss: 0.5601 - accuracy: 0.7581\n",
      "Epoch 92/100\n",
      "417/417 - 13s - loss: 0.5618 - accuracy: 0.7580\n",
      "Epoch 93/100\n",
      "417/417 - 13s - loss: 0.5625 - accuracy: 0.7585\n",
      "Epoch 94/100\n",
      "417/417 - 13s - loss: 0.5587 - accuracy: 0.7592\n",
      "Epoch 95/100\n",
      "417/417 - 13s - loss: 0.5610 - accuracy: 0.7567\n",
      "Epoch 96/100\n",
      "417/417 - 13s - loss: 0.5603 - accuracy: 0.7586\n",
      "Epoch 97/100\n",
      "417/417 - 13s - loss: 0.5569 - accuracy: 0.7625\n",
      "Epoch 98/100\n",
      "417/417 - 13s - loss: 0.5604 - accuracy: 0.7611\n",
      "Epoch 99/100\n",
      "417/417 - 13s - loss: 0.5580 - accuracy: 0.7610\n",
      "Epoch 100/100\n",
      "417/417 - 13s - loss: 0.5595 - accuracy: 0.7568\n",
      "417/417 - 3s - loss: 0.5921 - accuracy: 0.7484\n",
      "Epoch 1/100\n",
      "833/833 - 26s - loss: 0.8136 - accuracy: 0.7179\n",
      "Epoch 2/100\n",
      "833/833 - 25s - loss: 0.7533 - accuracy: 0.7396\n",
      "Epoch 3/100\n",
      "833/833 - 25s - loss: 0.7521 - accuracy: 0.7396\n",
      "Epoch 4/100\n",
      "833/833 - 26s - loss: 0.7505 - accuracy: 0.7396\n",
      "Epoch 5/100\n",
      "833/833 - 25s - loss: 0.7480 - accuracy: 0.7396\n",
      "Epoch 6/100\n",
      "833/833 - 25s - loss: 0.7436 - accuracy: 0.7396\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "833/833 - 25s - loss: 0.7352 - accuracy: 0.7396\n",
      "Epoch 8/100\n",
      "833/833 - 25s - loss: 0.7207 - accuracy: 0.7396\n",
      "Epoch 9/100\n",
      "833/833 - 25s - loss: 0.6996 - accuracy: 0.7396\n",
      "Epoch 10/100\n",
      "833/833 - 25s - loss: 0.6757 - accuracy: 0.7395\n",
      "Epoch 11/100\n",
      "833/833 - 25s - loss: 0.6573 - accuracy: 0.7401\n",
      "Epoch 12/100\n",
      "833/833 - 25s - loss: 0.6464 - accuracy: 0.7407\n",
      "Epoch 13/100\n",
      "833/833 - 25s - loss: 0.6415 - accuracy: 0.7409\n",
      "Epoch 14/100\n",
      "833/833 - 25s - loss: 0.6379 - accuracy: 0.7416\n",
      "Epoch 15/100\n",
      "833/833 - 25s - loss: 0.6353 - accuracy: 0.7417\n",
      "Epoch 16/100\n",
      "833/833 - 25s - loss: 0.6314 - accuracy: 0.7423\n",
      "Epoch 17/100\n",
      "833/833 - 25s - loss: 0.6310 - accuracy: 0.7427\n",
      "Epoch 18/100\n",
      "833/833 - 25s - loss: 0.6282 - accuracy: 0.7446\n",
      "Epoch 19/100\n",
      "833/833 - 25s - loss: 0.6264 - accuracy: 0.7444\n",
      "Epoch 20/100\n",
      "833/833 - 25s - loss: 0.6241 - accuracy: 0.7445\n",
      "Epoch 21/100\n",
      "833/833 - 25s - loss: 0.6245 - accuracy: 0.7452\n",
      "Epoch 22/100\n",
      "833/833 - 25s - loss: 0.6219 - accuracy: 0.7469\n",
      "Epoch 23/100\n",
      "833/833 - 25s - loss: 0.6205 - accuracy: 0.7456\n",
      "Epoch 24/100\n",
      "833/833 - 25s - loss: 0.6188 - accuracy: 0.7465\n",
      "Epoch 25/100\n",
      "833/833 - 25s - loss: 0.6163 - accuracy: 0.7464\n",
      "Epoch 26/100\n",
      "833/833 - 25s - loss: 0.6159 - accuracy: 0.7466\n",
      "Epoch 27/100\n",
      "833/833 - 25s - loss: 0.6144 - accuracy: 0.7465\n",
      "Epoch 28/100\n",
      "833/833 - 25s - loss: 0.6116 - accuracy: 0.7481\n",
      "Epoch 29/100\n",
      "833/833 - 25s - loss: 0.6107 - accuracy: 0.7474\n",
      "Epoch 30/100\n",
      "833/833 - 25s - loss: 0.6084 - accuracy: 0.7503\n",
      "Epoch 31/100\n",
      "833/833 - 25s - loss: 0.6080 - accuracy: 0.7487\n",
      "Epoch 32/100\n",
      "833/833 - 25s - loss: 0.6054 - accuracy: 0.7483\n",
      "Epoch 33/100\n",
      "833/833 - 25s - loss: 0.6048 - accuracy: 0.7482\n",
      "Epoch 34/100\n",
      "833/833 - 25s - loss: 0.6031 - accuracy: 0.7482\n",
      "Epoch 35/100\n",
      "833/833 - 25s - loss: 0.6015 - accuracy: 0.7491\n",
      "Epoch 36/100\n",
      "833/833 - 25s - loss: 0.6011 - accuracy: 0.7488\n",
      "Epoch 37/100\n",
      "833/833 - 25s - loss: 0.5989 - accuracy: 0.7500\n",
      "Epoch 38/100\n",
      "833/833 - 25s - loss: 0.5972 - accuracy: 0.7511\n",
      "Epoch 39/100\n",
      "833/833 - 25s - loss: 0.5977 - accuracy: 0.7494\n",
      "Epoch 40/100\n",
      "833/833 - 25s - loss: 0.5961 - accuracy: 0.7510\n",
      "Epoch 41/100\n",
      "833/833 - 25s - loss: 0.5949 - accuracy: 0.7516\n",
      "Epoch 42/100\n",
      "833/833 - 25s - loss: 0.5942 - accuracy: 0.7496\n",
      "Epoch 43/100\n",
      "833/833 - 25s - loss: 0.5929 - accuracy: 0.7504\n",
      "Epoch 44/100\n",
      "833/833 - 25s - loss: 0.5915 - accuracy: 0.7502\n",
      "Epoch 45/100\n",
      "833/833 - 25s - loss: 0.5919 - accuracy: 0.7499\n",
      "Epoch 46/100\n",
      "833/833 - 25s - loss: 0.5914 - accuracy: 0.7519\n",
      "Epoch 47/100\n",
      "833/833 - 25s - loss: 0.5891 - accuracy: 0.7528\n",
      "Epoch 48/100\n",
      "833/833 - 25s - loss: 0.5898 - accuracy: 0.7510\n",
      "Epoch 49/100\n",
      "833/833 - 25s - loss: 0.5888 - accuracy: 0.7530\n",
      "Epoch 50/100\n",
      "833/833 - 25s - loss: 0.5869 - accuracy: 0.7515\n",
      "Epoch 51/100\n",
      "833/833 - 25s - loss: 0.5882 - accuracy: 0.7521\n",
      "Epoch 52/100\n",
      "833/833 - 25s - loss: 0.5858 - accuracy: 0.7534\n",
      "Epoch 53/100\n",
      "833/833 - 25s - loss: 0.5850 - accuracy: 0.7540\n",
      "Epoch 54/100\n",
      "833/833 - 25s - loss: 0.5831 - accuracy: 0.7554\n",
      "Epoch 55/100\n",
      "833/833 - 25s - loss: 0.5823 - accuracy: 0.7542\n",
      "Epoch 56/100\n",
      "833/833 - 25s - loss: 0.5824 - accuracy: 0.7544\n",
      "Epoch 57/100\n",
      "833/833 - 25s - loss: 0.5824 - accuracy: 0.7545\n",
      "Epoch 58/100\n",
      "833/833 - 25s - loss: 0.5823 - accuracy: 0.7555\n",
      "Epoch 59/100\n",
      "833/833 - 25s - loss: 0.5805 - accuracy: 0.7568\n",
      "Epoch 60/100\n",
      "833/833 - 25s - loss: 0.5804 - accuracy: 0.7566\n",
      "Epoch 61/100\n",
      "833/833 - 25s - loss: 0.5787 - accuracy: 0.7564\n",
      "Epoch 62/100\n",
      "833/833 - 25s - loss: 0.5794 - accuracy: 0.7573\n",
      "Epoch 63/100\n",
      "833/833 - 25s - loss: 0.5798 - accuracy: 0.7553\n",
      "Epoch 64/100\n",
      "833/833 - 25s - loss: 0.5778 - accuracy: 0.7567\n",
      "Epoch 65/100\n",
      "833/833 - 25s - loss: 0.5765 - accuracy: 0.7579\n",
      "Epoch 66/100\n",
      "833/833 - 25s - loss: 0.5768 - accuracy: 0.7559\n",
      "Epoch 67/100\n",
      "833/833 - 25s - loss: 0.5761 - accuracy: 0.7555\n",
      "Epoch 68/100\n",
      "833/833 - 25s - loss: 0.5762 - accuracy: 0.7572\n",
      "Epoch 69/100\n",
      "833/833 - 25s - loss: 0.5739 - accuracy: 0.7569\n",
      "Epoch 70/100\n",
      "833/833 - 25s - loss: 0.5748 - accuracy: 0.7563\n",
      "Epoch 71/100\n",
      "833/833 - 25s - loss: 0.5747 - accuracy: 0.7581\n",
      "Epoch 72/100\n",
      "833/833 - 25s - loss: 0.5731 - accuracy: 0.7585\n",
      "Epoch 73/100\n",
      "833/833 - 25s - loss: 0.5730 - accuracy: 0.7595\n",
      "Epoch 74/100\n",
      "833/833 - 25s - loss: 0.5728 - accuracy: 0.7583\n",
      "Epoch 75/100\n",
      "833/833 - 25s - loss: 0.5736 - accuracy: 0.7569\n",
      "Epoch 76/100\n",
      "833/833 - 25s - loss: 0.5722 - accuracy: 0.7608\n",
      "Epoch 77/100\n",
      "833/833 - 25s - loss: 0.5712 - accuracy: 0.7599\n",
      "Epoch 78/100\n",
      "833/833 - 25s - loss: 0.5724 - accuracy: 0.7564\n",
      "Epoch 79/100\n",
      "833/833 - 25s - loss: 0.5717 - accuracy: 0.7595\n",
      "Epoch 80/100\n",
      "833/833 - 25s - loss: 0.5701 - accuracy: 0.7581\n",
      "Epoch 81/100\n",
      "833/833 - 25s - loss: 0.5704 - accuracy: 0.7603\n",
      "Epoch 82/100\n",
      "833/833 - 25s - loss: 0.5689 - accuracy: 0.7606\n",
      "Epoch 83/100\n",
      "833/833 - 25s - loss: 0.5701 - accuracy: 0.7601\n",
      "Epoch 84/100\n",
      "833/833 - 25s - loss: 0.5687 - accuracy: 0.7618\n",
      "Epoch 85/100\n",
      "833/833 - 25s - loss: 0.5688 - accuracy: 0.7597\n",
      "Epoch 86/100\n",
      "833/833 - 25s - loss: 0.5684 - accuracy: 0.7611\n",
      "Epoch 87/100\n",
      "833/833 - 25s - loss: 0.5689 - accuracy: 0.7588\n",
      "Epoch 88/100\n",
      "833/833 - 25s - loss: 0.5659 - accuracy: 0.7623\n",
      "Epoch 89/100\n",
      "833/833 - 25s - loss: 0.5673 - accuracy: 0.7589\n",
      "Epoch 90/100\n",
      "833/833 - 25s - loss: 0.5666 - accuracy: 0.7585\n",
      "Epoch 91/100\n",
      "833/833 - 25s - loss: 0.5650 - accuracy: 0.7605\n",
      "Epoch 92/100\n",
      "833/833 - 25s - loss: 0.5656 - accuracy: 0.7613\n",
      "Epoch 93/100\n",
      "833/833 - 25s - loss: 0.5641 - accuracy: 0.7599\n",
      "Epoch 94/100\n",
      "833/833 - 25s - loss: 0.5658 - accuracy: 0.7596\n",
      "Epoch 95/100\n",
      "833/833 - 25s - loss: 0.5653 - accuracy: 0.7621\n",
      "Epoch 96/100\n",
      "833/833 - 25s - loss: 0.5650 - accuracy: 0.7598\n",
      "Epoch 97/100\n",
      "833/833 - 25s - loss: 0.5632 - accuracy: 0.7605\n",
      "Epoch 98/100\n",
      "833/833 - 25s - loss: 0.5639 - accuracy: 0.7603\n",
      "Epoch 99/100\n",
      "833/833 - 25s - loss: 0.5634 - accuracy: 0.7629\n",
      "Epoch 100/100\n",
      "833/833 - 25s - loss: 0.5644 - accuracy: 0.7607\n",
      "417/417 - 3s - loss: 0.5490 - accuracy: 0.7625\n",
      "Epoch 1/100\n",
      "1249/1249 - 39s - loss: 0.7787 - accuracy: 0.7346\n",
      "Epoch 2/100\n",
      "1249/1249 - 39s - loss: 0.7438 - accuracy: 0.7441\n",
      "Epoch 3/100\n",
      "1249/1249 - 39s - loss: 0.7399 - accuracy: 0.7441\n",
      "Epoch 4/100\n",
      "1249/1249 - 39s - loss: 0.7297 - accuracy: 0.7441\n",
      "Epoch 5/100\n",
      "1249/1249 - 39s - loss: 0.7073 - accuracy: 0.7441\n",
      "Epoch 6/100\n",
      "1249/1249 - 38s - loss: 0.6767 - accuracy: 0.7440\n",
      "Epoch 7/100\n",
      "1249/1249 - 38s - loss: 0.6525 - accuracy: 0.7442\n",
      "Epoch 8/100\n",
      "1249/1249 - 38s - loss: 0.6369 - accuracy: 0.7441\n",
      "Epoch 9/100\n",
      "1249/1249 - 38s - loss: 0.6294 - accuracy: 0.7450\n",
      "Epoch 10/100\n",
      "1249/1249 - 38s - loss: 0.6237 - accuracy: 0.7451\n",
      "Epoch 11/100\n",
      "1249/1249 - 38s - loss: 0.6194 - accuracy: 0.7467\n",
      "Epoch 12/100\n",
      "1249/1249 - 38s - loss: 0.6153 - accuracy: 0.7475\n",
      "Epoch 13/100\n",
      "1249/1249 - 38s - loss: 0.6129 - accuracy: 0.7475\n",
      "Epoch 14/100\n",
      "1249/1249 - 38s - loss: 0.6089 - accuracy: 0.7484\n",
      "Epoch 15/100\n",
      "1249/1249 - 38s - loss: 0.6068 - accuracy: 0.7495\n",
      "Epoch 16/100\n",
      "1249/1249 - 38s - loss: 0.6031 - accuracy: 0.7492\n",
      "Epoch 17/100\n",
      "1249/1249 - 38s - loss: 0.6005 - accuracy: 0.7518\n",
      "Epoch 18/100\n",
      "1249/1249 - 38s - loss: 0.5991 - accuracy: 0.7497\n",
      "Epoch 19/100\n",
      "1249/1249 - 38s - loss: 0.5967 - accuracy: 0.7508\n",
      "Epoch 20/100\n",
      "1249/1249 - 38s - loss: 0.5940 - accuracy: 0.7520\n",
      "Epoch 21/100\n",
      "1249/1249 - 38s - loss: 0.5909 - accuracy: 0.7518\n",
      "Epoch 22/100\n",
      "1249/1249 - 38s - loss: 0.5890 - accuracy: 0.7548\n",
      "Epoch 23/100\n",
      "1249/1249 - 38s - loss: 0.5864 - accuracy: 0.7536\n",
      "Epoch 24/100\n",
      "1249/1249 - 38s - loss: 0.5844 - accuracy: 0.7548\n",
      "Epoch 25/100\n",
      "1249/1249 - 38s - loss: 0.5811 - accuracy: 0.7552\n",
      "Epoch 26/100\n",
      "1249/1249 - 38s - loss: 0.5781 - accuracy: 0.7557\n",
      "Epoch 27/100\n",
      "1249/1249 - 38s - loss: 0.5743 - accuracy: 0.7561\n",
      "Epoch 28/100\n",
      "1249/1249 - 38s - loss: 0.5740 - accuracy: 0.7543\n",
      "Epoch 29/100\n",
      "1249/1249 - 38s - loss: 0.5711 - accuracy: 0.7540\n",
      "Epoch 30/100\n",
      "1249/1249 - 38s - loss: 0.5660 - accuracy: 0.7567\n",
      "Epoch 31/100\n",
      "1249/1249 - 38s - loss: 0.5644 - accuracy: 0.7563\n",
      "Epoch 32/100\n",
      "1249/1249 - 38s - loss: 0.5630 - accuracy: 0.7569\n",
      "Epoch 33/100\n",
      "1249/1249 - 38s - loss: 0.5604 - accuracy: 0.7582\n",
      "Epoch 34/100\n",
      "1249/1249 - 38s - loss: 0.5582 - accuracy: 0.7573\n",
      "Epoch 35/100\n",
      "1249/1249 - 38s - loss: 0.5540 - accuracy: 0.7607\n",
      "Epoch 36/100\n",
      "1249/1249 - 38s - loss: 0.5529 - accuracy: 0.7588\n",
      "Epoch 37/100\n",
      "1249/1249 - 38s - loss: 0.5513 - accuracy: 0.7580\n",
      "Epoch 38/100\n",
      "1249/1249 - 38s - loss: 0.5511 - accuracy: 0.7590\n",
      "Epoch 39/100\n",
      "1249/1249 - 38s - loss: 0.5488 - accuracy: 0.7612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100\n",
      "1249/1249 - 38s - loss: 0.5485 - accuracy: 0.7618\n",
      "Epoch 41/100\n",
      "1249/1249 - 38s - loss: 0.5458 - accuracy: 0.7598\n",
      "Epoch 42/100\n",
      "1249/1249 - 38s - loss: 0.5443 - accuracy: 0.7623\n",
      "Epoch 43/100\n",
      "1249/1249 - 38s - loss: 0.5423 - accuracy: 0.7624\n",
      "Epoch 44/100\n",
      "1249/1249 - 38s - loss: 0.5433 - accuracy: 0.7614\n",
      "Epoch 45/100\n",
      "1249/1249 - 38s - loss: 0.5416 - accuracy: 0.7629\n",
      "Epoch 46/100\n",
      "1249/1249 - 38s - loss: 0.5399 - accuracy: 0.7635\n",
      "Epoch 47/100\n",
      "1249/1249 - 38s - loss: 0.5398 - accuracy: 0.7643\n",
      "Epoch 48/100\n",
      "1249/1249 - 38s - loss: 0.5395 - accuracy: 0.7632\n",
      "Epoch 49/100\n",
      "1249/1249 - 38s - loss: 0.5388 - accuracy: 0.7647\n",
      "Epoch 50/100\n",
      "1249/1249 - 38s - loss: 0.5376 - accuracy: 0.7642\n",
      "Epoch 51/100\n",
      "1249/1249 - 38s - loss: 0.5378 - accuracy: 0.7640\n",
      "Epoch 52/100\n",
      "1249/1249 - 38s - loss: 0.5355 - accuracy: 0.7641\n",
      "Epoch 53/100\n",
      "1249/1249 - 38s - loss: 0.5366 - accuracy: 0.7645\n",
      "Epoch 54/100\n",
      "1249/1249 - 38s - loss: 0.5344 - accuracy: 0.7668\n",
      "Epoch 55/100\n",
      "1249/1249 - 38s - loss: 0.5351 - accuracy: 0.7640\n",
      "Epoch 56/100\n",
      "1249/1249 - 38s - loss: 0.5334 - accuracy: 0.7678\n",
      "Epoch 57/100\n",
      "1249/1249 - 38s - loss: 0.5334 - accuracy: 0.7655\n",
      "Epoch 58/100\n",
      "1249/1249 - 38s - loss: 0.5329 - accuracy: 0.7677\n",
      "Epoch 59/100\n",
      "1249/1249 - 38s - loss: 0.5316 - accuracy: 0.7678\n",
      "Epoch 60/100\n",
      "1249/1249 - 38s - loss: 0.5293 - accuracy: 0.7675\n",
      "Epoch 61/100\n",
      "1249/1249 - 38s - loss: 0.5312 - accuracy: 0.7671\n",
      "Epoch 62/100\n",
      "1249/1249 - 38s - loss: 0.5307 - accuracy: 0.7682\n",
      "Epoch 63/100\n",
      "1249/1249 - 38s - loss: 0.5303 - accuracy: 0.7665\n",
      "Epoch 64/100\n",
      "1249/1249 - 38s - loss: 0.5307 - accuracy: 0.7666\n",
      "Epoch 65/100\n",
      "1249/1249 - 38s - loss: 0.5292 - accuracy: 0.7682\n",
      "Epoch 66/100\n",
      "1249/1249 - 38s - loss: 0.5290 - accuracy: 0.7678\n",
      "Epoch 67/100\n",
      "1249/1249 - 38s - loss: 0.5283 - accuracy: 0.7681\n",
      "Epoch 68/100\n",
      "1249/1249 - 38s - loss: 0.5282 - accuracy: 0.7684\n",
      "Epoch 69/100\n",
      "1249/1249 - 38s - loss: 0.5268 - accuracy: 0.7685\n",
      "Epoch 70/100\n",
      "1249/1249 - 38s - loss: 0.5276 - accuracy: 0.7679\n",
      "Epoch 71/100\n",
      "1249/1249 - 38s - loss: 0.5276 - accuracy: 0.7688\n",
      "Epoch 72/100\n",
      "1249/1249 - 38s - loss: 0.5261 - accuracy: 0.7687\n",
      "Epoch 73/100\n",
      "1249/1249 - 38s - loss: 0.5262 - accuracy: 0.7707\n",
      "Epoch 74/100\n",
      "1249/1249 - 38s - loss: 0.5251 - accuracy: 0.7707\n",
      "Epoch 75/100\n",
      "1249/1249 - 38s - loss: 0.5256 - accuracy: 0.7703\n",
      "Epoch 76/100\n",
      "1249/1249 - 38s - loss: 0.5244 - accuracy: 0.7692\n",
      "Epoch 77/100\n",
      "1249/1249 - 39s - loss: 0.5233 - accuracy: 0.7697\n",
      "Epoch 78/100\n",
      "1249/1249 - 39s - loss: 0.5231 - accuracy: 0.7704\n",
      "Epoch 79/100\n",
      "1249/1249 - 38s - loss: 0.5231 - accuracy: 0.7694\n",
      "Epoch 80/100\n",
      "1249/1249 - 38s - loss: 0.5227 - accuracy: 0.7699\n",
      "Epoch 81/100\n",
      "1249/1249 - 38s - loss: 0.5225 - accuracy: 0.7702\n",
      "Epoch 82/100\n",
      "1249/1249 - 38s - loss: 0.5221 - accuracy: 0.7714\n",
      "Epoch 83/100\n",
      "1249/1249 - 38s - loss: 0.5213 - accuracy: 0.7713\n",
      "Epoch 84/100\n",
      "1249/1249 - 39s - loss: 0.5212 - accuracy: 0.7712\n",
      "Epoch 85/100\n",
      "1249/1249 - 38s - loss: 0.5218 - accuracy: 0.7696\n",
      "Epoch 86/100\n",
      "1249/1249 - 39s - loss: 0.5194 - accuracy: 0.7708\n",
      "Epoch 87/100\n",
      "1249/1249 - 39s - loss: 0.5210 - accuracy: 0.7713\n",
      "Epoch 88/100\n",
      "1249/1249 - 38s - loss: 0.5201 - accuracy: 0.7721\n",
      "Epoch 89/100\n",
      "1249/1249 - 38s - loss: 0.5185 - accuracy: 0.7729\n",
      "Epoch 90/100\n",
      "1249/1249 - 38s - loss: 0.5202 - accuracy: 0.7716\n",
      "Epoch 91/100\n",
      "1249/1249 - 38s - loss: 0.5181 - accuracy: 0.7723\n",
      "Epoch 92/100\n",
      "1249/1249 - 38s - loss: 0.5188 - accuracy: 0.7715\n",
      "Epoch 93/100\n",
      "1249/1249 - 39s - loss: 0.5178 - accuracy: 0.7728\n",
      "Epoch 94/100\n",
      "1249/1249 - 39s - loss: 0.5182 - accuracy: 0.7722\n",
      "Epoch 95/100\n",
      "1249/1249 - 38s - loss: 0.5174 - accuracy: 0.7728\n",
      "Epoch 96/100\n",
      "1249/1249 - 38s - loss: 0.5181 - accuracy: 0.7719\n",
      "Epoch 97/100\n",
      "1249/1249 - 39s - loss: 0.5164 - accuracy: 0.7722\n",
      "Epoch 98/100\n",
      "1249/1249 - 39s - loss: 0.5168 - accuracy: 0.7714\n",
      "Epoch 99/100\n",
      "1249/1249 - 38s - loss: 0.5165 - accuracy: 0.7736\n",
      "Epoch 100/100\n",
      "1249/1249 - 38s - loss: 0.5164 - accuracy: 0.7728\n",
      "417/417 - 3s - loss: 0.5602 - accuracy: 0.7415\n",
      "Epoch 1/100\n",
      "1666/1666 - 52s - loss: 0.7724 - accuracy: 0.7300\n",
      "Epoch 2/100\n",
      "1666/1666 - 51s - loss: 0.7414 - accuracy: 0.7413\n",
      "Epoch 3/100\n",
      "1666/1666 - 51s - loss: 0.7212 - accuracy: 0.7413\n",
      "Epoch 4/100\n",
      "1666/1666 - 51s - loss: 0.6874 - accuracy: 0.7412\n",
      "Epoch 5/100\n",
      "1666/1666 - 51s - loss: 0.6610 - accuracy: 0.7415\n",
      "Epoch 6/100\n",
      "1666/1666 - 51s - loss: 0.6483 - accuracy: 0.7417\n",
      "Epoch 7/100\n",
      "1666/1666 - 51s - loss: 0.6402 - accuracy: 0.7427\n",
      "Epoch 8/100\n",
      "1666/1666 - 51s - loss: 0.6349 - accuracy: 0.7429\n",
      "Epoch 9/100\n",
      "1666/1666 - 51s - loss: 0.6311 - accuracy: 0.7424\n",
      "Epoch 10/100\n",
      "1666/1666 - 51s - loss: 0.6273 - accuracy: 0.7420\n",
      "Epoch 11/100\n",
      "1666/1666 - 50s - loss: 0.6232 - accuracy: 0.7418\n",
      "Epoch 12/100\n",
      "1666/1666 - 50s - loss: 0.6203 - accuracy: 0.7435\n",
      "Epoch 13/100\n",
      "1666/1666 - 50s - loss: 0.6176 - accuracy: 0.7427\n",
      "Epoch 14/100\n",
      "1666/1666 - 50s - loss: 0.6156 - accuracy: 0.7423\n",
      "Epoch 15/100\n",
      "1666/1666 - 50s - loss: 0.6128 - accuracy: 0.7435\n",
      "Epoch 16/100\n",
      "1666/1666 - 50s - loss: 0.6092 - accuracy: 0.7439\n",
      "Epoch 17/100\n",
      "1666/1666 - 50s - loss: 0.6058 - accuracy: 0.7432\n",
      "Epoch 18/100\n",
      "1666/1666 - 50s - loss: 0.6033 - accuracy: 0.7439\n",
      "Epoch 19/100\n",
      "1666/1666 - 50s - loss: 0.5979 - accuracy: 0.7449\n",
      "Epoch 20/100\n",
      "1666/1666 - 50s - loss: 0.5941 - accuracy: 0.7452\n",
      "Epoch 21/100\n",
      "1666/1666 - 50s - loss: 0.5906 - accuracy: 0.7460\n",
      "Epoch 22/100\n",
      "1666/1666 - 50s - loss: 0.5873 - accuracy: 0.7464\n",
      "Epoch 23/100\n",
      "1666/1666 - 50s - loss: 0.5825 - accuracy: 0.7488\n",
      "Epoch 24/100\n",
      "1666/1666 - 50s - loss: 0.5809 - accuracy: 0.7473\n",
      "Epoch 25/100\n",
      "1666/1666 - 50s - loss: 0.5773 - accuracy: 0.7496\n",
      "Epoch 26/100\n",
      "1666/1666 - 50s - loss: 0.5752 - accuracy: 0.7509\n",
      "Epoch 27/100\n",
      "1666/1666 - 50s - loss: 0.5726 - accuracy: 0.7526\n",
      "Epoch 28/100\n",
      "1666/1666 - 50s - loss: 0.5698 - accuracy: 0.7533\n",
      "Epoch 29/100\n",
      "1666/1666 - 50s - loss: 0.5674 - accuracy: 0.7531\n",
      "Epoch 30/100\n",
      "1666/1666 - 50s - loss: 0.5645 - accuracy: 0.7545\n",
      "Epoch 31/100\n",
      "1666/1666 - 50s - loss: 0.5605 - accuracy: 0.7560\n",
      "Epoch 32/100\n",
      "1666/1666 - 50s - loss: 0.5599 - accuracy: 0.7540\n",
      "Epoch 33/100\n",
      "1666/1666 - 50s - loss: 0.5579 - accuracy: 0.7548\n",
      "Epoch 34/100\n",
      "1666/1666 - 50s - loss: 0.5548 - accuracy: 0.7558\n",
      "Epoch 35/100\n",
      "1666/1666 - 50s - loss: 0.5540 - accuracy: 0.7549\n",
      "Epoch 36/100\n",
      "1666/1666 - 51s - loss: 0.5512 - accuracy: 0.7571\n",
      "Epoch 37/100\n",
      "1666/1666 - 50s - loss: 0.5499 - accuracy: 0.7578\n",
      "Epoch 38/100\n",
      "1666/1666 - 50s - loss: 0.5489 - accuracy: 0.7564\n",
      "Epoch 39/100\n",
      "1666/1666 - 51s - loss: 0.5478 - accuracy: 0.7573\n",
      "Epoch 40/100\n",
      "1666/1666 - 50s - loss: 0.5462 - accuracy: 0.7595\n",
      "Epoch 41/100\n",
      "1666/1666 - 50s - loss: 0.5448 - accuracy: 0.7600\n",
      "Epoch 42/100\n",
      "1666/1666 - 50s - loss: 0.5439 - accuracy: 0.7581\n",
      "Epoch 43/100\n",
      "1666/1666 - 50s - loss: 0.5432 - accuracy: 0.7590\n",
      "Epoch 44/100\n",
      "1666/1666 - 50s - loss: 0.5427 - accuracy: 0.7600\n",
      "Epoch 45/100\n",
      "1666/1666 - 50s - loss: 0.5408 - accuracy: 0.7593\n",
      "Epoch 46/100\n",
      "1666/1666 - 50s - loss: 0.5396 - accuracy: 0.7591\n",
      "Epoch 47/100\n",
      "1666/1666 - 50s - loss: 0.5398 - accuracy: 0.7602\n",
      "Epoch 48/100\n",
      "1666/1666 - 51s - loss: 0.5400 - accuracy: 0.7603\n",
      "Epoch 49/100\n",
      "1666/1666 - 51s - loss: 0.5381 - accuracy: 0.7611\n",
      "Epoch 50/100\n",
      "1666/1666 - 51s - loss: 0.5375 - accuracy: 0.7602\n",
      "Epoch 51/100\n",
      "1666/1666 - 51s - loss: 0.5360 - accuracy: 0.7617\n",
      "Epoch 52/100\n",
      "1666/1666 - 50s - loss: 0.5359 - accuracy: 0.7620\n",
      "Epoch 53/100\n",
      "1666/1666 - 51s - loss: 0.5361 - accuracy: 0.7624\n",
      "Epoch 54/100\n",
      "1666/1666 - 51s - loss: 0.5350 - accuracy: 0.7630\n",
      "Epoch 55/100\n",
      "1666/1666 - 50s - loss: 0.5344 - accuracy: 0.7607\n",
      "Epoch 56/100\n",
      "1666/1666 - 51s - loss: 0.5341 - accuracy: 0.7620\n",
      "Epoch 57/100\n",
      "1666/1666 - 51s - loss: 0.5338 - accuracy: 0.7627\n",
      "Epoch 58/100\n",
      "1666/1666 - 51s - loss: 0.5329 - accuracy: 0.7636\n",
      "Epoch 59/100\n",
      "1666/1666 - 51s - loss: 0.5335 - accuracy: 0.7629\n",
      "Epoch 60/100\n",
      "1666/1666 - 50s - loss: 0.5323 - accuracy: 0.7631\n",
      "Epoch 61/100\n",
      "1666/1666 - 51s - loss: 0.5323 - accuracy: 0.7638\n",
      "Epoch 62/100\n",
      "1666/1666 - 51s - loss: 0.5322 - accuracy: 0.7633\n",
      "Epoch 63/100\n",
      "1666/1666 - 51s - loss: 0.5311 - accuracy: 0.7634\n",
      "Epoch 64/100\n",
      "1666/1666 - 51s - loss: 0.5321 - accuracy: 0.7638\n",
      "Epoch 65/100\n",
      "1666/1666 - 51s - loss: 0.5309 - accuracy: 0.7635\n",
      "Epoch 66/100\n",
      "1666/1666 - 51s - loss: 0.5311 - accuracy: 0.7629\n",
      "Epoch 67/100\n",
      "1666/1666 - 50s - loss: 0.5299 - accuracy: 0.7639\n",
      "Epoch 68/100\n",
      "1666/1666 - 51s - loss: 0.5298 - accuracy: 0.7639\n",
      "Epoch 69/100\n",
      "1666/1666 - 51s - loss: 0.5289 - accuracy: 0.7641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/100\n",
      "1666/1666 - 53s - loss: 0.5289 - accuracy: 0.7651\n",
      "Epoch 71/100\n",
      "1666/1666 - 51s - loss: 0.5286 - accuracy: 0.7639\n",
      "Epoch 72/100\n",
      "1666/1666 - 51s - loss: 0.5283 - accuracy: 0.7654\n",
      "Epoch 73/100\n",
      "1666/1666 - 51s - loss: 0.5281 - accuracy: 0.7647\n",
      "Epoch 74/100\n",
      "1666/1666 - 51s - loss: 0.5280 - accuracy: 0.7665\n",
      "Epoch 75/100\n",
      "1666/1666 - 51s - loss: 0.5279 - accuracy: 0.7658\n",
      "Epoch 76/100\n",
      "1666/1666 - 50s - loss: 0.5272 - accuracy: 0.7643\n",
      "Epoch 77/100\n",
      "1666/1666 - 51s - loss: 0.5261 - accuracy: 0.7661\n",
      "Epoch 78/100\n",
      "1666/1666 - 51s - loss: 0.5270 - accuracy: 0.7654\n",
      "Epoch 79/100\n",
      "1666/1666 - 51s - loss: 0.5264 - accuracy: 0.7659\n",
      "Epoch 80/100\n",
      "1666/1666 - 51s - loss: 0.5272 - accuracy: 0.7648\n",
      "Epoch 81/100\n",
      "1666/1666 - 51s - loss: 0.5259 - accuracy: 0.7650\n",
      "Epoch 82/100\n",
      "1666/1666 - 51s - loss: 0.5249 - accuracy: 0.7658\n",
      "Epoch 83/100\n",
      "1666/1666 - 51s - loss: 0.5255 - accuracy: 0.7650\n",
      "Epoch 84/100\n",
      "1666/1666 - 51s - loss: 0.5246 - accuracy: 0.7679\n",
      "Epoch 85/100\n",
      "1666/1666 - 51s - loss: 0.5249 - accuracy: 0.7661\n",
      "Epoch 86/100\n",
      "1666/1666 - 51s - loss: 0.5244 - accuracy: 0.7664\n",
      "Epoch 87/100\n",
      "1666/1666 - 51s - loss: 0.5240 - accuracy: 0.7671\n",
      "Epoch 88/100\n",
      "1666/1666 - 51s - loss: 0.5244 - accuracy: 0.7661\n",
      "Epoch 89/100\n",
      "1666/1666 - 51s - loss: 0.5232 - accuracy: 0.7666\n",
      "Epoch 90/100\n",
      "1666/1666 - 51s - loss: 0.5237 - accuracy: 0.7670\n",
      "Epoch 91/100\n",
      "1666/1666 - 51s - loss: 0.5239 - accuracy: 0.7668\n",
      "Epoch 92/100\n",
      "1666/1666 - 51s - loss: 0.5235 - accuracy: 0.7674\n",
      "Epoch 93/100\n",
      "1666/1666 - 51s - loss: 0.5228 - accuracy: 0.7677\n",
      "Epoch 94/100\n",
      "1666/1666 - 51s - loss: 0.5220 - accuracy: 0.7681\n",
      "Epoch 95/100\n",
      "1666/1666 - 51s - loss: 0.5224 - accuracy: 0.7688\n",
      "Epoch 96/100\n",
      "1666/1666 - 51s - loss: 0.5218 - accuracy: 0.7676\n",
      "Epoch 97/100\n",
      "1666/1666 - 51s - loss: 0.5225 - accuracy: 0.7683\n",
      "Epoch 98/100\n",
      "1666/1666 - 51s - loss: 0.5229 - accuracy: 0.7665\n",
      "Epoch 99/100\n",
      "1666/1666 - 51s - loss: 0.5216 - accuracy: 0.7675\n",
      "Epoch 100/100\n",
      "1666/1666 - 51s - loss: 0.5208 - accuracy: 0.7690\n",
      "417/417 - 3s - loss: 0.5213 - accuracy: 0.7491\n",
      "Epoch 1/100\n",
      "2082/2082 - 65s - loss: 0.7674 - accuracy: 0.7374\n",
      "Epoch 2/100\n",
      "2082/2082 - 63s - loss: 0.7337 - accuracy: 0.7406\n",
      "Epoch 3/100\n",
      "2082/2082 - 63s - loss: 0.6945 - accuracy: 0.7406\n",
      "Epoch 4/100\n",
      "2082/2082 - 62s - loss: 0.6525 - accuracy: 0.7401\n",
      "Epoch 5/100\n",
      "2082/2082 - 63s - loss: 0.6382 - accuracy: 0.7411\n",
      "Epoch 6/100\n",
      "2082/2082 - 63s - loss: 0.6308 - accuracy: 0.7410\n",
      "Epoch 7/100\n",
      "2082/2082 - 63s - loss: 0.6262 - accuracy: 0.7412\n",
      "Epoch 8/100\n",
      "2082/2082 - 63s - loss: 0.6227 - accuracy: 0.7418\n",
      "Epoch 9/100\n",
      "2082/2082 - 63s - loss: 0.6185 - accuracy: 0.7418\n",
      "Epoch 10/100\n",
      "2082/2082 - 63s - loss: 0.6154 - accuracy: 0.7440\n",
      "Epoch 11/100\n",
      "2082/2082 - 63s - loss: 0.6123 - accuracy: 0.7411\n",
      "Epoch 12/100\n",
      "2082/2082 - 63s - loss: 0.6089 - accuracy: 0.7440\n",
      "Epoch 13/100\n",
      "2082/2082 - 63s - loss: 0.6053 - accuracy: 0.7438\n",
      "Epoch 14/100\n",
      "2082/2082 - 63s - loss: 0.6012 - accuracy: 0.7446\n",
      "Epoch 15/100\n",
      "2082/2082 - 63s - loss: 0.5993 - accuracy: 0.7455\n",
      "Epoch 16/100\n",
      "2082/2082 - 63s - loss: 0.5958 - accuracy: 0.7471\n",
      "Epoch 17/100\n",
      "2082/2082 - 63s - loss: 0.5928 - accuracy: 0.7476\n",
      "Epoch 18/100\n",
      "2082/2082 - 63s - loss: 0.5904 - accuracy: 0.7470\n",
      "Epoch 19/100\n",
      "2082/2082 - 63s - loss: 0.5889 - accuracy: 0.7481\n",
      "Epoch 20/100\n",
      "2082/2082 - 63s - loss: 0.5852 - accuracy: 0.7477\n",
      "Epoch 21/100\n",
      "2082/2082 - 63s - loss: 0.5833 - accuracy: 0.7488\n",
      "Epoch 22/100\n",
      "2082/2082 - 63s - loss: 0.5811 - accuracy: 0.7494\n",
      "Epoch 23/100\n",
      "2082/2082 - 63s - loss: 0.5769 - accuracy: 0.7498\n",
      "Epoch 24/100\n",
      "2082/2082 - 63s - loss: 0.5731 - accuracy: 0.7504\n",
      "Epoch 25/100\n",
      "2082/2082 - 63s - loss: 0.5695 - accuracy: 0.7516\n",
      "Epoch 26/100\n",
      "2082/2082 - 62s - loss: 0.5674 - accuracy: 0.7511\n",
      "Epoch 27/100\n",
      "2082/2082 - 62s - loss: 0.5643 - accuracy: 0.7511\n",
      "Epoch 28/100\n",
      "2082/2082 - 62s - loss: 0.5606 - accuracy: 0.7534\n",
      "Epoch 29/100\n",
      "2082/2082 - 62s - loss: 0.5591 - accuracy: 0.7532\n",
      "Epoch 30/100\n",
      "2082/2082 - 62s - loss: 0.5568 - accuracy: 0.7520\n",
      "Epoch 31/100\n",
      "2082/2082 - 63s - loss: 0.5550 - accuracy: 0.7526\n",
      "Epoch 32/100\n",
      "2082/2082 - 62s - loss: 0.5527 - accuracy: 0.7551\n",
      "Epoch 33/100\n",
      "2082/2082 - 62s - loss: 0.5513 - accuracy: 0.7544\n",
      "Epoch 34/100\n",
      "2082/2082 - 62s - loss: 0.5485 - accuracy: 0.7563\n",
      "Epoch 35/100\n",
      "2082/2082 - 62s - loss: 0.5477 - accuracy: 0.7551\n",
      "Epoch 36/100\n",
      "2082/2082 - 62s - loss: 0.5459 - accuracy: 0.7557\n",
      "Epoch 37/100\n",
      "2082/2082 - 63s - loss: 0.5448 - accuracy: 0.7563\n",
      "Epoch 38/100\n",
      "2082/2082 - 62s - loss: 0.5441 - accuracy: 0.7567\n",
      "Epoch 39/100\n",
      "2082/2082 - 61s - loss: 0.5424 - accuracy: 0.7575\n",
      "Epoch 40/100\n",
      "2082/2082 - 62s - loss: 0.5415 - accuracy: 0.7574\n",
      "Epoch 41/100\n",
      "2082/2082 - 61s - loss: 0.5410 - accuracy: 0.7564\n",
      "Epoch 42/100\n",
      "2082/2082 - 61s - loss: 0.5407 - accuracy: 0.7578\n",
      "Epoch 43/100\n",
      "2082/2082 - 61s - loss: 0.5388 - accuracy: 0.7582\n",
      "Epoch 44/100\n",
      "2082/2082 - 61s - loss: 0.5372 - accuracy: 0.7595\n",
      "Epoch 45/100\n",
      "2082/2082 - 61s - loss: 0.5365 - accuracy: 0.7582\n",
      "Epoch 46/100\n",
      "2082/2082 - 61s - loss: 0.5370 - accuracy: 0.7582\n",
      "Epoch 47/100\n",
      "2082/2082 - 62s - loss: 0.5360 - accuracy: 0.7573\n",
      "Epoch 48/100\n",
      "2082/2082 - 61s - loss: 0.5344 - accuracy: 0.7589\n",
      "Epoch 49/100\n",
      "2082/2082 - 61s - loss: 0.5334 - accuracy: 0.7592\n",
      "Epoch 50/100\n",
      "2082/2082 - 62s - loss: 0.5329 - accuracy: 0.7599\n",
      "Epoch 51/100\n",
      "2082/2082 - 61s - loss: 0.5322 - accuracy: 0.7594\n",
      "Epoch 52/100\n",
      "2082/2082 - 61s - loss: 0.5321 - accuracy: 0.7599\n",
      "Epoch 53/100\n",
      "2082/2082 - 61s - loss: 0.5319 - accuracy: 0.7588\n",
      "Epoch 54/100\n",
      "2082/2082 - 61s - loss: 0.5308 - accuracy: 0.7611\n",
      "Epoch 55/100\n",
      "2082/2082 - 61s - loss: 0.5305 - accuracy: 0.7604\n",
      "Epoch 56/100\n",
      "2082/2082 - 61s - loss: 0.5301 - accuracy: 0.7604\n",
      "Epoch 57/100\n",
      "2082/2082 - 61s - loss: 0.5292 - accuracy: 0.7604\n",
      "Epoch 58/100\n",
      "2082/2082 - 62s - loss: 0.5285 - accuracy: 0.7605\n",
      "Epoch 59/100\n",
      "2082/2082 - 62s - loss: 0.5288 - accuracy: 0.7610\n",
      "Epoch 60/100\n",
      "2082/2082 - 62s - loss: 0.5288 - accuracy: 0.7604\n",
      "Epoch 61/100\n",
      "2082/2082 - 62s - loss: 0.5277 - accuracy: 0.7604\n",
      "Epoch 62/100\n",
      "2082/2082 - 62s - loss: 0.5268 - accuracy: 0.7616\n",
      "Epoch 63/100\n",
      "2082/2082 - 62s - loss: 0.5271 - accuracy: 0.7603\n",
      "Epoch 64/100\n",
      "2082/2082 - 62s - loss: 0.5273 - accuracy: 0.7609\n",
      "Epoch 65/100\n",
      "2082/2082 - 62s - loss: 0.5263 - accuracy: 0.7605\n",
      "Epoch 66/100\n",
      "2082/2082 - 62s - loss: 0.5266 - accuracy: 0.7619\n",
      "Epoch 67/100\n",
      "2082/2082 - 62s - loss: 0.5258 - accuracy: 0.7609\n",
      "Epoch 68/100\n",
      "2082/2082 - 62s - loss: 0.5254 - accuracy: 0.7618\n",
      "Epoch 69/100\n",
      "2082/2082 - 62s - loss: 0.5246 - accuracy: 0.7617\n",
      "Epoch 70/100\n",
      "2082/2082 - 62s - loss: 0.5246 - accuracy: 0.7638\n",
      "Epoch 71/100\n",
      "2082/2082 - 62s - loss: 0.5228 - accuracy: 0.7633\n",
      "Epoch 72/100\n",
      "2082/2082 - 62s - loss: 0.5231 - accuracy: 0.7624\n",
      "Epoch 73/100\n",
      "2082/2082 - 63s - loss: 0.5235 - accuracy: 0.7621\n",
      "Epoch 74/100\n",
      "2082/2082 - 63s - loss: 0.5233 - accuracy: 0.7623\n",
      "Epoch 75/100\n",
      "2082/2082 - 63s - loss: 0.5223 - accuracy: 0.7625\n",
      "Epoch 76/100\n",
      "2082/2082 - 63s - loss: 0.5220 - accuracy: 0.7635\n",
      "Epoch 77/100\n",
      "2082/2082 - 63s - loss: 0.5215 - accuracy: 0.7617\n",
      "Epoch 78/100\n",
      "2082/2082 - 63s - loss: 0.5214 - accuracy: 0.7626\n",
      "Epoch 79/100\n",
      "2082/2082 - 63s - loss: 0.5202 - accuracy: 0.7620\n",
      "Epoch 80/100\n",
      "2082/2082 - 63s - loss: 0.5198 - accuracy: 0.7642\n",
      "Epoch 81/100\n",
      "2082/2082 - 63s - loss: 0.5203 - accuracy: 0.7632\n",
      "Epoch 82/100\n",
      "2082/2082 - 63s - loss: 0.5198 - accuracy: 0.7628\n",
      "Epoch 83/100\n",
      "2082/2082 - 63s - loss: 0.5196 - accuracy: 0.7625\n",
      "Epoch 84/100\n",
      "2082/2082 - 63s - loss: 0.5184 - accuracy: 0.7644\n",
      "Epoch 85/100\n",
      "2082/2082 - 63s - loss: 0.5192 - accuracy: 0.7637\n",
      "Epoch 86/100\n",
      "2082/2082 - 63s - loss: 0.5182 - accuracy: 0.7628\n",
      "Epoch 87/100\n",
      "2082/2082 - 63s - loss: 0.5177 - accuracy: 0.7639\n",
      "Epoch 88/100\n",
      "2082/2082 - 63s - loss: 0.5176 - accuracy: 0.7645\n",
      "Epoch 89/100\n",
      "2082/2082 - 63s - loss: 0.5175 - accuracy: 0.7651\n",
      "Epoch 90/100\n",
      "2082/2082 - 63s - loss: 0.5176 - accuracy: 0.7633\n",
      "Epoch 91/100\n",
      "2082/2082 - 63s - loss: 0.5167 - accuracy: 0.7664\n",
      "Epoch 92/100\n",
      "2082/2082 - 63s - loss: 0.5169 - accuracy: 0.7643\n",
      "Epoch 93/100\n",
      "2082/2082 - 63s - loss: 0.5161 - accuracy: 0.7655\n",
      "Epoch 94/100\n",
      "2082/2082 - 63s - loss: 0.5159 - accuracy: 0.7656\n",
      "Epoch 95/100\n",
      "2082/2082 - 63s - loss: 0.5157 - accuracy: 0.7657\n",
      "Epoch 96/100\n",
      "2082/2082 - 63s - loss: 0.5152 - accuracy: 0.7641\n",
      "Epoch 97/100\n",
      "2082/2082 - 63s - loss: 0.5148 - accuracy: 0.7658\n",
      "Epoch 98/100\n",
      "2082/2082 - 63s - loss: 0.5145 - accuracy: 0.7660\n",
      "Epoch 99/100\n",
      "2082/2082 - 63s - loss: 0.5138 - accuracy: 0.7661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100\n",
      "2082/2082 - 63s - loss: 0.5144 - accuracy: 0.7655\n",
      "417/417 - 3s - loss: 0.6235 - accuracy: 0.7327\n",
      "Epoch 1/100\n",
      "417/417 - 13s - loss: 0.8394 - accuracy: 0.7177\n",
      "Epoch 2/100\n",
      "417/417 - 13s - loss: 0.7536 - accuracy: 0.7389\n",
      "Epoch 3/100\n",
      "417/417 - 13s - loss: 0.7533 - accuracy: 0.7389\n",
      "Epoch 4/100\n",
      "417/417 - 13s - loss: 0.7524 - accuracy: 0.7389\n",
      "Epoch 5/100\n",
      "417/417 - 13s - loss: 0.7512 - accuracy: 0.7389\n",
      "Epoch 6/100\n",
      "417/417 - 13s - loss: 0.7494 - accuracy: 0.7389\n",
      "Epoch 7/100\n",
      "417/417 - 13s - loss: 0.7479 - accuracy: 0.7389\n",
      "Epoch 8/100\n",
      "417/417 - 13s - loss: 0.7451 - accuracy: 0.7389\n",
      "Epoch 9/100\n",
      "417/417 - 13s - loss: 0.7412 - accuracy: 0.7389\n",
      "Epoch 10/100\n",
      "417/417 - 13s - loss: 0.7375 - accuracy: 0.7389\n",
      "Epoch 11/100\n",
      "417/417 - 13s - loss: 0.7324 - accuracy: 0.7389\n",
      "Epoch 12/100\n",
      "417/417 - 13s - loss: 0.7264 - accuracy: 0.7389\n",
      "Epoch 13/100\n",
      "417/417 - 13s - loss: 0.7208 - accuracy: 0.7389\n",
      "Epoch 14/100\n",
      "417/417 - 13s - loss: 0.7153 - accuracy: 0.7389\n",
      "Epoch 15/100\n",
      "417/417 - 13s - loss: 0.7077 - accuracy: 0.7389\n",
      "Epoch 16/100\n",
      "417/417 - 13s - loss: 0.7009 - accuracy: 0.7389\n",
      "Epoch 17/100\n",
      "417/417 - 13s - loss: 0.6939 - accuracy: 0.7389\n",
      "Epoch 18/100\n",
      "417/417 - 13s - loss: 0.6865 - accuracy: 0.7389\n",
      "Epoch 19/100\n",
      "417/417 - 13s - loss: 0.6789 - accuracy: 0.7389\n",
      "Epoch 20/100\n",
      "417/417 - 13s - loss: 0.6724 - accuracy: 0.7388\n",
      "Epoch 21/100\n",
      "417/417 - 13s - loss: 0.6690 - accuracy: 0.7390\n",
      "Epoch 22/100\n",
      "417/417 - 13s - loss: 0.6632 - accuracy: 0.7395\n",
      "Epoch 23/100\n",
      "417/417 - 12s - loss: 0.6597 - accuracy: 0.7381\n",
      "Epoch 24/100\n",
      "417/417 - 12s - loss: 0.6590 - accuracy: 0.7386\n",
      "Epoch 25/100\n",
      "417/417 - 12s - loss: 0.6536 - accuracy: 0.7381\n",
      "Epoch 26/100\n",
      "417/417 - 12s - loss: 0.6508 - accuracy: 0.7384\n",
      "Epoch 27/100\n",
      "417/417 - 12s - loss: 0.6496 - accuracy: 0.7388\n",
      "Epoch 28/100\n",
      "417/417 - 13s - loss: 0.6467 - accuracy: 0.7390\n",
      "Epoch 29/100\n",
      "417/417 - 12s - loss: 0.6469 - accuracy: 0.7389\n",
      "Epoch 30/100\n",
      "417/417 - 12s - loss: 0.6421 - accuracy: 0.7400\n",
      "Epoch 31/100\n",
      "417/417 - 13s - loss: 0.6404 - accuracy: 0.7393\n",
      "Epoch 32/100\n",
      "417/417 - 12s - loss: 0.6410 - accuracy: 0.7396\n",
      "Epoch 33/100\n",
      "417/417 - 13s - loss: 0.6367 - accuracy: 0.7402\n",
      "Epoch 34/100\n",
      "417/417 - 13s - loss: 0.6350 - accuracy: 0.7421\n",
      "Epoch 35/100\n",
      "417/417 - 13s - loss: 0.6314 - accuracy: 0.7424\n",
      "Epoch 36/100\n",
      "417/417 - 13s - loss: 0.6313 - accuracy: 0.7424\n",
      "Epoch 37/100\n",
      "417/417 - 13s - loss: 0.6297 - accuracy: 0.7441\n",
      "Epoch 38/100\n",
      "417/417 - 13s - loss: 0.6282 - accuracy: 0.7428\n",
      "Epoch 39/100\n",
      "417/417 - 13s - loss: 0.6267 - accuracy: 0.7435\n",
      "Epoch 40/100\n",
      "417/417 - 13s - loss: 0.6256 - accuracy: 0.7451\n",
      "Epoch 41/100\n",
      "417/417 - 13s - loss: 0.6227 - accuracy: 0.7462\n",
      "Epoch 42/100\n",
      "417/417 - 13s - loss: 0.6212 - accuracy: 0.7440\n",
      "Epoch 43/100\n",
      "417/417 - 13s - loss: 0.6204 - accuracy: 0.7464\n",
      "Epoch 44/100\n",
      "417/417 - 13s - loss: 0.6178 - accuracy: 0.7453\n",
      "Epoch 45/100\n",
      "417/417 - 13s - loss: 0.6177 - accuracy: 0.7453\n",
      "Epoch 46/100\n",
      "417/417 - 13s - loss: 0.6145 - accuracy: 0.7468\n",
      "Epoch 47/100\n",
      "417/417 - 13s - loss: 0.6153 - accuracy: 0.7457\n",
      "Epoch 48/100\n",
      "417/417 - 13s - loss: 0.6123 - accuracy: 0.7462\n",
      "Epoch 49/100\n",
      "417/417 - 13s - loss: 0.6125 - accuracy: 0.7471\n",
      "Epoch 50/100\n",
      "417/417 - 13s - loss: 0.6110 - accuracy: 0.7470\n",
      "Epoch 51/100\n",
      "417/417 - 13s - loss: 0.6100 - accuracy: 0.7463\n",
      "Epoch 52/100\n",
      "417/417 - 13s - loss: 0.6077 - accuracy: 0.7464\n",
      "Epoch 53/100\n",
      "417/417 - 13s - loss: 0.6065 - accuracy: 0.7505\n",
      "Epoch 54/100\n",
      "417/417 - 13s - loss: 0.6056 - accuracy: 0.7474\n",
      "Epoch 55/100\n",
      "417/417 - 13s - loss: 0.6042 - accuracy: 0.7517\n",
      "Epoch 56/100\n",
      "417/417 - 13s - loss: 0.6048 - accuracy: 0.7471\n",
      "Epoch 57/100\n",
      "417/417 - 13s - loss: 0.6046 - accuracy: 0.7492\n",
      "Epoch 58/100\n",
      "417/417 - 13s - loss: 0.6022 - accuracy: 0.7505\n",
      "Epoch 59/100\n",
      "417/417 - 13s - loss: 0.6037 - accuracy: 0.7493\n",
      "Epoch 60/100\n",
      "417/417 - 13s - loss: 0.6001 - accuracy: 0.7515\n",
      "Epoch 61/100\n",
      "417/417 - 13s - loss: 0.6015 - accuracy: 0.7495\n",
      "Epoch 62/100\n",
      "417/417 - 13s - loss: 0.5997 - accuracy: 0.7502\n",
      "Epoch 63/100\n",
      "417/417 - 13s - loss: 0.5989 - accuracy: 0.7535\n",
      "Epoch 64/100\n",
      "417/417 - 13s - loss: 0.5995 - accuracy: 0.7496\n",
      "Epoch 65/100\n",
      "417/417 - 13s - loss: 0.5985 - accuracy: 0.7533\n",
      "Epoch 66/100\n",
      "417/417 - 13s - loss: 0.5971 - accuracy: 0.7525\n",
      "Epoch 67/100\n",
      "417/417 - 13s - loss: 0.5956 - accuracy: 0.7524\n",
      "Epoch 68/100\n",
      "417/417 - 13s - loss: 0.5961 - accuracy: 0.7523\n",
      "Epoch 69/100\n",
      "417/417 - 13s - loss: 0.5938 - accuracy: 0.7550\n",
      "Epoch 70/100\n",
      "417/417 - 13s - loss: 0.5938 - accuracy: 0.7529\n",
      "Epoch 71/100\n",
      "417/417 - 13s - loss: 0.5932 - accuracy: 0.7549\n",
      "Epoch 72/100\n",
      "417/417 - 13s - loss: 0.5937 - accuracy: 0.7532\n",
      "Epoch 73/100\n",
      "417/417 - 13s - loss: 0.5944 - accuracy: 0.7540\n",
      "Epoch 74/100\n",
      "417/417 - 13s - loss: 0.5932 - accuracy: 0.7508\n",
      "Epoch 75/100\n",
      "417/417 - 13s - loss: 0.5929 - accuracy: 0.7521\n",
      "Epoch 76/100\n",
      "417/417 - 13s - loss: 0.5905 - accuracy: 0.7539\n",
      "Epoch 77/100\n",
      "417/417 - 13s - loss: 0.5899 - accuracy: 0.7555\n",
      "Epoch 78/100\n",
      "417/417 - 13s - loss: 0.5897 - accuracy: 0.7537\n",
      "Epoch 79/100\n",
      "417/417 - 13s - loss: 0.5907 - accuracy: 0.7517\n",
      "Epoch 80/100\n",
      "417/417 - 13s - loss: 0.5884 - accuracy: 0.7524\n",
      "Epoch 81/100\n",
      "417/417 - 13s - loss: 0.5888 - accuracy: 0.7515\n",
      "Epoch 82/100\n",
      "417/417 - 13s - loss: 0.5901 - accuracy: 0.7545\n",
      "Epoch 83/100\n",
      "417/417 - 13s - loss: 0.5862 - accuracy: 0.7551\n",
      "Epoch 84/100\n",
      "417/417 - 13s - loss: 0.5862 - accuracy: 0.7528\n",
      "Epoch 85/100\n",
      "417/417 - 13s - loss: 0.5838 - accuracy: 0.7566\n",
      "Epoch 86/100\n",
      "417/417 - 13s - loss: 0.5846 - accuracy: 0.7550\n",
      "Epoch 87/100\n",
      "417/417 - 13s - loss: 0.5839 - accuracy: 0.7590\n",
      "Epoch 88/100\n",
      "417/417 - 13s - loss: 0.5815 - accuracy: 0.7579\n",
      "Epoch 89/100\n",
      "417/417 - 13s - loss: 0.5830 - accuracy: 0.7550\n",
      "Epoch 90/100\n",
      "417/417 - 13s - loss: 0.5802 - accuracy: 0.7545\n",
      "Epoch 91/100\n",
      "417/417 - 13s - loss: 0.5805 - accuracy: 0.7582\n",
      "Epoch 92/100\n",
      "417/417 - 13s - loss: 0.5818 - accuracy: 0.7553\n",
      "Epoch 93/100\n",
      "417/417 - 13s - loss: 0.5806 - accuracy: 0.7552\n",
      "Epoch 94/100\n",
      "417/417 - 13s - loss: 0.5806 - accuracy: 0.7541\n",
      "Epoch 95/100\n",
      "417/417 - 13s - loss: 0.5805 - accuracy: 0.7549\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00095: early stopping\n",
      "417/417 - 3s - loss: 0.6001 - accuracy: 0.7500\n",
      "Epoch 1/100\n",
      "833/833 - 25s - loss: 0.7976 - accuracy: 0.7358\n",
      "Epoch 2/100\n",
      "833/833 - 25s - loss: 0.7524 - accuracy: 0.7396\n",
      "Epoch 3/100\n",
      "833/833 - 25s - loss: 0.7496 - accuracy: 0.7396\n",
      "Epoch 4/100\n",
      "833/833 - 25s - loss: 0.7456 - accuracy: 0.7396\n",
      "Epoch 5/100\n",
      "833/833 - 25s - loss: 0.7373 - accuracy: 0.7396\n",
      "Epoch 6/100\n",
      "833/833 - 25s - loss: 0.7216 - accuracy: 0.7396\n",
      "Epoch 7/100\n",
      "833/833 - 25s - loss: 0.6992 - accuracy: 0.7396\n",
      "Epoch 8/100\n",
      "833/833 - 25s - loss: 0.6756 - accuracy: 0.7396\n",
      "Epoch 9/100\n",
      "833/833 - 25s - loss: 0.6602 - accuracy: 0.7388\n",
      "Epoch 10/100\n",
      "833/833 - 25s - loss: 0.6507 - accuracy: 0.7390\n",
      "Epoch 11/100\n",
      "833/833 - 25s - loss: 0.6440 - accuracy: 0.7393\n",
      "Epoch 12/100\n",
      "833/833 - 25s - loss: 0.6409 - accuracy: 0.7397\n",
      "Epoch 13/100\n",
      "833/833 - 25s - loss: 0.6352 - accuracy: 0.7408\n",
      "Epoch 14/100\n",
      "833/833 - 25s - loss: 0.6325 - accuracy: 0.7411\n",
      "Epoch 15/100\n",
      "833/833 - 25s - loss: 0.6290 - accuracy: 0.7422\n",
      "Epoch 16/100\n",
      "833/833 - 25s - loss: 0.6274 - accuracy: 0.7418\n",
      "Epoch 17/100\n",
      "833/833 - 25s - loss: 0.6243 - accuracy: 0.7427\n",
      "Epoch 18/100\n",
      "833/833 - 25s - loss: 0.6229 - accuracy: 0.7441\n",
      "Epoch 19/100\n",
      "833/833 - 25s - loss: 0.6206 - accuracy: 0.7455\n",
      "Epoch 20/100\n",
      "833/833 - 25s - loss: 0.6187 - accuracy: 0.7450\n",
      "Epoch 21/100\n",
      "833/833 - 25s - loss: 0.6185 - accuracy: 0.7457\n",
      "Epoch 22/100\n",
      "833/833 - 25s - loss: 0.6143 - accuracy: 0.7484\n",
      "Epoch 23/100\n",
      "833/833 - 25s - loss: 0.6141 - accuracy: 0.7471\n",
      "Epoch 24/100\n",
      "833/833 - 25s - loss: 0.6128 - accuracy: 0.7485\n",
      "Epoch 25/100\n",
      "833/833 - 25s - loss: 0.6109 - accuracy: 0.7481\n",
      "Epoch 26/100\n",
      "833/833 - 25s - loss: 0.6100 - accuracy: 0.7472\n",
      "Epoch 27/100\n",
      "833/833 - 25s - loss: 0.6086 - accuracy: 0.7476\n",
      "Epoch 28/100\n",
      "833/833 - 25s - loss: 0.6055 - accuracy: 0.7481\n",
      "Epoch 29/100\n",
      "833/833 - 25s - loss: 0.6033 - accuracy: 0.7476\n",
      "Epoch 30/100\n",
      "833/833 - 25s - loss: 0.6018 - accuracy: 0.7486\n",
      "Epoch 31/100\n",
      "833/833 - 25s - loss: 0.5987 - accuracy: 0.7498\n",
      "Epoch 32/100\n",
      "833/833 - 25s - loss: 0.5948 - accuracy: 0.7489\n",
      "Epoch 33/100\n",
      "833/833 - 25s - loss: 0.5929 - accuracy: 0.7512\n",
      "Epoch 34/100\n",
      "833/833 - 25s - loss: 0.5896 - accuracy: 0.7506\n",
      "Epoch 35/100\n",
      "833/833 - 25s - loss: 0.5862 - accuracy: 0.7495\n",
      "Epoch 36/100\n",
      "833/833 - 25s - loss: 0.5852 - accuracy: 0.7512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100\n",
      "833/833 - 25s - loss: 0.5838 - accuracy: 0.7507\n",
      "Epoch 38/100\n",
      "833/833 - 25s - loss: 0.5828 - accuracy: 0.7504\n",
      "Epoch 39/100\n",
      "833/833 - 25s - loss: 0.5811 - accuracy: 0.7512\n",
      "Epoch 40/100\n",
      "833/833 - 25s - loss: 0.5787 - accuracy: 0.7523\n",
      "Epoch 41/100\n",
      "833/833 - 25s - loss: 0.5765 - accuracy: 0.7531\n",
      "Epoch 42/100\n",
      "833/833 - 25s - loss: 0.5757 - accuracy: 0.7513\n",
      "Epoch 43/100\n",
      "833/833 - 25s - loss: 0.5730 - accuracy: 0.7542\n",
      "Epoch 44/100\n",
      "833/833 - 25s - loss: 0.5731 - accuracy: 0.7538\n",
      "Epoch 45/100\n",
      "833/833 - 25s - loss: 0.5726 - accuracy: 0.7530\n",
      "Epoch 46/100\n",
      "833/833 - 25s - loss: 0.5696 - accuracy: 0.7548\n",
      "Epoch 47/100\n",
      "833/833 - 25s - loss: 0.5700 - accuracy: 0.7532\n",
      "Epoch 48/100\n",
      "833/833 - 25s - loss: 0.5693 - accuracy: 0.7552\n",
      "Epoch 49/100\n",
      "833/833 - 25s - loss: 0.5663 - accuracy: 0.7550\n",
      "Epoch 50/100\n",
      "833/833 - 25s - loss: 0.5656 - accuracy: 0.7573\n",
      "Epoch 51/100\n",
      "833/833 - 25s - loss: 0.5656 - accuracy: 0.7581\n",
      "Epoch 52/100\n",
      "833/833 - 25s - loss: 0.5637 - accuracy: 0.7569\n",
      "Epoch 53/100\n",
      "833/833 - 25s - loss: 0.5634 - accuracy: 0.7569\n",
      "Epoch 54/100\n",
      "833/833 - 25s - loss: 0.5617 - accuracy: 0.7602\n",
      "Epoch 55/100\n",
      "833/833 - 25s - loss: 0.5605 - accuracy: 0.7587\n",
      "Epoch 56/100\n",
      "833/833 - 25s - loss: 0.5608 - accuracy: 0.7591\n",
      "Epoch 57/100\n",
      "833/833 - 25s - loss: 0.5601 - accuracy: 0.7578\n",
      "Epoch 58/100\n",
      "833/833 - 25s - loss: 0.5591 - accuracy: 0.7578\n",
      "Epoch 59/100\n",
      "833/833 - 25s - loss: 0.5593 - accuracy: 0.7577\n",
      "Epoch 60/100\n",
      "833/833 - 25s - loss: 0.5585 - accuracy: 0.7581\n",
      "Epoch 61/100\n",
      "833/833 - 25s - loss: 0.5567 - accuracy: 0.7582\n",
      "Epoch 62/100\n",
      "833/833 - 25s - loss: 0.5569 - accuracy: 0.7619\n",
      "Epoch 63/100\n",
      "833/833 - 25s - loss: 0.5561 - accuracy: 0.7593\n",
      "Epoch 64/100\n",
      "833/833 - 25s - loss: 0.5552 - accuracy: 0.7602\n",
      "Epoch 65/100\n",
      "833/833 - 25s - loss: 0.5541 - accuracy: 0.7609\n",
      "Epoch 66/100\n",
      "833/833 - 25s - loss: 0.5551 - accuracy: 0.7611\n",
      "Epoch 67/100\n",
      "833/833 - 25s - loss: 0.5531 - accuracy: 0.7598\n",
      "Epoch 68/100\n",
      "833/833 - 25s - loss: 0.5530 - accuracy: 0.7612\n",
      "Epoch 69/100\n",
      "833/833 - 25s - loss: 0.5514 - accuracy: 0.7633\n",
      "Epoch 70/100\n",
      "833/833 - 25s - loss: 0.5504 - accuracy: 0.7629\n",
      "Epoch 71/100\n",
      "833/833 - 25s - loss: 0.5499 - accuracy: 0.7620\n",
      "Epoch 72/100\n",
      "833/833 - 25s - loss: 0.5506 - accuracy: 0.7585\n",
      "Epoch 73/100\n",
      "833/833 - 25s - loss: 0.5478 - accuracy: 0.7631\n",
      "Epoch 74/100\n",
      "833/833 - 25s - loss: 0.5494 - accuracy: 0.7638\n",
      "Epoch 75/100\n",
      "833/833 - 25s - loss: 0.5487 - accuracy: 0.7634\n",
      "Epoch 76/100\n",
      "833/833 - 25s - loss: 0.5480 - accuracy: 0.7634\n",
      "Epoch 77/100\n",
      "833/833 - 25s - loss: 0.5479 - accuracy: 0.7630\n",
      "Epoch 78/100\n",
      "833/833 - 25s - loss: 0.5462 - accuracy: 0.7641\n",
      "Epoch 79/100\n",
      "833/833 - 25s - loss: 0.5458 - accuracy: 0.7649\n",
      "Epoch 80/100\n",
      "833/833 - 25s - loss: 0.5456 - accuracy: 0.7638\n",
      "Epoch 81/100\n",
      "833/833 - 25s - loss: 0.5457 - accuracy: 0.7655\n",
      "Epoch 82/100\n",
      "833/833 - 25s - loss: 0.5450 - accuracy: 0.7640\n",
      "Epoch 83/100\n",
      "833/833 - 25s - loss: 0.5447 - accuracy: 0.7635\n",
      "Epoch 84/100\n",
      "833/833 - 25s - loss: 0.5443 - accuracy: 0.7643\n",
      "Epoch 85/100\n",
      "833/833 - 25s - loss: 0.5441 - accuracy: 0.7628\n",
      "Epoch 86/100\n",
      "833/833 - 25s - loss: 0.5444 - accuracy: 0.7670\n",
      "Epoch 87/100\n",
      "833/833 - 25s - loss: 0.5428 - accuracy: 0.7651\n",
      "Epoch 88/100\n",
      "833/833 - 25s - loss: 0.5419 - accuracy: 0.7652\n",
      "Epoch 89/100\n",
      "833/833 - 25s - loss: 0.5430 - accuracy: 0.7644\n",
      "Epoch 90/100\n",
      "833/833 - 25s - loss: 0.5414 - accuracy: 0.7668\n",
      "Epoch 91/100\n",
      "833/833 - 25s - loss: 0.5411 - accuracy: 0.7647\n",
      "Epoch 92/100\n",
      "833/833 - 25s - loss: 0.5400 - accuracy: 0.7658\n",
      "Epoch 93/100\n",
      "833/833 - 25s - loss: 0.5400 - accuracy: 0.7649\n",
      "Epoch 94/100\n",
      "833/833 - 25s - loss: 0.5397 - accuracy: 0.7656\n",
      "Epoch 95/100\n",
      "833/833 - 25s - loss: 0.5382 - accuracy: 0.7662\n",
      "Epoch 96/100\n",
      "833/833 - 25s - loss: 0.5394 - accuracy: 0.7669\n",
      "Epoch 97/100\n",
      "833/833 - 25s - loss: 0.5390 - accuracy: 0.7663\n",
      "Epoch 98/100\n",
      "833/833 - 26s - loss: 0.5396 - accuracy: 0.7656\n",
      "Epoch 99/100\n",
      "833/833 - 25s - loss: 0.5383 - accuracy: 0.7667\n",
      "Epoch 100/100\n",
      "833/833 - 25s - loss: 0.5378 - accuracy: 0.7664\n",
      "417/417 - 3s - loss: 0.5372 - accuracy: 0.7616\n",
      "Epoch 1/100\n",
      "1249/1249 - 39s - loss: 0.7770 - accuracy: 0.7386\n",
      "Epoch 2/100\n",
      "1249/1249 - 38s - loss: 0.7439 - accuracy: 0.7441\n",
      "Epoch 3/100\n",
      "1249/1249 - 38s - loss: 0.7405 - accuracy: 0.7441\n",
      "Epoch 4/100\n",
      "1249/1249 - 38s - loss: 0.7327 - accuracy: 0.7441\n",
      "Epoch 5/100\n",
      "1249/1249 - 38s - loss: 0.7133 - accuracy: 0.7441\n",
      "Epoch 6/100\n",
      "1249/1249 - 37s - loss: 0.6822 - accuracy: 0.7441\n",
      "Epoch 7/100\n",
      "1249/1249 - 37s - loss: 0.6542 - accuracy: 0.7437\n",
      "Epoch 8/100\n",
      "1249/1249 - 37s - loss: 0.6403 - accuracy: 0.7438\n",
      "Epoch 9/100\n",
      "1249/1249 - 37s - loss: 0.6324 - accuracy: 0.7440\n",
      "Epoch 10/100\n",
      "1249/1249 - 37s - loss: 0.6277 - accuracy: 0.7451\n",
      "Epoch 11/100\n",
      "1249/1249 - 37s - loss: 0.6233 - accuracy: 0.7452\n",
      "Epoch 12/100\n",
      "1249/1249 - 37s - loss: 0.6218 - accuracy: 0.7451\n",
      "Epoch 13/100\n",
      "1249/1249 - 37s - loss: 0.6181 - accuracy: 0.7462\n",
      "Epoch 14/100\n",
      "1249/1249 - 38s - loss: 0.6158 - accuracy: 0.7457\n",
      "Epoch 15/100\n",
      "1249/1249 - 38s - loss: 0.6141 - accuracy: 0.7461\n",
      "Epoch 16/100\n",
      "1249/1249 - 38s - loss: 0.6113 - accuracy: 0.7458\n",
      "Epoch 17/100\n",
      "1249/1249 - 38s - loss: 0.6092 - accuracy: 0.7466\n",
      "Epoch 18/100\n",
      "1249/1249 - 38s - loss: 0.6063 - accuracy: 0.7468\n",
      "Epoch 19/100\n",
      "1249/1249 - 38s - loss: 0.6038 - accuracy: 0.7474\n",
      "Epoch 20/100\n",
      "1249/1249 - 38s - loss: 0.6011 - accuracy: 0.7471\n",
      "Epoch 21/100\n",
      "1249/1249 - 38s - loss: 0.5986 - accuracy: 0.7473\n",
      "Epoch 22/100\n",
      "1249/1249 - 38s - loss: 0.5960 - accuracy: 0.7485\n",
      "Epoch 23/100\n",
      "1249/1249 - 38s - loss: 0.5930 - accuracy: 0.7499\n",
      "Epoch 24/100\n",
      "1249/1249 - 38s - loss: 0.5907 - accuracy: 0.7506\n",
      "Epoch 25/100\n",
      "1249/1249 - 38s - loss: 0.5873 - accuracy: 0.7511\n",
      "Epoch 26/100\n",
      "1249/1249 - 38s - loss: 0.5850 - accuracy: 0.7511\n",
      "Epoch 27/100\n",
      "1249/1249 - 38s - loss: 0.5834 - accuracy: 0.7520\n",
      "Epoch 28/100\n",
      "1249/1249 - 38s - loss: 0.5814 - accuracy: 0.7535\n",
      "Epoch 29/100\n",
      "1249/1249 - 38s - loss: 0.5786 - accuracy: 0.7546\n",
      "Epoch 30/100\n",
      "1249/1249 - 38s - loss: 0.5769 - accuracy: 0.7559\n",
      "Epoch 31/100\n",
      "1249/1249 - 38s - loss: 0.5750 - accuracy: 0.7565\n",
      "Epoch 32/100\n",
      "1249/1249 - 38s - loss: 0.5714 - accuracy: 0.7567\n",
      "Epoch 33/100\n",
      "1249/1249 - 38s - loss: 0.5719 - accuracy: 0.7564\n",
      "Epoch 34/100\n",
      "1249/1249 - 38s - loss: 0.5697 - accuracy: 0.7580\n",
      "Epoch 35/100\n",
      "1249/1249 - 38s - loss: 0.5684 - accuracy: 0.7600\n",
      "Epoch 36/100\n",
      "1249/1249 - 38s - loss: 0.5661 - accuracy: 0.7601\n",
      "Epoch 37/100\n",
      "1249/1249 - 38s - loss: 0.5663 - accuracy: 0.7586\n",
      "Epoch 38/100\n",
      "1249/1249 - 38s - loss: 0.5635 - accuracy: 0.7609\n",
      "Epoch 39/100\n",
      "1249/1249 - 38s - loss: 0.5615 - accuracy: 0.7626\n",
      "Epoch 40/100\n",
      "1249/1249 - 38s - loss: 0.5608 - accuracy: 0.7626\n",
      "Epoch 41/100\n",
      "1249/1249 - 38s - loss: 0.5591 - accuracy: 0.7622\n",
      "Epoch 42/100\n",
      "1249/1249 - 38s - loss: 0.5570 - accuracy: 0.7631\n",
      "Epoch 43/100\n",
      "1249/1249 - 38s - loss: 0.5550 - accuracy: 0.7616\n",
      "Epoch 44/100\n",
      "1249/1249 - 38s - loss: 0.5548 - accuracy: 0.7642\n",
      "Epoch 45/100\n",
      "1249/1249 - 38s - loss: 0.5523 - accuracy: 0.7620\n",
      "Epoch 46/100\n",
      "1249/1249 - 37s - loss: 0.5494 - accuracy: 0.7645\n",
      "Epoch 47/100\n",
      "1249/1249 - 38s - loss: 0.5476 - accuracy: 0.7633\n",
      "Epoch 48/100\n",
      "1249/1249 - 38s - loss: 0.5476 - accuracy: 0.7634\n",
      "Epoch 49/100\n",
      "1249/1249 - 38s - loss: 0.5459 - accuracy: 0.7643\n",
      "Epoch 50/100\n",
      "1249/1249 - 38s - loss: 0.5451 - accuracy: 0.7656\n",
      "Epoch 51/100\n",
      "1249/1249 - 38s - loss: 0.5442 - accuracy: 0.7637\n",
      "Epoch 52/100\n",
      "1249/1249 - 38s - loss: 0.5427 - accuracy: 0.7643\n",
      "Epoch 53/100\n",
      "1249/1249 - 38s - loss: 0.5423 - accuracy: 0.7637\n",
      "Epoch 54/100\n",
      "1249/1249 - 38s - loss: 0.5417 - accuracy: 0.7640\n",
      "Epoch 55/100\n",
      "1249/1249 - 38s - loss: 0.5408 - accuracy: 0.7643\n",
      "Epoch 56/100\n",
      "1249/1249 - 38s - loss: 0.5403 - accuracy: 0.7651\n",
      "Epoch 57/100\n",
      "1249/1249 - 38s - loss: 0.5382 - accuracy: 0.7663\n",
      "Epoch 58/100\n",
      "1249/1249 - 38s - loss: 0.5392 - accuracy: 0.7647\n",
      "Epoch 59/100\n",
      "1249/1249 - 38s - loss: 0.5376 - accuracy: 0.7659\n",
      "Epoch 60/100\n",
      "1249/1249 - 38s - loss: 0.5360 - accuracy: 0.7663\n",
      "Epoch 61/100\n",
      "1249/1249 - 38s - loss: 0.5363 - accuracy: 0.7650\n",
      "Epoch 62/100\n",
      "1249/1249 - 38s - loss: 0.5361 - accuracy: 0.7657\n",
      "Epoch 63/100\n",
      "1249/1249 - 38s - loss: 0.5355 - accuracy: 0.7671\n",
      "Epoch 64/100\n",
      "1249/1249 - 38s - loss: 0.5351 - accuracy: 0.7650\n",
      "Epoch 65/100\n",
      "1249/1249 - 38s - loss: 0.5327 - accuracy: 0.7674\n",
      "Epoch 66/100\n",
      "1249/1249 - 38s - loss: 0.5335 - accuracy: 0.7674\n",
      "Epoch 67/100\n",
      "1249/1249 - 38s - loss: 0.5330 - accuracy: 0.7656\n",
      "Epoch 68/100\n",
      "1249/1249 - 38s - loss: 0.5325 - accuracy: 0.7670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/100\n",
      "1249/1249 - 38s - loss: 0.5309 - accuracy: 0.7664\n",
      "Epoch 70/100\n",
      "1249/1249 - 38s - loss: 0.5304 - accuracy: 0.7672\n",
      "Epoch 71/100\n",
      "1249/1249 - 38s - loss: 0.5292 - accuracy: 0.7684\n",
      "Epoch 72/100\n",
      "1249/1249 - 38s - loss: 0.5290 - accuracy: 0.7685\n",
      "Epoch 73/100\n",
      "1249/1249 - 38s - loss: 0.5293 - accuracy: 0.7681\n",
      "Epoch 74/100\n",
      "1249/1249 - 38s - loss: 0.5290 - accuracy: 0.7694\n",
      "Epoch 75/100\n",
      "1249/1249 - 38s - loss: 0.5287 - accuracy: 0.7688\n",
      "Epoch 76/100\n",
      "1249/1249 - 38s - loss: 0.5278 - accuracy: 0.7703\n",
      "Epoch 77/100\n",
      "1249/1249 - 38s - loss: 0.5272 - accuracy: 0.7690\n",
      "Epoch 78/100\n",
      "1249/1249 - 38s - loss: 0.5271 - accuracy: 0.7689\n",
      "Epoch 79/100\n",
      "1249/1249 - 38s - loss: 0.5267 - accuracy: 0.7666\n",
      "Epoch 80/100\n",
      "1249/1249 - 38s - loss: 0.5263 - accuracy: 0.7699\n",
      "Epoch 81/100\n",
      "1249/1249 - 38s - loss: 0.5254 - accuracy: 0.7712\n",
      "Epoch 82/100\n",
      "1249/1249 - 38s - loss: 0.5249 - accuracy: 0.7704\n",
      "Epoch 83/100\n",
      "1249/1249 - 38s - loss: 0.5259 - accuracy: 0.7698\n",
      "Epoch 84/100\n",
      "1249/1249 - 38s - loss: 0.5247 - accuracy: 0.7705\n",
      "Epoch 85/100\n",
      "1249/1249 - 38s - loss: 0.5253 - accuracy: 0.7699\n",
      "Epoch 86/100\n",
      "1249/1249 - 38s - loss: 0.5230 - accuracy: 0.7717\n",
      "Epoch 87/100\n",
      "1249/1249 - 38s - loss: 0.5236 - accuracy: 0.7709\n",
      "Epoch 88/100\n",
      "1249/1249 - 38s - loss: 0.5233 - accuracy: 0.7713\n",
      "Epoch 89/100\n",
      "1249/1249 - 38s - loss: 0.5225 - accuracy: 0.7717\n",
      "Epoch 90/100\n",
      "1249/1249 - 38s - loss: 0.5237 - accuracy: 0.7718\n",
      "Epoch 91/100\n",
      "1249/1249 - 38s - loss: 0.5227 - accuracy: 0.7702\n",
      "Epoch 92/100\n",
      "1249/1249 - 38s - loss: 0.5220 - accuracy: 0.7710\n",
      "Epoch 93/100\n",
      "1249/1249 - 38s - loss: 0.5224 - accuracy: 0.7695\n",
      "Epoch 94/100\n",
      "1249/1249 - 38s - loss: 0.5212 - accuracy: 0.7719\n",
      "Epoch 95/100\n",
      "1249/1249 - 38s - loss: 0.5221 - accuracy: 0.7716\n",
      "Epoch 96/100\n",
      "1249/1249 - 38s - loss: 0.5211 - accuracy: 0.7716\n",
      "Epoch 97/100\n",
      "1249/1249 - 38s - loss: 0.5208 - accuracy: 0.7720\n",
      "Epoch 98/100\n",
      "1249/1249 - 38s - loss: 0.5203 - accuracy: 0.7739\n",
      "Epoch 99/100\n",
      "1249/1249 - 38s - loss: 0.5193 - accuracy: 0.7733\n",
      "Epoch 100/100\n",
      "1249/1249 - 38s - loss: 0.5200 - accuracy: 0.7717\n",
      "417/417 - 3s - loss: 0.5744 - accuracy: 0.7451\n",
      "Epoch 1/100\n",
      "1666/1666 - 52s - loss: 0.7712 - accuracy: 0.7383\n",
      "Epoch 2/100\n",
      "1666/1666 - 51s - loss: 0.7406 - accuracy: 0.7413\n",
      "Epoch 3/100\n",
      "1666/1666 - 51s - loss: 0.7196 - accuracy: 0.7413\n",
      "Epoch 4/100\n",
      "1666/1666 - 50s - loss: 0.6785 - accuracy: 0.7412\n",
      "Epoch 5/100\n",
      "1666/1666 - 51s - loss: 0.6516 - accuracy: 0.7410\n",
      "Epoch 6/100\n",
      "1666/1666 - 51s - loss: 0.6428 - accuracy: 0.7414\n",
      "Epoch 7/100\n",
      "1666/1666 - 51s - loss: 0.6380 - accuracy: 0.7409\n",
      "Epoch 8/100\n",
      "1666/1666 - 51s - loss: 0.6341 - accuracy: 0.7414\n",
      "Epoch 9/100\n",
      "1666/1666 - 51s - loss: 0.6299 - accuracy: 0.7416\n",
      "Epoch 10/100\n",
      "1666/1666 - 51s - loss: 0.6276 - accuracy: 0.7419\n",
      "Epoch 11/100\n",
      "1666/1666 - 51s - loss: 0.6252 - accuracy: 0.7416\n",
      "Epoch 12/100\n",
      "1666/1666 - 51s - loss: 0.6220 - accuracy: 0.7423\n",
      "Epoch 13/100\n",
      "1666/1666 - 51s - loss: 0.6193 - accuracy: 0.7429\n",
      "Epoch 14/100\n",
      "1666/1666 - 51s - loss: 0.6165 - accuracy: 0.7432\n",
      "Epoch 15/100\n",
      "1666/1666 - 51s - loss: 0.6140 - accuracy: 0.7438\n",
      "Epoch 16/100\n",
      "1666/1666 - 51s - loss: 0.6112 - accuracy: 0.7445\n",
      "Epoch 17/100\n",
      "1666/1666 - 54s - loss: 0.6086 - accuracy: 0.7445\n",
      "Epoch 18/100\n",
      "1666/1666 - 51s - loss: 0.6055 - accuracy: 0.7465\n",
      "Epoch 19/100\n",
      "1666/1666 - 51s - loss: 0.6015 - accuracy: 0.7484\n",
      "Epoch 20/100\n",
      "1666/1666 - 51s - loss: 0.5993 - accuracy: 0.7476\n",
      "Epoch 21/100\n",
      "1666/1666 - 51s - loss: 0.5969 - accuracy: 0.7490\n",
      "Epoch 22/100\n",
      "1666/1666 - 52s - loss: 0.5956 - accuracy: 0.7490\n",
      "Epoch 23/100\n",
      "1666/1666 - 51s - loss: 0.5929 - accuracy: 0.7502\n",
      "Epoch 24/100\n",
      "1666/1666 - 51s - loss: 0.5912 - accuracy: 0.7498\n",
      "Epoch 25/100\n",
      "1666/1666 - 51s - loss: 0.5885 - accuracy: 0.7519\n",
      "Epoch 26/100\n",
      "1666/1666 - 51s - loss: 0.5869 - accuracy: 0.7535\n",
      "Epoch 27/100\n",
      "1666/1666 - 51s - loss: 0.5843 - accuracy: 0.7534\n",
      "Epoch 28/100\n",
      "1666/1666 - 51s - loss: 0.5834 - accuracy: 0.7551\n",
      "Epoch 29/100\n",
      "1666/1666 - 51s - loss: 0.5826 - accuracy: 0.7548\n",
      "Epoch 30/100\n",
      "1666/1666 - 51s - loss: 0.5810 - accuracy: 0.7554\n",
      "Epoch 31/100\n",
      "1666/1666 - 51s - loss: 0.5787 - accuracy: 0.7555\n",
      "Epoch 32/100\n",
      "1666/1666 - 50s - loss: 0.5774 - accuracy: 0.7569\n",
      "Epoch 33/100\n",
      "1666/1666 - 50s - loss: 0.5767 - accuracy: 0.7563\n",
      "Epoch 34/100\n",
      "1666/1666 - 50s - loss: 0.5763 - accuracy: 0.7561\n",
      "Epoch 35/100\n",
      "1666/1666 - 50s - loss: 0.5746 - accuracy: 0.7581\n",
      "Epoch 36/100\n",
      "1666/1666 - 51s - loss: 0.5734 - accuracy: 0.7570\n",
      "Epoch 37/100\n",
      "1666/1666 - 50s - loss: 0.5727 - accuracy: 0.7575\n",
      "Epoch 38/100\n",
      "1666/1666 - 50s - loss: 0.5726 - accuracy: 0.7560\n",
      "Epoch 39/100\n",
      "1666/1666 - 50s - loss: 0.5720 - accuracy: 0.7577\n",
      "Epoch 40/100\n",
      "1666/1666 - 50s - loss: 0.5703 - accuracy: 0.7576\n",
      "Epoch 41/100\n",
      "1666/1666 - 51s - loss: 0.5697 - accuracy: 0.7575\n",
      "Epoch 42/100\n",
      "1666/1666 - 50s - loss: 0.5685 - accuracy: 0.7571\n",
      "Epoch 43/100\n",
      "1666/1666 - 50s - loss: 0.5674 - accuracy: 0.7583\n",
      "Epoch 44/100\n",
      "1666/1666 - 51s - loss: 0.5663 - accuracy: 0.7590\n",
      "Epoch 45/100\n",
      "1666/1666 - 51s - loss: 0.5652 - accuracy: 0.7579\n",
      "Epoch 46/100\n",
      "1666/1666 - 51s - loss: 0.5657 - accuracy: 0.7579\n",
      "Epoch 47/100\n",
      "1666/1666 - 51s - loss: 0.5640 - accuracy: 0.7594\n",
      "Epoch 48/100\n",
      "1666/1666 - 50s - loss: 0.5638 - accuracy: 0.7580\n",
      "Epoch 49/100\n",
      "1666/1666 - 51s - loss: 0.5622 - accuracy: 0.7607\n",
      "Epoch 50/100\n",
      "1666/1666 - 51s - loss: 0.5627 - accuracy: 0.7589\n",
      "Epoch 51/100\n",
      "1666/1666 - 51s - loss: 0.5627 - accuracy: 0.7588\n",
      "Epoch 52/100\n",
      "1666/1666 - 50s - loss: 0.5610 - accuracy: 0.7608\n",
      "Epoch 53/100\n",
      "1666/1666 - 50s - loss: 0.5595 - accuracy: 0.7592\n",
      "Epoch 54/100\n",
      "1666/1666 - 51s - loss: 0.5595 - accuracy: 0.7605\n",
      "Epoch 55/100\n",
      "1666/1666 - 51s - loss: 0.5585 - accuracy: 0.7602\n",
      "Epoch 56/100\n",
      "1666/1666 - 51s - loss: 0.5587 - accuracy: 0.7597\n",
      "Epoch 57/100\n",
      "1666/1666 - 51s - loss: 0.5572 - accuracy: 0.7601\n",
      "Epoch 58/100\n",
      "1666/1666 - 51s - loss: 0.5565 - accuracy: 0.7584\n",
      "Epoch 59/100\n",
      "1666/1666 - 51s - loss: 0.5556 - accuracy: 0.7602\n",
      "Epoch 60/100\n",
      "1666/1666 - 51s - loss: 0.5548 - accuracy: 0.7598\n",
      "Epoch 61/100\n",
      "1666/1666 - 51s - loss: 0.5529 - accuracy: 0.7600\n",
      "Epoch 62/100\n",
      "1666/1666 - 51s - loss: 0.5523 - accuracy: 0.7603\n",
      "Epoch 63/100\n",
      "1666/1666 - 51s - loss: 0.5514 - accuracy: 0.7599\n",
      "Epoch 64/100\n",
      "1666/1666 - 51s - loss: 0.5503 - accuracy: 0.7596\n",
      "Epoch 65/100\n",
      "1666/1666 - 51s - loss: 0.5482 - accuracy: 0.7619\n",
      "Epoch 66/100\n",
      "1666/1666 - 51s - loss: 0.5475 - accuracy: 0.7607\n",
      "Epoch 67/100\n",
      "1666/1666 - 51s - loss: 0.5485 - accuracy: 0.7611\n",
      "Epoch 68/100\n",
      "1666/1666 - 51s - loss: 0.5462 - accuracy: 0.7600\n",
      "Epoch 69/100\n",
      "1666/1666 - 51s - loss: 0.5457 - accuracy: 0.7594\n",
      "Epoch 70/100\n",
      "1666/1666 - 51s - loss: 0.5457 - accuracy: 0.7591\n",
      "Epoch 71/100\n",
      "1666/1666 - 51s - loss: 0.5439 - accuracy: 0.7616\n",
      "Epoch 72/100\n",
      "1666/1666 - 51s - loss: 0.5445 - accuracy: 0.7606\n",
      "Epoch 73/100\n",
      "1666/1666 - 51s - loss: 0.5438 - accuracy: 0.7605\n",
      "Epoch 74/100\n",
      "1666/1666 - 51s - loss: 0.5424 - accuracy: 0.7612\n",
      "Epoch 75/100\n",
      "1666/1666 - 51s - loss: 0.5418 - accuracy: 0.7613\n",
      "Epoch 76/100\n",
      "1666/1666 - 51s - loss: 0.5416 - accuracy: 0.7606\n",
      "Epoch 77/100\n",
      "1666/1666 - 51s - loss: 0.5401 - accuracy: 0.7623\n",
      "Epoch 78/100\n",
      "1666/1666 - 51s - loss: 0.5399 - accuracy: 0.7608\n",
      "Epoch 79/100\n",
      "1666/1666 - 51s - loss: 0.5382 - accuracy: 0.7633\n",
      "Epoch 80/100\n",
      "1666/1666 - 51s - loss: 0.5390 - accuracy: 0.7607\n",
      "Epoch 81/100\n",
      "1666/1666 - 51s - loss: 0.5383 - accuracy: 0.7625\n",
      "Epoch 82/100\n",
      "1666/1666 - 51s - loss: 0.5376 - accuracy: 0.7635\n",
      "Epoch 83/100\n",
      "1666/1666 - 51s - loss: 0.5383 - accuracy: 0.7633\n",
      "Epoch 84/100\n",
      "1666/1666 - 51s - loss: 0.5369 - accuracy: 0.7632\n",
      "Epoch 85/100\n",
      "1666/1666 - 51s - loss: 0.5359 - accuracy: 0.7629\n",
      "Epoch 86/100\n",
      "1666/1666 - 51s - loss: 0.5355 - accuracy: 0.7629\n",
      "Epoch 87/100\n",
      "1666/1666 - 51s - loss: 0.5345 - accuracy: 0.7650\n",
      "Epoch 88/100\n",
      "1666/1666 - 51s - loss: 0.5337 - accuracy: 0.7631\n",
      "Epoch 89/100\n",
      "1666/1666 - 51s - loss: 0.5336 - accuracy: 0.7630\n",
      "Epoch 90/100\n",
      "1666/1666 - 51s - loss: 0.5327 - accuracy: 0.7652\n",
      "Epoch 91/100\n",
      "1666/1666 - 51s - loss: 0.5335 - accuracy: 0.7638\n",
      "Epoch 92/100\n",
      "1666/1666 - 51s - loss: 0.5326 - accuracy: 0.7626\n",
      "Epoch 93/100\n",
      "1666/1666 - 51s - loss: 0.5323 - accuracy: 0.7643\n",
      "Epoch 94/100\n",
      "1666/1666 - 51s - loss: 0.5317 - accuracy: 0.7648\n",
      "Epoch 95/100\n",
      "1666/1666 - 51s - loss: 0.5309 - accuracy: 0.7644\n",
      "Epoch 96/100\n",
      "1666/1666 - 51s - loss: 0.5307 - accuracy: 0.7631\n",
      "Epoch 97/100\n",
      "1666/1666 - 51s - loss: 0.5302 - accuracy: 0.7643\n",
      "Epoch 98/100\n",
      "1666/1666 - 51s - loss: 0.5301 - accuracy: 0.7651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/100\n",
      "1666/1666 - 51s - loss: 0.5295 - accuracy: 0.7648\n",
      "Epoch 100/100\n",
      "1666/1666 - 51s - loss: 0.5286 - accuracy: 0.7657\n",
      "417/417 - 3s - loss: 0.5374 - accuracy: 0.7508\n",
      "Epoch 1/100\n",
      "2082/2082 - 64s - loss: 0.7697 - accuracy: 0.7384\n",
      "Epoch 2/100\n",
      "2082/2082 - 63s - loss: 0.7364 - accuracy: 0.7406\n",
      "Epoch 3/100\n",
      "2082/2082 - 63s - loss: 0.6940 - accuracy: 0.7406\n",
      "Epoch 4/100\n",
      "2082/2082 - 63s - loss: 0.6511 - accuracy: 0.7404\n",
      "Epoch 5/100\n",
      "2082/2082 - 63s - loss: 0.6361 - accuracy: 0.7406\n",
      "Epoch 6/100\n",
      "2082/2082 - 63s - loss: 0.6292 - accuracy: 0.7415\n",
      "Epoch 7/100\n",
      "2082/2082 - 63s - loss: 0.6246 - accuracy: 0.7407\n",
      "Epoch 8/100\n",
      "2082/2082 - 63s - loss: 0.6208 - accuracy: 0.7407\n",
      "Epoch 9/100\n",
      "2082/2082 - 62s - loss: 0.6169 - accuracy: 0.7406\n",
      "Epoch 10/100\n",
      "2082/2082 - 63s - loss: 0.6135 - accuracy: 0.7417\n",
      "Epoch 11/100\n",
      "2082/2082 - 63s - loss: 0.6102 - accuracy: 0.7417\n",
      "Epoch 12/100\n",
      "2082/2082 - 63s - loss: 0.6072 - accuracy: 0.7417\n",
      "Epoch 13/100\n",
      "2082/2082 - 63s - loss: 0.6039 - accuracy: 0.7414\n",
      "Epoch 14/100\n",
      "2082/2082 - 62s - loss: 0.6014 - accuracy: 0.7423\n",
      "Epoch 15/100\n",
      "2082/2082 - 62s - loss: 0.5991 - accuracy: 0.7420\n",
      "Epoch 16/100\n",
      "2082/2082 - 62s - loss: 0.5950 - accuracy: 0.7431\n",
      "Epoch 17/100\n",
      "2082/2082 - 62s - loss: 0.5928 - accuracy: 0.7437\n",
      "Epoch 18/100\n",
      "2082/2082 - 62s - loss: 0.5890 - accuracy: 0.7443\n",
      "Epoch 19/100\n",
      "2082/2082 - 63s - loss: 0.5850 - accuracy: 0.7439\n",
      "Epoch 20/100\n",
      "2082/2082 - 62s - loss: 0.5807 - accuracy: 0.7455\n",
      "Epoch 21/100\n",
      "2082/2082 - 62s - loss: 0.5758 - accuracy: 0.7457\n",
      "Epoch 22/100\n",
      "2082/2082 - 62s - loss: 0.5729 - accuracy: 0.7471\n",
      "Epoch 23/100\n",
      "2082/2082 - 63s - loss: 0.5700 - accuracy: 0.7489\n",
      "Epoch 24/100\n",
      "2082/2082 - 62s - loss: 0.5672 - accuracy: 0.7506\n",
      "Epoch 25/100\n",
      "2082/2082 - 63s - loss: 0.5638 - accuracy: 0.7510\n",
      "Epoch 26/100\n",
      "2082/2082 - 63s - loss: 0.5616 - accuracy: 0.7515\n",
      "Epoch 27/100\n",
      "2082/2082 - 63s - loss: 0.5599 - accuracy: 0.7505\n",
      "Epoch 28/100\n",
      "2082/2082 - 62s - loss: 0.5590 - accuracy: 0.7507\n",
      "Epoch 29/100\n",
      "2082/2082 - 61s - loss: 0.5559 - accuracy: 0.7517\n",
      "Epoch 30/100\n",
      "2082/2082 - 62s - loss: 0.5542 - accuracy: 0.7520\n",
      "Epoch 31/100\n",
      "2082/2082 - 62s - loss: 0.5519 - accuracy: 0.7533\n",
      "Epoch 32/100\n",
      "2082/2082 - 62s - loss: 0.5494 - accuracy: 0.7547\n",
      "Epoch 33/100\n",
      "2082/2082 - 62s - loss: 0.5486 - accuracy: 0.7543\n",
      "Epoch 34/100\n",
      "2082/2082 - 61s - loss: 0.5466 - accuracy: 0.7552\n",
      "Epoch 35/100\n",
      "2082/2082 - 62s - loss: 0.5449 - accuracy: 0.7546\n",
      "Epoch 36/100\n",
      "2082/2082 - 62s - loss: 0.5443 - accuracy: 0.7556\n",
      "Epoch 37/100\n",
      "2082/2082 - 62s - loss: 0.5437 - accuracy: 0.7570\n",
      "Epoch 38/100\n",
      "2082/2082 - 62s - loss: 0.5429 - accuracy: 0.7562\n",
      "Epoch 39/100\n",
      "2082/2082 - 62s - loss: 0.5423 - accuracy: 0.7541\n",
      "Epoch 40/100\n",
      "2082/2082 - 62s - loss: 0.5404 - accuracy: 0.7563\n",
      "Epoch 41/100\n",
      "2082/2082 - 61s - loss: 0.5391 - accuracy: 0.7554\n",
      "Epoch 42/100\n",
      "2082/2082 - 62s - loss: 0.5388 - accuracy: 0.7560\n",
      "Epoch 43/100\n",
      "2082/2082 - 61s - loss: 0.5386 - accuracy: 0.7581\n",
      "Epoch 44/100\n",
      "2082/2082 - 62s - loss: 0.5374 - accuracy: 0.7558\n",
      "Epoch 45/100\n",
      "2082/2082 - 62s - loss: 0.5369 - accuracy: 0.7587\n",
      "Epoch 46/100\n",
      "2082/2082 - 61s - loss: 0.5373 - accuracy: 0.7573\n",
      "Epoch 47/100\n",
      "2082/2082 - 62s - loss: 0.5357 - accuracy: 0.7574\n",
      "Epoch 48/100\n",
      "2082/2082 - 63s - loss: 0.5361 - accuracy: 0.7567\n",
      "Epoch 49/100\n",
      "2082/2082 - 62s - loss: 0.5346 - accuracy: 0.7582\n",
      "Epoch 50/100\n",
      "2082/2082 - 63s - loss: 0.5338 - accuracy: 0.7603\n",
      "Epoch 51/100\n",
      "2082/2082 - 63s - loss: 0.5337 - accuracy: 0.7592\n",
      "Epoch 52/100\n",
      "2082/2082 - 63s - loss: 0.5335 - accuracy: 0.7588\n",
      "Epoch 53/100\n",
      "2082/2082 - 63s - loss: 0.5322 - accuracy: 0.7590\n",
      "Epoch 54/100\n",
      "2082/2082 - 63s - loss: 0.5324 - accuracy: 0.7595\n",
      "Epoch 55/100\n",
      "2082/2082 - 63s - loss: 0.5310 - accuracy: 0.7603\n",
      "Epoch 56/100\n",
      "2082/2082 - 63s - loss: 0.5307 - accuracy: 0.7595\n",
      "Epoch 57/100\n",
      "2082/2082 - 62s - loss: 0.5302 - accuracy: 0.7594\n",
      "Epoch 58/100\n",
      "2082/2082 - 62s - loss: 0.5300 - accuracy: 0.7595\n",
      "Epoch 59/100\n",
      "2082/2082 - 63s - loss: 0.5295 - accuracy: 0.7593\n",
      "Epoch 60/100\n",
      "2082/2082 - 63s - loss: 0.5291 - accuracy: 0.7600\n",
      "Epoch 61/100\n",
      "2082/2082 - 62s - loss: 0.5294 - accuracy: 0.7586\n",
      "Epoch 62/100\n",
      "2082/2082 - 62s - loss: 0.5286 - accuracy: 0.7611\n",
      "Epoch 63/100\n",
      "2082/2082 - 62s - loss: 0.5284 - accuracy: 0.7608\n",
      "Epoch 64/100\n",
      "2082/2082 - 62s - loss: 0.5277 - accuracy: 0.7619\n",
      "Epoch 65/100\n",
      "2082/2082 - 63s - loss: 0.5280 - accuracy: 0.7601\n",
      "Epoch 66/100\n",
      "2082/2082 - 63s - loss: 0.5266 - accuracy: 0.7607\n",
      "Epoch 67/100\n",
      "2082/2082 - 63s - loss: 0.5266 - accuracy: 0.7605\n",
      "Epoch 68/100\n",
      "2082/2082 - 63s - loss: 0.5270 - accuracy: 0.7598\n",
      "Epoch 69/100\n",
      "2082/2082 - 62s - loss: 0.5252 - accuracy: 0.7623\n",
      "Epoch 70/100\n",
      "2082/2082 - 62s - loss: 0.5256 - accuracy: 0.7621\n",
      "Epoch 71/100\n",
      "2082/2082 - 62s - loss: 0.5250 - accuracy: 0.7627\n",
      "Epoch 72/100\n",
      "2082/2082 - 62s - loss: 0.5254 - accuracy: 0.7617\n",
      "Epoch 73/100\n",
      "2082/2082 - 61s - loss: 0.5245 - accuracy: 0.7597\n",
      "Epoch 74/100\n",
      "2082/2082 - 62s - loss: 0.5233 - accuracy: 0.7629\n",
      "Epoch 75/100\n",
      "2082/2082 - 61s - loss: 0.5239 - accuracy: 0.7626\n",
      "Epoch 76/100\n",
      "2082/2082 - 61s - loss: 0.5233 - accuracy: 0.7625\n",
      "Epoch 77/100\n",
      "2082/2082 - 62s - loss: 0.5233 - accuracy: 0.7631\n",
      "Epoch 78/100\n",
      "2082/2082 - 61s - loss: 0.5234 - accuracy: 0.7618\n",
      "Epoch 79/100\n",
      "2082/2082 - 62s - loss: 0.5226 - accuracy: 0.7623\n",
      "Epoch 80/100\n",
      "2082/2082 - 62s - loss: 0.5213 - accuracy: 0.7630\n",
      "Epoch 81/100\n",
      "2082/2082 - 62s - loss: 0.5219 - accuracy: 0.7633\n",
      "Epoch 82/100\n",
      "2082/2082 - 64s - loss: 0.5223 - accuracy: 0.7632\n",
      "Epoch 83/100\n",
      "2082/2082 - 61s - loss: 0.5212 - accuracy: 0.7627\n",
      "Epoch 84/100\n",
      "2082/2082 - 61s - loss: 0.5211 - accuracy: 0.7610\n",
      "Epoch 85/100\n",
      "2082/2082 - 61s - loss: 0.5209 - accuracy: 0.7637\n",
      "Epoch 86/100\n",
      "2082/2082 - 62s - loss: 0.5208 - accuracy: 0.7629\n",
      "Epoch 87/100\n",
      "2082/2082 - 62s - loss: 0.5200 - accuracy: 0.7640\n",
      "Epoch 88/100\n",
      "2082/2082 - 62s - loss: 0.5198 - accuracy: 0.7637\n",
      "Epoch 89/100\n",
      "2082/2082 - 62s - loss: 0.5200 - accuracy: 0.7629\n",
      "Epoch 90/100\n",
      "2082/2082 - 62s - loss: 0.5191 - accuracy: 0.7637\n",
      "Epoch 91/100\n",
      "2082/2082 - 61s - loss: 0.5189 - accuracy: 0.7639\n",
      "Epoch 92/100\n",
      "2082/2082 - 61s - loss: 0.5189 - accuracy: 0.7644\n",
      "Epoch 93/100\n",
      "2082/2082 - 62s - loss: 0.5185 - accuracy: 0.7647\n",
      "Epoch 94/100\n",
      "2082/2082 - 62s - loss: 0.5185 - accuracy: 0.7647\n",
      "Epoch 95/100\n",
      "2082/2082 - 62s - loss: 0.5179 - accuracy: 0.7634\n",
      "Epoch 96/100\n",
      "2082/2082 - 62s - loss: 0.5181 - accuracy: 0.7649\n",
      "Epoch 97/100\n",
      "2082/2082 - 63s - loss: 0.5178 - accuracy: 0.7645\n",
      "Epoch 98/100\n",
      "2082/2082 - 62s - loss: 0.5174 - accuracy: 0.7645\n",
      "Epoch 99/100\n",
      "2082/2082 - 62s - loss: 0.5171 - accuracy: 0.7651\n",
      "Epoch 100/100\n",
      "2082/2082 - 61s - loss: 0.5167 - accuracy: 0.7645\n",
      "417/417 - 3s - loss: 0.6194 - accuracy: 0.7310\n",
      "Epoch 1/100\n",
      "2498/2498 - 76s - loss: 0.7804 - accuracy: 0.7231\n",
      "Epoch 2/100\n",
      "2498/2498 - 75s - loss: 0.7529 - accuracy: 0.7344\n",
      "Epoch 3/100\n",
      "2498/2498 - 77s - loss: 0.7133 - accuracy: 0.7344\n",
      "Epoch 4/100\n",
      "2498/2498 - 76s - loss: 0.6657 - accuracy: 0.7339\n",
      "Epoch 5/100\n",
      "2498/2498 - 75s - loss: 0.6468 - accuracy: 0.7339\n",
      "Epoch 6/100\n",
      "2498/2498 - 76s - loss: 0.6378 - accuracy: 0.7344\n",
      "Epoch 7/100\n",
      "2498/2498 - 75s - loss: 0.6325 - accuracy: 0.7351\n",
      "Epoch 8/100\n",
      "2498/2498 - 76s - loss: 0.6286 - accuracy: 0.7366\n",
      "Epoch 9/100\n",
      "2498/2498 - 76s - loss: 0.6252 - accuracy: 0.7353\n",
      "Epoch 10/100\n",
      "2498/2498 - 75s - loss: 0.6222 - accuracy: 0.7366\n",
      "Epoch 11/100\n",
      "2498/2498 - 76s - loss: 0.6200 - accuracy: 0.7375\n",
      "Epoch 12/100\n",
      "2498/2498 - 76s - loss: 0.6154 - accuracy: 0.7376\n",
      "Epoch 13/100\n",
      "2498/2498 - 75s - loss: 0.6136 - accuracy: 0.7379\n",
      "Epoch 14/100\n",
      "2498/2498 - 75s - loss: 0.6103 - accuracy: 0.7392\n",
      "Epoch 15/100\n",
      "2498/2498 - 75s - loss: 0.6073 - accuracy: 0.7384\n",
      "Epoch 16/100\n",
      "2498/2498 - 76s - loss: 0.6041 - accuracy: 0.7402\n",
      "Epoch 17/100\n",
      "2498/2498 - 76s - loss: 0.6011 - accuracy: 0.7416\n",
      "Epoch 18/100\n",
      "2498/2498 - 75s - loss: 0.5977 - accuracy: 0.7411\n",
      "Epoch 19/100\n",
      "2498/2498 - 75s - loss: 0.5942 - accuracy: 0.7418\n",
      "Epoch 20/100\n",
      "2498/2498 - 75s - loss: 0.5922 - accuracy: 0.7439\n",
      "Epoch 21/100\n",
      "2498/2498 - 75s - loss: 0.5897 - accuracy: 0.7447\n",
      "Epoch 22/100\n",
      "2498/2498 - 77s - loss: 0.5851 - accuracy: 0.7457\n",
      "Epoch 23/100\n",
      "2498/2498 - 75s - loss: 0.5825 - accuracy: 0.7460\n",
      "Epoch 24/100\n",
      "2498/2498 - 75s - loss: 0.5789 - accuracy: 0.7472\n",
      "Epoch 25/100\n",
      "2498/2498 - 74s - loss: 0.5799 - accuracy: 0.7488\n",
      "Epoch 26/100\n",
      "2498/2498 - 74s - loss: 0.5752 - accuracy: 0.7494\n",
      "Epoch 27/100\n",
      "2498/2498 - 74s - loss: 0.5737 - accuracy: 0.7484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100\n",
      "2498/2498 - 75s - loss: 0.5707 - accuracy: 0.7489\n",
      "Epoch 29/100\n",
      "2498/2498 - 74s - loss: 0.5695 - accuracy: 0.7496\n",
      "Epoch 30/100\n",
      "2498/2498 - 74s - loss: 0.5659 - accuracy: 0.7507\n",
      "Epoch 31/100\n",
      "2498/2498 - 74s - loss: 0.5649 - accuracy: 0.7507\n",
      "Epoch 32/100\n",
      "2498/2498 - 75s - loss: 0.5622 - accuracy: 0.7495\n",
      "Epoch 33/100\n",
      "2498/2498 - 74s - loss: 0.5594 - accuracy: 0.7509\n",
      "Epoch 34/100\n",
      "2498/2498 - 74s - loss: 0.5575 - accuracy: 0.7523\n",
      "Epoch 35/100\n",
      "2498/2498 - 74s - loss: 0.5557 - accuracy: 0.7522\n",
      "Epoch 36/100\n",
      "2498/2498 - 74s - loss: 0.5542 - accuracy: 0.7518\n",
      "Epoch 37/100\n",
      "2498/2498 - 75s - loss: 0.5518 - accuracy: 0.7539\n",
      "Epoch 38/100\n",
      "2498/2498 - 75s - loss: 0.5510 - accuracy: 0.7530\n",
      "Epoch 39/100\n",
      "2498/2498 - 75s - loss: 0.5493 - accuracy: 0.7556\n",
      "Epoch 40/100\n",
      "2498/2498 - 75s - loss: 0.5478 - accuracy: 0.7531\n",
      "Epoch 41/100\n",
      "2498/2498 - 75s - loss: 0.5469 - accuracy: 0.7544\n",
      "Epoch 42/100\n",
      "2498/2498 - 75s - loss: 0.5458 - accuracy: 0.7554\n",
      "Epoch 43/100\n",
      "2498/2498 - 75s - loss: 0.5446 - accuracy: 0.7554\n",
      "Epoch 44/100\n",
      "2498/2498 - 75s - loss: 0.5442 - accuracy: 0.7530\n",
      "Epoch 45/100\n",
      "2498/2498 - 75s - loss: 0.5421 - accuracy: 0.7556\n",
      "Epoch 46/100\n",
      "2498/2498 - 75s - loss: 0.5423 - accuracy: 0.7546\n",
      "Epoch 47/100\n",
      "2498/2498 - 75s - loss: 0.5411 - accuracy: 0.7544\n",
      "Epoch 48/100\n",
      "2498/2498 - 75s - loss: 0.5397 - accuracy: 0.7554\n",
      "Epoch 49/100\n",
      "2498/2498 - 75s - loss: 0.5388 - accuracy: 0.7564\n",
      "Epoch 50/100\n",
      "2498/2498 - 75s - loss: 0.5387 - accuracy: 0.7567\n",
      "Epoch 51/100\n",
      "2498/2498 - 75s - loss: 0.5377 - accuracy: 0.7564\n",
      "Epoch 52/100\n",
      "2498/2498 - 75s - loss: 0.5376 - accuracy: 0.7549\n",
      "Epoch 53/100\n",
      "2498/2498 - 75s - loss: 0.5367 - accuracy: 0.7556\n",
      "Epoch 54/100\n",
      "2498/2498 - 75s - loss: 0.5360 - accuracy: 0.7562\n",
      "Epoch 55/100\n",
      "2498/2498 - 75s - loss: 0.5353 - accuracy: 0.7567\n",
      "Epoch 56/100\n",
      "2498/2498 - 75s - loss: 0.5347 - accuracy: 0.7563\n",
      "Epoch 57/100\n",
      "2498/2498 - 75s - loss: 0.5338 - accuracy: 0.7567\n",
      "Epoch 58/100\n",
      "2498/2498 - 75s - loss: 0.5331 - accuracy: 0.7562\n",
      "Epoch 59/100\n",
      "2498/2498 - 75s - loss: 0.5334 - accuracy: 0.7580\n",
      "Epoch 60/100\n",
      "2498/2498 - 75s - loss: 0.5332 - accuracy: 0.7572\n",
      "Epoch 61/100\n",
      "2498/2498 - 75s - loss: 0.5320 - accuracy: 0.7577\n",
      "Epoch 62/100\n",
      "2498/2498 - 77s - loss: 0.5314 - accuracy: 0.7569\n",
      "Epoch 63/100\n",
      "2498/2498 - 73s - loss: 0.5304 - accuracy: 0.7596\n",
      "Epoch 64/100\n",
      "2498/2498 - 73s - loss: 0.5305 - accuracy: 0.7579\n",
      "Epoch 65/100\n",
      "2498/2498 - 73s - loss: 0.5302 - accuracy: 0.7584\n",
      "Epoch 66/100\n",
      "2498/2498 - 73s - loss: 0.5292 - accuracy: 0.7588\n",
      "Epoch 67/100\n",
      "2498/2498 - 73s - loss: 0.5284 - accuracy: 0.7591\n",
      "Epoch 68/100\n",
      "2498/2498 - 73s - loss: 0.5285 - accuracy: 0.7594\n",
      "Epoch 69/100\n",
      "2498/2498 - 73s - loss: 0.5282 - accuracy: 0.7588\n",
      "Epoch 70/100\n",
      "2498/2498 - 73s - loss: 0.5273 - accuracy: 0.7597\n",
      "Epoch 71/100\n",
      "2498/2498 - 74s - loss: 0.5272 - accuracy: 0.7582\n",
      "Epoch 72/100\n",
      "2498/2498 - 74s - loss: 0.5261 - accuracy: 0.7598\n",
      "Epoch 73/100\n",
      "2498/2498 - 74s - loss: 0.5257 - accuracy: 0.7595\n",
      "Epoch 74/100\n",
      "2498/2498 - 75s - loss: 0.5252 - accuracy: 0.7596\n",
      "Epoch 75/100\n",
      "2498/2498 - 74s - loss: 0.5246 - accuracy: 0.7609\n",
      "Epoch 76/100\n",
      "2498/2498 - 74s - loss: 0.5237 - accuracy: 0.7616\n",
      "Epoch 77/100\n",
      "2498/2498 - 74s - loss: 0.5243 - accuracy: 0.7592\n",
      "Epoch 78/100\n",
      "2498/2498 - 74s - loss: 0.5241 - accuracy: 0.7609\n",
      "Epoch 79/100\n",
      "2498/2498 - 74s - loss: 0.5233 - accuracy: 0.7605\n",
      "Epoch 80/100\n",
      "2498/2498 - 74s - loss: 0.5232 - accuracy: 0.7610\n",
      "Epoch 81/100\n",
      "2498/2498 - 74s - loss: 0.5226 - accuracy: 0.7609\n",
      "Epoch 82/100\n",
      "2498/2498 - 74s - loss: 0.5217 - accuracy: 0.7625\n",
      "Epoch 83/100\n",
      "2498/2498 - 75s - loss: 0.5218 - accuracy: 0.7612\n",
      "Epoch 84/100\n",
      "2498/2498 - 74s - loss: 0.5209 - accuracy: 0.7625\n",
      "Epoch 85/100\n",
      "2498/2498 - 74s - loss: 0.5214 - accuracy: 0.7621\n",
      "Epoch 86/100\n",
      "2498/2498 - 74s - loss: 0.5212 - accuracy: 0.7620\n",
      "Epoch 87/100\n",
      "2498/2498 - 75s - loss: 0.5197 - accuracy: 0.7625\n",
      "Epoch 88/100\n",
      "2498/2498 - 76s - loss: 0.5194 - accuracy: 0.7636\n",
      "Epoch 89/100\n",
      "2498/2498 - 76s - loss: 0.5188 - accuracy: 0.7633\n",
      "Epoch 90/100\n",
      "2498/2498 - 74s - loss: 0.5186 - accuracy: 0.7630\n",
      "Epoch 91/100\n",
      "2498/2498 - 75s - loss: 0.5185 - accuracy: 0.7624\n",
      "Epoch 92/100\n",
      "2498/2498 - 74s - loss: 0.5191 - accuracy: 0.7627\n",
      "Epoch 93/100\n",
      "2498/2498 - 75s - loss: 0.5178 - accuracy: 0.7639\n",
      "Epoch 94/100\n",
      "2498/2498 - 74s - loss: 0.5173 - accuracy: 0.7635\n",
      "Epoch 95/100\n",
      "2498/2498 - 75s - loss: 0.5174 - accuracy: 0.7643\n",
      "Epoch 96/100\n",
      "2498/2498 - 74s - loss: 0.5172 - accuracy: 0.7638\n",
      "Epoch 97/100\n",
      "2498/2498 - 74s - loss: 0.5161 - accuracy: 0.7651\n",
      "Epoch 98/100\n",
      "2498/2498 - 74s - loss: 0.5168 - accuracy: 0.7640\n",
      "Epoch 99/100\n",
      "2498/2498 - 74s - loss: 0.5158 - accuracy: 0.7644\n",
      "Epoch 100/100\n",
      "2498/2498 - 74s - loss: 0.5156 - accuracy: 0.7637\n",
      "OF dropout rate: 0.4\n",
      "OFI dropout rate: 0.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, TimeSeriesSplit, GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow import device\n",
    "\n",
    "with device('cpu:0'):\n",
    "    for time_series in model_details:\n",
    "\n",
    "        param_grid = {'time_series': [time_series],\n",
    "                  'lag_param': [model_details[time_series]['lag_param']],\n",
    "                  'dropout': [0.1,0.2,0.3,0.4,0.5]} # list of 5 dropout rates to iterate through in the search\n",
    "        tscv = TimeSeriesSplit(n_splits = 5) # kth split returns first k folds for training and (k+1)st fold for testing\n",
    "        model = KerasRegressor(build_fn=model_details[time_series]['function'], \n",
    "                               epochs=max_epochs, batch_size=batch_size, verbose=2)\n",
    "        grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=tscv, n_jobs=1, verbose=1)\n",
    "        grid_result = grid.fit(model_details[time_series]['train_x'], model_details[time_series]['train_y'],\n",
    "                               callbacks=[es])\n",
    "\n",
    "        # save the dropout rate that performs the best\n",
    "        model_details[time_series]['dropout'] = grid_result.best_params_['dropout']\n",
    "\n",
    "# print the optimized dropout rates\n",
    "for time_series in model_details:\n",
    "    print(time_series + ' dropout rate: ' + str(model_details[time_series]['dropout']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Models\n",
    "Time to train and save the models. I want to really ensure convergence of accuracy and loss, so I increase the maximum number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_356\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_520 (InputLayer)          [(None, 69, 20, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4454 (Conv2D)            (None, 69, 10, 16)   48          input_520[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4454 (LeakyReLU)    (None, 69, 10, 16)   0           conv2d_4454[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4455 (Conv2D)            (None, 69, 10, 16)   1040        leaky_re_lu_4454[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4455 (LeakyReLU)    (None, 69, 10, 16)   0           conv2d_4455[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4456 (Conv2D)            (None, 69, 10, 16)   1040        leaky_re_lu_4455[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4456 (LeakyReLU)    (None, 69, 10, 16)   0           conv2d_4456[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4457 (Conv2D)            (None, 69, 1, 16)    2576        leaky_re_lu_4456[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4457 (LeakyReLU)    (None, 69, 1, 16)    0           conv2d_4457[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4458 (Conv2D)            (None, 69, 1, 32)    544         leaky_re_lu_4457[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4460 (Conv2D)            (None, 69, 1, 32)    544         leaky_re_lu_4457[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4458 (LeakyReLU)    (None, 69, 1, 32)    0           conv2d_4458[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4460 (LeakyReLU)    (None, 69, 1, 32)    0           conv2d_4460[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_519 (MaxPooling2D (None, 69, 1, 16)    0           leaky_re_lu_4457[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4459 (Conv2D)            (None, 69, 1, 32)    3104        leaky_re_lu_4458[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4461 (Conv2D)            (None, 69, 1, 32)    5152        leaky_re_lu_4460[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4462 (Conv2D)            (None, 69, 1, 32)    544         max_pooling2d_519[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4459 (LeakyReLU)    (None, 69, 1, 32)    0           conv2d_4459[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4461 (LeakyReLU)    (None, 69, 1, 32)    0           conv2d_4461[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4462 (LeakyReLU)    (None, 69, 1, 32)    0           conv2d_4462[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_519 (Concatenate)   (None, 69, 1, 96)    0           leaky_re_lu_4459[0][0]           \n",
      "                                                                 leaky_re_lu_4461[0][0]           \n",
      "                                                                 leaky_re_lu_4462[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reshape_519 (Reshape)           (None, 69, 96)       0           concatenate_519[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_357 (Dropout)           (None, 69, 96)       0           reshape_519[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_357 (LSTM)                 (None, 64)           41216       dropout_357[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_356 (Dense)               (None, 3)            195         lstm_357[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 56,003\n",
      "Trainable params: 56,003\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/250\n",
      "2498/2498 [==============================] - 75s 29ms/step - loss: 0.7797 - accuracy: 0.7307\n",
      "Epoch 2/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.7616 - accuracy: 0.7344\n",
      "Epoch 3/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.7542 - accuracy: 0.7344\n",
      "Epoch 4/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.6959 - accuracy: 0.7344\n",
      "Epoch 5/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.6324 - accuracy: 0.7374\n",
      "Epoch 6/250\n",
      "2498/2498 [==============================] - 74s 29ms/step - loss: 0.6162 - accuracy: 0.7424\n",
      "Epoch 7/250\n",
      "2498/2498 [==============================] - 73s 29ms/step - loss: 0.6069 - accuracy: 0.7464\n",
      "Epoch 8/250\n",
      "2498/2498 [==============================] - 73s 29ms/step - loss: 0.5996 - accuracy: 0.7491\n",
      "Epoch 9/250\n",
      "2498/2498 [==============================] - 72s 29ms/step - loss: 0.5943 - accuracy: 0.7507\n",
      "Epoch 10/250\n",
      "2498/2498 [==============================] - 72s 29ms/step - loss: 0.5868 - accuracy: 0.7514\n",
      "Epoch 11/250\n",
      "2498/2498 [==============================] - 72s 29ms/step - loss: 0.5778 - accuracy: 0.7541\n",
      "Epoch 12/250\n",
      "2498/2498 [==============================] - 72s 29ms/step - loss: 0.5719 - accuracy: 0.7559\n",
      "Epoch 13/250\n",
      "2498/2498 [==============================] - 72s 29ms/step - loss: 0.5667 - accuracy: 0.7562\n",
      "Epoch 14/250\n",
      "2498/2498 [==============================] - 72s 29ms/step - loss: 0.5605 - accuracy: 0.7574\n",
      "Epoch 15/250\n",
      "2498/2498 [==============================] - 72s 29ms/step - loss: 0.5560 - accuracy: 0.7568\n",
      "Epoch 16/250\n",
      "2498/2498 [==============================] - 72s 29ms/step - loss: 0.5529 - accuracy: 0.7583\n",
      "Epoch 17/250\n",
      "2498/2498 [==============================] - 72s 29ms/step - loss: 0.5478 - accuracy: 0.7584\n",
      "Epoch 18/250\n",
      "2498/2498 [==============================] - 72s 29ms/step - loss: 0.5456 - accuracy: 0.7590\n",
      "Epoch 19/250\n",
      "2498/2498 [==============================] - 73s 29ms/step - loss: 0.5415 - accuracy: 0.7604\n",
      "Epoch 20/250\n",
      "2498/2498 [==============================] - 73s 29ms/step - loss: 0.5392 - accuracy: 0.7605\n",
      "Epoch 21/250\n",
      "2498/2498 [==============================] - 73s 29ms/step - loss: 0.5362 - accuracy: 0.7597\n",
      "Epoch 22/250\n",
      "2498/2498 [==============================] - 73s 29ms/step - loss: 0.5344 - accuracy: 0.7614\n",
      "Epoch 23/250\n",
      "2498/2498 [==============================] - 73s 29ms/step - loss: 0.5324 - accuracy: 0.7627\n",
      "Epoch 24/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.5310 - accuracy: 0.7606\n",
      "Epoch 25/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.5286 - accuracy: 0.7629\n",
      "Epoch 26/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.5265 - accuracy: 0.7629\n",
      "Epoch 27/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.5256 - accuracy: 0.7639\n",
      "Epoch 28/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.5238 - accuracy: 0.7640\n",
      "Epoch 29/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.5218 - accuracy: 0.7650\n",
      "Epoch 30/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.5212 - accuracy: 0.7649\n",
      "Epoch 31/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.5203 - accuracy: 0.7651\n",
      "Epoch 32/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.5183 - accuracy: 0.7662\n",
      "Epoch 33/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.5167 - accuracy: 0.7665\n",
      "Epoch 34/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.5159 - accuracy: 0.7656\n",
      "Epoch 35/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.5137 - accuracy: 0.7677\n",
      "Epoch 36/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.5131 - accuracy: 0.7689\n",
      "Epoch 37/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.5120 - accuracy: 0.7686\n",
      "Epoch 38/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.5104 - accuracy: 0.7692\n",
      "Epoch 39/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.5105 - accuracy: 0.7685\n",
      "Epoch 40/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.5086 - accuracy: 0.7703\n",
      "Epoch 41/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.5085 - accuracy: 0.7698\n",
      "Epoch 42/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.5084 - accuracy: 0.7685\n",
      "Epoch 43/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.5064 - accuracy: 0.7702\n",
      "Epoch 44/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.5062 - accuracy: 0.7710\n",
      "Epoch 45/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.5047 - accuracy: 0.7711\n",
      "Epoch 46/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.5048 - accuracy: 0.7709\n",
      "Epoch 47/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.5043 - accuracy: 0.7704\n",
      "Epoch 48/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.5036 - accuracy: 0.7712\n",
      "Epoch 49/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.5024 - accuracy: 0.7712\n",
      "Epoch 50/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.5026 - accuracy: 0.7716\n",
      "Epoch 51/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.5010 - accuracy: 0.7720\n",
      "Epoch 52/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.5014 - accuracy: 0.7723\n",
      "Epoch 53/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.5015 - accuracy: 0.7733\n",
      "Epoch 54/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.5003 - accuracy: 0.7726\n",
      "Epoch 55/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.5001 - accuracy: 0.7735\n",
      "Epoch 56/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4988 - accuracy: 0.7735\n",
      "Epoch 57/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4986 - accuracy: 0.7747\n",
      "Epoch 58/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4988 - accuracy: 0.7743\n",
      "Epoch 59/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4981 - accuracy: 0.7737\n",
      "Epoch 60/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4983 - accuracy: 0.7736\n",
      "Epoch 61/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4968 - accuracy: 0.7745\n",
      "Epoch 62/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4967 - accuracy: 0.7740\n",
      "Epoch 63/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4962 - accuracy: 0.7758\n",
      "Epoch 64/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4960 - accuracy: 0.7765\n",
      "Epoch 65/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4950 - accuracy: 0.7764\n",
      "Epoch 66/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4948 - accuracy: 0.7756\n",
      "Epoch 67/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4940 - accuracy: 0.7760\n",
      "Epoch 68/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4940 - accuracy: 0.7762\n",
      "Epoch 69/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4945 - accuracy: 0.7749\n",
      "Epoch 70/250\n",
      "2498/2498 [==============================] - 79s 32ms/step - loss: 0.4939 - accuracy: 0.7770\n",
      "Epoch 71/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4936 - accuracy: 0.7747\n",
      "Epoch 72/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4928 - accuracy: 0.7771\n",
      "Epoch 73/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4925 - accuracy: 0.7775\n",
      "Epoch 74/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4915 - accuracy: 0.7772\n",
      "Epoch 75/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4916 - accuracy: 0.7770\n",
      "Epoch 76/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4909 - accuracy: 0.7774\n",
      "Epoch 77/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4906 - accuracy: 0.7778\n",
      "Epoch 78/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4905 - accuracy: 0.7779\n",
      "Epoch 79/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4902 - accuracy: 0.7777\n",
      "Epoch 80/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4897 - accuracy: 0.7786\n",
      "Epoch 81/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4898 - accuracy: 0.7777\n",
      "Epoch 82/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4892 - accuracy: 0.7790\n",
      "Epoch 83/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4891 - accuracy: 0.7790\n",
      "Epoch 84/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4890 - accuracy: 0.7796\n",
      "Epoch 85/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4880 - accuracy: 0.7789\n",
      "Epoch 86/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4885 - accuracy: 0.7787\n",
      "Epoch 87/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4872 - accuracy: 0.7786\n",
      "Epoch 88/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4873 - accuracy: 0.7790\n",
      "Epoch 89/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4873 - accuracy: 0.7797\n",
      "Epoch 90/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4867 - accuracy: 0.7796\n",
      "Epoch 91/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4868 - accuracy: 0.7798\n",
      "Epoch 92/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4863 - accuracy: 0.7798\n",
      "Epoch 93/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4856 - accuracy: 0.7797\n",
      "Epoch 94/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4858 - accuracy: 0.7792\n",
      "Epoch 95/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4852 - accuracy: 0.7804\n",
      "Epoch 96/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4849 - accuracy: 0.7819\n",
      "Epoch 97/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4851 - accuracy: 0.7802\n",
      "Epoch 98/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4843 - accuracy: 0.7818\n",
      "Epoch 99/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4840 - accuracy: 0.7814\n",
      "Epoch 100/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4842 - accuracy: 0.7810\n",
      "Epoch 101/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4838 - accuracy: 0.7809\n",
      "Epoch 102/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4831 - accuracy: 0.7816\n",
      "Epoch 103/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4827 - accuracy: 0.7815\n",
      "Epoch 104/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4822 - accuracy: 0.7812\n",
      "Epoch 105/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4825 - accuracy: 0.7819\n",
      "Epoch 106/250\n",
      "2498/2498 [==============================] - 73s 29ms/step - loss: 0.4815 - accuracy: 0.7823\n",
      "Epoch 107/250\n",
      "2498/2498 [==============================] - 73s 29ms/step - loss: 0.4819 - accuracy: 0.7828\n",
      "Epoch 108/250\n",
      "2498/2498 [==============================] - 73s 29ms/step - loss: 0.4826 - accuracy: 0.7830\n",
      "Epoch 109/250\n",
      "2498/2498 [==============================] - 74s 29ms/step - loss: 0.4810 - accuracy: 0.7818\n",
      "Epoch 110/250\n",
      "2498/2498 [==============================] - 74s 29ms/step - loss: 0.4813 - accuracy: 0.7835\n",
      "Epoch 111/250\n",
      "2498/2498 [==============================] - 74s 29ms/step - loss: 0.4813 - accuracy: 0.7818\n",
      "Epoch 112/250\n",
      "2498/2498 [==============================] - 74s 29ms/step - loss: 0.4802 - accuracy: 0.7834\n",
      "Epoch 113/250\n",
      "2498/2498 [==============================] - 74s 29ms/step - loss: 0.4797 - accuracy: 0.7836\n",
      "Epoch 114/250\n",
      "2498/2498 [==============================] - 74s 29ms/step - loss: 0.4795 - accuracy: 0.7837\n",
      "Epoch 115/250\n",
      "2498/2498 [==============================] - 74s 29ms/step - loss: 0.4794 - accuracy: 0.7840\n",
      "Epoch 116/250\n",
      "2498/2498 [==============================] - 73s 29ms/step - loss: 0.4799 - accuracy: 0.7843\n",
      "Epoch 117/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4791 - accuracy: 0.7835\n",
      "Epoch 118/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4790 - accuracy: 0.7846\n",
      "Epoch 119/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4783 - accuracy: 0.7842\n",
      "Epoch 120/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4790 - accuracy: 0.7839\n",
      "Epoch 121/250\n",
      "2498/2498 [==============================] - 74s 29ms/step - loss: 0.4779 - accuracy: 0.7844\n",
      "Epoch 122/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4777 - accuracy: 0.7852\n",
      "Epoch 123/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4779 - accuracy: 0.7845\n",
      "Epoch 124/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.4773 - accuracy: 0.7851\n",
      "Epoch 125/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4789 - accuracy: 0.7842\n",
      "Epoch 126/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4771 - accuracy: 0.7844\n",
      "Epoch 127/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4765 - accuracy: 0.7849\n",
      "Epoch 128/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4758 - accuracy: 0.7853\n",
      "Epoch 129/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4763 - accuracy: 0.7847\n",
      "Epoch 130/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4758 - accuracy: 0.7858\n",
      "Epoch 131/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4754 - accuracy: 0.7857\n",
      "Epoch 132/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4753 - accuracy: 0.7856\n",
      "Epoch 133/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4747 - accuracy: 0.7866\n",
      "Epoch 134/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4753 - accuracy: 0.7854\n",
      "Epoch 135/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4752 - accuracy: 0.7851\n",
      "Epoch 136/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.4744 - accuracy: 0.7861\n",
      "Epoch 137/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4742 - accuracy: 0.7856\n",
      "Epoch 138/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4738 - accuracy: 0.7865\n",
      "Epoch 139/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4737 - accuracy: 0.7859\n",
      "Epoch 140/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4732 - accuracy: 0.7878\n",
      "Epoch 141/250\n",
      "2498/2498 [==============================] - 74s 29ms/step - loss: 0.4722 - accuracy: 0.7877\n",
      "Epoch 142/250\n",
      "2498/2498 [==============================] - 74s 29ms/step - loss: 0.4727 - accuracy: 0.7868\n",
      "Epoch 143/250\n",
      "2498/2498 [==============================] - 73s 29ms/step - loss: 0.4719 - accuracy: 0.7870\n",
      "Epoch 144/250\n",
      "2498/2498 [==============================] - 74s 29ms/step - loss: 0.4727 - accuracy: 0.7867\n",
      "Epoch 145/250\n",
      "2498/2498 [==============================] - 74s 29ms/step - loss: 0.4722 - accuracy: 0.7880\n",
      "Epoch 146/250\n",
      "2498/2498 [==============================] - 74s 29ms/step - loss: 0.4712 - accuracy: 0.7877\n",
      "Epoch 147/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4719 - accuracy: 0.7878\n",
      "Epoch 148/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.4710 - accuracy: 0.7881\n",
      "Epoch 149/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4708 - accuracy: 0.7882\n",
      "Epoch 150/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4705 - accuracy: 0.7872\n",
      "Epoch 151/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4699 - accuracy: 0.7890\n",
      "Epoch 152/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4706 - accuracy: 0.7881\n",
      "Epoch 153/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4697 - accuracy: 0.7893\n",
      "Epoch 154/250\n",
      "2498/2498 [==============================] - 74s 29ms/step - loss: 0.4698 - accuracy: 0.7894\n",
      "Epoch 155/250\n",
      "2498/2498 [==============================] - 73s 29ms/step - loss: 0.4689 - accuracy: 0.7885\n",
      "Epoch 156/250\n",
      "2498/2498 [==============================] - 74s 29ms/step - loss: 0.4696 - accuracy: 0.7898\n",
      "Epoch 157/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4693 - accuracy: 0.7898\n",
      "Epoch 158/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.4677 - accuracy: 0.7892\n",
      "Epoch 159/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4681 - accuracy: 0.7893\n",
      "Epoch 160/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4685 - accuracy: 0.7879\n",
      "Epoch 161/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4668 - accuracy: 0.7903\n",
      "Epoch 162/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4671 - accuracy: 0.7901\n",
      "Epoch 163/250\n",
      "2498/2498 [==============================] - 73s 29ms/step - loss: 0.4675 - accuracy: 0.7889\n",
      "Epoch 164/250\n",
      "2498/2498 [==============================] - 73s 29ms/step - loss: 0.4677 - accuracy: 0.7916\n",
      "Epoch 165/250\n",
      "2498/2498 [==============================] - 73s 29ms/step - loss: 0.4665 - accuracy: 0.7901\n",
      "Epoch 166/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4663 - accuracy: 0.7895\n",
      "Epoch 167/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.4648 - accuracy: 0.7923\n",
      "Epoch 168/250\n",
      "2498/2498 [==============================] - 73s 29ms/step - loss: 0.4655 - accuracy: 0.7907\n",
      "Epoch 169/250\n",
      "2498/2498 [==============================] - 73s 29ms/step - loss: 0.4665 - accuracy: 0.7909\n",
      "Epoch 170/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4681 - accuracy: 0.7896\n",
      "Epoch 171/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4657 - accuracy: 0.7907\n",
      "Epoch 172/250\n",
      "2498/2498 [==============================] - 74s 30ms/step - loss: 0.4651 - accuracy: 0.7908\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00172: early stopping\n",
      "Model: \"model_357\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_521 (InputLayer)          [(None, 76, 10, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4463 (Conv2D)            (None, 76, 10, 16)   80          input_521[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4463 (LeakyReLU)    (None, 76, 10, 16)   0           conv2d_4463[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4464 (Conv2D)            (None, 76, 10, 16)   1040        leaky_re_lu_4463[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4464 (LeakyReLU)    (None, 76, 10, 16)   0           conv2d_4464[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4465 (Conv2D)            (None, 76, 1, 16)    2576        leaky_re_lu_4464[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4465 (LeakyReLU)    (None, 76, 1, 16)    0           conv2d_4465[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4466 (Conv2D)            (None, 76, 1, 32)    544         leaky_re_lu_4465[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4468 (Conv2D)            (None, 76, 1, 32)    544         leaky_re_lu_4465[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4466 (LeakyReLU)    (None, 76, 1, 32)    0           conv2d_4466[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4468 (LeakyReLU)    (None, 76, 1, 32)    0           conv2d_4468[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_520 (MaxPooling2D (None, 76, 1, 16)    0           leaky_re_lu_4465[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4467 (Conv2D)            (None, 76, 1, 32)    3104        leaky_re_lu_4466[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4469 (Conv2D)            (None, 76, 1, 32)    5152        leaky_re_lu_4468[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4470 (Conv2D)            (None, 76, 1, 32)    544         max_pooling2d_520[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4467 (LeakyReLU)    (None, 76, 1, 32)    0           conv2d_4467[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4469 (LeakyReLU)    (None, 76, 1, 32)    0           conv2d_4469[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4470 (LeakyReLU)    (None, 76, 1, 32)    0           conv2d_4470[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_520 (Concatenate)   (None, 76, 1, 96)    0           leaky_re_lu_4467[0][0]           \n",
      "                                                                 leaky_re_lu_4469[0][0]           \n",
      "                                                                 leaky_re_lu_4470[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reshape_520 (Reshape)           (None, 76, 96)       0           concatenate_520[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_358 (Dropout)           (None, 76, 96)       0           reshape_520[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_358 (LSTM)                 (None, 64)           41216       dropout_358[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_357 (Dense)               (None, 3)            195         lstm_358[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 54,995\n",
      "Trainable params: 54,995\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.7742 - accuracy: 0.7300\n",
      "Epoch 2/250\n",
      "2498/2498 [==============================] - 76s 31ms/step - loss: 0.7495 - accuracy: 0.7344\n",
      "Epoch 3/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.7093 - accuracy: 0.7344\n",
      "Epoch 4/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.6605 - accuracy: 0.7342\n",
      "Epoch 5/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.6451 - accuracy: 0.7344\n",
      "Epoch 6/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.6385 - accuracy: 0.7355\n",
      "Epoch 7/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.6324 - accuracy: 0.7364\n",
      "Epoch 8/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.6296 - accuracy: 0.7370\n",
      "Epoch 9/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.6261 - accuracy: 0.7363\n",
      "Epoch 10/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.6229 - accuracy: 0.7380\n",
      "Epoch 11/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.6202 - accuracy: 0.7388\n",
      "Epoch 12/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.6170 - accuracy: 0.7386\n",
      "Epoch 13/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.6136 - accuracy: 0.7377\n",
      "Epoch 14/250\n",
      "2498/2498 [==============================] - 76s 31ms/step - loss: 0.6090 - accuracy: 0.7403\n",
      "Epoch 15/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.6056 - accuracy: 0.7420\n",
      "Epoch 16/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.6016 - accuracy: 0.7428\n",
      "Epoch 17/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5974 - accuracy: 0.7439\n",
      "Epoch 18/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5945 - accuracy: 0.7444\n",
      "Epoch 19/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5918 - accuracy: 0.7449\n",
      "Epoch 20/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5891 - accuracy: 0.7452\n",
      "Epoch 21/250\n",
      "2498/2498 [==============================] - 78s 31ms/step - loss: 0.5861 - accuracy: 0.7466\n",
      "Epoch 22/250\n",
      "2498/2498 [==============================] - 78s 31ms/step - loss: 0.5832 - accuracy: 0.7453\n",
      "Epoch 23/250\n",
      "2498/2498 [==============================] - 76s 31ms/step - loss: 0.5804 - accuracy: 0.7465\n",
      "Epoch 24/250\n",
      "2498/2498 [==============================] - 78s 31ms/step - loss: 0.5789 - accuracy: 0.7467\n",
      "Epoch 25/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5758 - accuracy: 0.7484\n",
      "Epoch 26/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5739 - accuracy: 0.7483\n",
      "Epoch 27/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5725 - accuracy: 0.7470\n",
      "Epoch 28/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5699 - accuracy: 0.7479\n",
      "Epoch 29/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5681 - accuracy: 0.7484\n",
      "Epoch 30/250\n",
      "2498/2498 [==============================] - 76s 31ms/step - loss: 0.5663 - accuracy: 0.7494\n",
      "Epoch 31/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5636 - accuracy: 0.7505\n",
      "Epoch 32/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5630 - accuracy: 0.7491\n",
      "Epoch 33/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5601 - accuracy: 0.7514\n",
      "Epoch 34/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5579 - accuracy: 0.7521\n",
      "Epoch 35/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5562 - accuracy: 0.7517\n",
      "Epoch 36/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5546 - accuracy: 0.7522\n",
      "Epoch 37/250\n",
      "2498/2498 [==============================] - 76s 31ms/step - loss: 0.5532 - accuracy: 0.7531\n",
      "Epoch 38/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5521 - accuracy: 0.7541\n",
      "Epoch 39/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5504 - accuracy: 0.7526\n",
      "Epoch 40/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5493 - accuracy: 0.7533\n",
      "Epoch 41/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5475 - accuracy: 0.7557\n",
      "Epoch 42/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5462 - accuracy: 0.7553\n",
      "Epoch 43/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5462 - accuracy: 0.7541\n",
      "Epoch 44/250\n",
      "2498/2498 [==============================] - 78s 31ms/step - loss: 0.5448 - accuracy: 0.7530\n",
      "Epoch 45/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5441 - accuracy: 0.7549\n",
      "Epoch 46/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5431 - accuracy: 0.7562\n",
      "Epoch 47/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5422 - accuracy: 0.7560\n",
      "Epoch 48/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5411 - accuracy: 0.7575\n",
      "Epoch 49/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5403 - accuracy: 0.7571\n",
      "Epoch 50/250\n",
      "2498/2498 [==============================] - 76s 31ms/step - loss: 0.5399 - accuracy: 0.7565\n",
      "Epoch 51/250\n",
      "2498/2498 [==============================] - 76s 31ms/step - loss: 0.5395 - accuracy: 0.7566\n",
      "Epoch 52/250\n",
      "2498/2498 [==============================] - 76s 31ms/step - loss: 0.5378 - accuracy: 0.7570\n",
      "Epoch 53/250\n",
      "2498/2498 [==============================] - 76s 31ms/step - loss: 0.5379 - accuracy: 0.7566\n",
      "Epoch 54/250\n",
      "2498/2498 [==============================] - 76s 31ms/step - loss: 0.5369 - accuracy: 0.7583\n",
      "Epoch 55/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5363 - accuracy: 0.7582\n",
      "Epoch 56/250\n",
      "2498/2498 [==============================] - 76s 31ms/step - loss: 0.5350 - accuracy: 0.7585\n",
      "Epoch 57/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5358 - accuracy: 0.7587\n",
      "Epoch 58/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5352 - accuracy: 0.7585\n",
      "Epoch 59/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5343 - accuracy: 0.7575\n",
      "Epoch 60/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5333 - accuracy: 0.7604\n",
      "Epoch 61/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5332 - accuracy: 0.7594\n",
      "Epoch 62/250\n",
      "2498/2498 [==============================] - 76s 31ms/step - loss: 0.5327 - accuracy: 0.7597\n",
      "Epoch 63/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5321 - accuracy: 0.7591\n",
      "Epoch 64/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5313 - accuracy: 0.7590\n",
      "Epoch 65/250\n",
      "2498/2498 [==============================] - 76s 31ms/step - loss: 0.5316 - accuracy: 0.7597\n",
      "Epoch 66/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5307 - accuracy: 0.7589\n",
      "Epoch 67/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5304 - accuracy: 0.7596\n",
      "Epoch 68/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5308 - accuracy: 0.7595\n",
      "Epoch 69/250\n",
      "2498/2498 [==============================] - 76s 31ms/step - loss: 0.5300 - accuracy: 0.7588\n",
      "Epoch 70/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5292 - accuracy: 0.7611\n",
      "Epoch 71/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5286 - accuracy: 0.7604\n",
      "Epoch 72/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5287 - accuracy: 0.7606\n",
      "Epoch 73/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5280 - accuracy: 0.7605\n",
      "Epoch 74/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5277 - accuracy: 0.7604\n",
      "Epoch 75/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5277 - accuracy: 0.7620\n",
      "Epoch 76/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5277 - accuracy: 0.7611\n",
      "Epoch 77/250\n",
      "2498/2498 [==============================] - 76s 31ms/step - loss: 0.5259 - accuracy: 0.7611\n",
      "Epoch 78/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.5263 - accuracy: 0.7623\n",
      "Epoch 79/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.5259 - accuracy: 0.7618\n",
      "Epoch 80/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5257 - accuracy: 0.7621\n",
      "Epoch 81/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.5248 - accuracy: 0.7623\n",
      "Epoch 82/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.5251 - accuracy: 0.7627\n",
      "Epoch 83/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.5242 - accuracy: 0.7642\n",
      "Epoch 84/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.5249 - accuracy: 0.7621\n",
      "Epoch 85/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.5238 - accuracy: 0.7626\n",
      "Epoch 86/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.5234 - accuracy: 0.7619\n",
      "Epoch 87/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5238 - accuracy: 0.7622\n",
      "Epoch 88/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5227 - accuracy: 0.7636\n",
      "Epoch 89/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.5223 - accuracy: 0.7645\n",
      "Epoch 90/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5219 - accuracy: 0.7645\n",
      "Epoch 91/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5218 - accuracy: 0.7633\n",
      "Epoch 92/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.5213 - accuracy: 0.7640\n",
      "Epoch 93/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5217 - accuracy: 0.7640\n",
      "Epoch 94/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5201 - accuracy: 0.7637\n",
      "Epoch 95/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5204 - accuracy: 0.7638\n",
      "Epoch 96/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5207 - accuracy: 0.7639\n",
      "Epoch 97/250\n",
      "2498/2498 [==============================] - 76s 31ms/step - loss: 0.5197 - accuracy: 0.7647\n",
      "Epoch 98/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5199 - accuracy: 0.7646\n",
      "Epoch 99/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5195 - accuracy: 0.7649\n",
      "Epoch 100/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5192 - accuracy: 0.7646\n",
      "Epoch 101/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5182 - accuracy: 0.7656\n",
      "Epoch 102/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5185 - accuracy: 0.7648\n",
      "Epoch 103/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5177 - accuracy: 0.7652\n",
      "Epoch 104/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5179 - accuracy: 0.7651\n",
      "Epoch 105/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.5180 - accuracy: 0.7645\n",
      "Epoch 106/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.5175 - accuracy: 0.7656\n",
      "Epoch 107/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.5169 - accuracy: 0.7666\n",
      "Epoch 108/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.5174 - accuracy: 0.7661\n",
      "Epoch 109/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.5168 - accuracy: 0.7661\n",
      "Epoch 110/250\n",
      "2498/2498 [==============================] - 75s 30ms/step - loss: 0.5167 - accuracy: 0.7662\n",
      "Epoch 111/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5162 - accuracy: 0.7670\n",
      "Epoch 112/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5159 - accuracy: 0.7664\n",
      "Epoch 113/250\n",
      "2498/2498 [==============================] - 76s 31ms/step - loss: 0.5157 - accuracy: 0.7659\n",
      "Epoch 114/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5156 - accuracy: 0.7666\n",
      "Epoch 115/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5157 - accuracy: 0.7663\n",
      "Epoch 116/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5151 - accuracy: 0.7668\n",
      "Epoch 117/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5147 - accuracy: 0.7671\n",
      "Epoch 118/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5147 - accuracy: 0.7673\n",
      "Epoch 119/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5143 - accuracy: 0.7676\n",
      "Epoch 120/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5148 - accuracy: 0.7667\n",
      "Epoch 121/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5137 - accuracy: 0.7679\n",
      "Epoch 122/250\n",
      "2498/2498 [==============================] - 76s 31ms/step - loss: 0.5136 - accuracy: 0.7678\n",
      "Epoch 123/250\n",
      "2498/2498 [==============================] - 76s 31ms/step - loss: 0.5137 - accuracy: 0.7684\n",
      "Epoch 124/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5138 - accuracy: 0.7674\n",
      "Epoch 125/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5127 - accuracy: 0.7673\n",
      "Epoch 126/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5130 - accuracy: 0.7679\n",
      "Epoch 127/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5125 - accuracy: 0.7684\n",
      "Epoch 128/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5120 - accuracy: 0.7683\n",
      "Epoch 129/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5119 - accuracy: 0.7680\n",
      "Epoch 130/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5114 - accuracy: 0.7692\n",
      "Epoch 131/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5118 - accuracy: 0.7685\n",
      "Epoch 132/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5118 - accuracy: 0.7680\n",
      "Epoch 133/250\n",
      "2498/2498 [==============================] - 78s 31ms/step - loss: 0.5116 - accuracy: 0.7687\n",
      "Epoch 134/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5108 - accuracy: 0.7684\n",
      "Epoch 135/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5109 - accuracy: 0.7677\n",
      "Epoch 136/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5105 - accuracy: 0.7688\n",
      "Epoch 137/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5098 - accuracy: 0.7692\n",
      "Epoch 138/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5102 - accuracy: 0.7694\n",
      "Epoch 139/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5099 - accuracy: 0.7685\n",
      "Epoch 140/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5104 - accuracy: 0.7689\n",
      "Epoch 141/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5099 - accuracy: 0.7690\n",
      "Epoch 142/250\n",
      "2498/2498 [==============================] - 76s 31ms/step - loss: 0.5087 - accuracy: 0.7694\n",
      "Epoch 143/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5094 - accuracy: 0.7690\n",
      "Epoch 144/250\n",
      "2498/2498 [==============================] - 76s 31ms/step - loss: 0.5086 - accuracy: 0.7697\n",
      "Epoch 145/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5085 - accuracy: 0.7706\n",
      "Epoch 146/250\n",
      "2498/2498 [==============================] - 76s 31ms/step - loss: 0.5090 - accuracy: 0.7687\n",
      "Epoch 147/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5083 - accuracy: 0.7699\n",
      "Epoch 148/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5083 - accuracy: 0.7690\n",
      "Epoch 149/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5081 - accuracy: 0.7699\n",
      "Epoch 150/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5081 - accuracy: 0.7707\n",
      "Epoch 151/250\n",
      "2498/2498 [==============================] - 77s 31ms/step - loss: 0.5070 - accuracy: 0.7700\n",
      "Epoch 152/250\n",
      "2498/2498 [==============================] - 76s 31ms/step - loss: 0.5064 - accuracy: 0.7710\n",
      "Epoch 153/250\n",
      "2498/2498 [==============================] - 78s 31ms/step - loss: 0.5072 - accuracy: 0.7704\n",
      "Epoch 154/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2498/2498 [==============================] - 76s 31ms/step - loss: 0.5066 - accuracy: 0.7708\n",
      "Epoch 155/250\n",
      "2498/2498 [==============================] - 76s 31ms/step - loss: 0.5065 - accuracy: 0.7700\n",
      "Epoch 156/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5065 - accuracy: 0.7704\n",
      "Epoch 157/250\n",
      "2498/2498 [==============================] - 76s 30ms/step - loss: 0.5065 - accuracy: 0.7705\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00157: early stopping\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 250\n",
    "\n",
    "with device('cpu:0'):\n",
    "    for time_series in model_details:\n",
    "        model = model_details[time_series]['function'](time_series, \n",
    "                                           model_details[time_series]['lag_param'], \n",
    "                                           model_details[time_series]['dropout'])\n",
    "        model.summary()\n",
    "\n",
    "        model.fit(model_details[time_series]['train_x'], model_details[time_series]['train_y'], epochs=max_epochs, batch_size=batch_size, callbacks=[es])\n",
    "        model_details[time_series]['model'] = model\n",
    "\n",
    "        # save the trained model\n",
    "        model.save(time_series + '-CNN-LSTM-SAVED.hdf5', overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if already trained, we can load the saved models\n",
    "# from keras.models import load_model\n",
    "# for time_series in model_details:\n",
    "#     model_details[time_series]['model'] = load_model(time_series + '-CNN-LSTM-SAVED.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "As in any forecasting problem, our solution at each timestamp is the [conditional expectation](https://en.wikipedia.org/wiki/Conditional_expectation) of the data given the model's parameters. So to account for uncertainty in our model parameters, we use MC dropout to better approximate the conditional expectations at each observation by performing 100 forward passes on each model and storing the average. That is, as the number of out-of-sample predictions we execute grows very large, the average across those predictions will approach the true conditional probability of an outcome given the training data. We also want to store a vector of these predictions from each pass, as that information will be useful for model comparison and in constructing a trading strategy that utilizes uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 100\n",
    "\n",
    "with device('cpu:0'):\n",
    "    for time_series in model_details:\n",
    "        # Store out-of-sample predictions of model\n",
    "        model_details[time_series]['preds'] = [model_details[time_series]['model'].predict(model_details[time_series]['test_x']) for _ in range(num_iterations)]\n",
    "        model_details[time_series]['preds_mean'] = np.mean(model_details[time_series]['preds'], \n",
    "                                                           axis=0).reshape(model_details[time_series]['test_y'].shape) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OF\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down       0.40      0.25      0.30      3544\n",
      "  Stationary       0.77      0.90      0.83     12738\n",
      "          Up       0.59      0.45      0.51      3649\n",
      "\n",
      "   micro avg       0.70      0.70      0.70     19931\n",
      "   macro avg       0.58      0.53      0.55     19931\n",
      "weighted avg       0.67      0.70      0.68     19931\n",
      " samples avg       0.70      0.70      0.70     19931\n",
      "\n",
      "OFI\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down       0.38      0.12      0.18      3542\n",
      "  Stationary       0.77      0.89      0.83     12733\n",
      "          Up       0.47      0.53      0.50      3649\n",
      "\n",
      "   micro avg       0.69      0.69      0.69     19924\n",
      "   macro avg       0.54      0.51      0.50     19924\n",
      "weighted avg       0.65      0.69      0.65     19924\n",
      " samples avg       0.69      0.69      0.69     19924\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for time_series in model_details:\n",
    "    model_details[time_series]['pred_y'] = np.zeros(model_details[time_series]['test_y'].shape)\n",
    "    for i in range(len(model_details[time_series]['test_y'])):\n",
    "        model_details[time_series]['pred_y'][i,np.argmax(model_details[time_series]['preds_mean'][i])] = 1\n",
    "    print(time_series)\n",
    "    print(classification_report(model_details[time_series]['test_y'],\n",
    "                                model_details[time_series]['pred_y'],\n",
    "                                target_names=['Down', 'Stationary', 'Up']))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As anticipated, our model trained on order flow outperforms the model trained on order flow imbalance. It tends to predict downward moves better and upward moves only slightly better on the whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "fig, (ax1,ax2) = plt.subplots(1,2, sharex=True, sharey=True)\n",
    "\n",
    "mat_con1 = confusion_matrix(model_details['OF']['test_y'].argmax(axis=1), \n",
    "                           model_details['OF']['pred_y'].argmax(axis=1))\n",
    "mat_con2 = confusion_matrix(model_details['OFI']['test_y'].argmax(axis=1), \n",
    "                           model_details['OFI']['pred_y'].argmax(axis=1))\n",
    "\n",
    "for ax in zip((ax1,ax2),(mat_con1,mat_con2),('OF','OFI')):\n",
    "    ax[0].matshow(ax[1], cmap=plt.cm.Blues, alpha=0.5)\n",
    "    \n",
    "    for m in range(ax[1].shape[0]):\n",
    "        for n in range(ax[1].shape[1]):\n",
    "            ax[0].text(x=m, y=n, s=ax[1][m, n], va='center', ha='center', size='xx-large')\n",
    "\n",
    "    ax[0].set_xlabel('Predictions', fontsize=16)\n",
    "    ax[0].set_ylabel('True', fontsize=16)\n",
    "    plt.xticks(np.arange(3), ['Down', 'Stationary', 'Up'])\n",
    "    plt.yticks(np.arange(3), ['Down', 'Stationary', 'Up'], rotation='vertical', va='center')\n",
    "    ax[0].set_title(ax[2] + ' Confusion Matrix', fontsize=15)\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrices reinforce the fact that the order flow model makes better predictions. This is particularly the case when it comes to upward moves; the order flow model is more illiberal in its propensity to make upward predictions, but the predictions it does make are clearly more in line with the true moves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Trading Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, these evaluation metrics are cool and all, but is there a profitable trading strategy utilizing this model? How risky is it? And how can uncertainty information be used to inform trading decisions?\n",
    "\n",
    "To answer this, we compare two trading strategies using our test set as the trading period. We ignore transaction costs, close positions at the end of the trading period, and trade the mid price of the limit order book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the values we trade\n",
    "for time_series in model_details:\n",
    "    model_details[time_series]['test_mid'] = (lob_data.iloc[1+int(len(model_details[time_series]['data'])*train_weight):]['PRICE_ASK_0'] \n",
    "                  + lob_data.iloc[1+int(len(model_details[time_series]['data'])*train_weight):]['PRICE_BID_0']) / 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Trading Strategy\n",
    "For this strategy, we choose a threshold probability $\\alpha$ and go long if $\\hat{p}_{1,t}>\\alpha$ and go short if $\\hat{p}_{-1,t}>\\alpha$, where $\\hat{p}_{1,t}$ is the predicted probability of the mid price increasing at time $t$ and $\\hat{p}_{-1,t}$ is the predicted probability of the mid price decreasing at time $t$. We store the cumulative profits and their ratios to transaction volume ratios for a few threshold values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.7,0.8,0.9]\n",
    "\n",
    "for time_series in model_details:\n",
    "    for a in alphas:\n",
    "        \n",
    "        position = 0\n",
    "        entry_last = None\n",
    "        pnl = 0\n",
    "        num_trades = 0\n",
    "        \n",
    "        for t in range(len(model_details[time_series]['preds_mean'])):\n",
    "\n",
    "            if np.argmax(model_details[time_series]['preds_mean'][t]) == 2 and np.max(model_details[time_series]['preds_mean'][t]) > a:\n",
    "                # Increase long position\n",
    "                position += 1\n",
    "                entry_now = model_details[time_series]['test_mid'].iloc[t]\n",
    "                if entry_last != None:\n",
    "                    pnl += entry_now - entry_last\n",
    "                entry_last = entry_now\n",
    "                num_trades += 1\n",
    "\n",
    "            elif np.argmax(model_details[time_series]['preds_mean'][t]) == 0 and np.max(model_details[time_series]['preds_mean'][t]) > a:\n",
    "                # Increase short position\n",
    "                position -= 1\n",
    "                entry_now = model_details[time_series]['test_mid'].iloc[t]\n",
    "                if entry_last != None:\n",
    "                    pnl += entry_last - entry_now\n",
    "                entry_last = entry_now\n",
    "                num_trades += 1\n",
    "                \n",
    "        # close position\n",
    "        if position > 0:\n",
    "            # sell to close\n",
    "            entry_now = model_details[time_series]['test_mid'].iloc[-1]\n",
    "            pnl += position * (entry_last - entry_now)\n",
    "            num_trades += 1\n",
    "        elif position < 0:\n",
    "            # buy to close\n",
    "            entry_now = model_details[time_series]['test_mid'].iloc[-1]\n",
    "            pnl += -position * (entry_now - entry_last)\n",
    "            num_trades += 1\n",
    "\n",
    "        model_details[time_series]['softmax_profits_{}'.format(a)] = pnl\n",
    "        model_details[time_series]['softmax_profits_normalized_{}'.format(a)] = pnl/num_trades\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Trading Strategy\n",
    "This strategy borrows BDLOB's use of *predictive [entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory))* to summarize classification uncertainty due to variational dropout[<sub>[2]</sub>](#ref2). The metric of predictive entropy $\\mathbb{H}$ follows from our aforementioned understanding of conditional expectation by making use of the predictive distribution captured in our 100 forward passes from earlier. For an input $x_t$, a predicted output $y_t$, training data $\\mathcal{D}_{\\text{train}}$, and estimated model parameters $\\hat{w}$, we define predictive entropy by\n",
    "$$\n",
    "\\begin{equation}\n",
    "     \\begin{aligned}\n",
    "     \\mathbb{H}(y_{t}|x_{t},\\mathcal{D}_{\\text{train}}) &= -\\sum_{j=-1}^{1} p(y_{t}=j|x_{t},\\mathcal{D}_{\\text{train}})\\log p(y_{t}=j|x_{t},\\mathcal{D}_{\\text{train}}) \\\\\n",
    "    &\\approx -\\sum_{j=-1}^{1} \\left( \\frac{1}{100}\\sum_{k=1}^{100} p(y_{t}=j|x_{t},\\hat{w})\\right) \\log \\left(\\frac{1}{100}\\sum_{k=1}^{100} p(y_{t}=j|x_{t},\\hat{w})\\right) \\\\\n",
    "    &=: \\tilde{\\mathbb{H}}_t.\n",
    "    \\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Essentially, $j$ iterates over each class and summarizes the average level of uncertainty for outcomes of that class. The function is minimized when the model is certain; one class has probability 1 and all others are 0. The function is maximized when the model is very uncertain; probability is uniform across the classes. Also observe that our earlier notation $\\hat{p}_{j,t}$ is shorthand for $\\frac{1}{100}\\sum_{k=1}^{100} p(y_{t}=j|x_{t},\\hat{w})$.\n",
    "\n",
    "Using this metric, we upsize our positions if our model is certain and downsize our positions if the model is uncertain. More specifically, we still go long or short if $\\hat{p}_{1,t}>\\alpha$ or $\\hat{p}_{-1,t}>\\alpha$, respectively, but we upsize our positions to $1.5 \\times \\mu$ if $\\tilde{\\mathbb{H}}_t<\\beta_1$, keep our size $\\mu$ if $\\beta_1< \\tilde{\\mathbb{H}}_t < \\beta_2$, and downsize to $0.5 \\times \\mu$ if $\\tilde{\\mathbb{H}}_t>\\beta_2$. Like in BDLOB, we fix values for $\\alpha$ and $\\beta_1$ and test different values for $\\beta_2$[<sub>[2]</sub>](#ref2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.7\n",
    "beta1 = 0.3\n",
    "beta2 = [0.6,0.7,0.8]\n",
    "\n",
    "for time_series in model_details:\n",
    "    for b2 in beta2:\n",
    "        \n",
    "        long_position = 0\n",
    "        short_position = 0\n",
    "        entry_last = None\n",
    "        pnl = 0\n",
    "        num_trades = 0\n",
    "        \n",
    "        for t in range(len(model_details[time_series]['preds_mean'])):\n",
    "            \n",
    "            entropy = -np.sum(np.max(model_details[time_series]['preds_mean'][t])*\n",
    "                              np.log(np.max(model_details[time_series]['preds_mean'][t])))\n",
    "            \n",
    "            if np.argmax(model_details[time_series]['preds_mean'][t]) == 2 and np.max(model_details[time_series]['preds_mean'][t]) > threshold:\n",
    "                entry_now = model_details[time_series]['test_mid'].iloc[t]\n",
    "                if beta1 < entropy < b2:\n",
    "                    # enter long\n",
    "                    long_position += 1\n",
    "                    if entry_last != None:\n",
    "                        pnl += entry_now - entry_last\n",
    "                    entry_last = entry_now\n",
    "                    num_trades += 1\n",
    "                    \n",
    "                elif entropy < beta1:\n",
    "                    # enter upsized long\n",
    "                    long_position += 1.5\n",
    "                    if entry_last != None:\n",
    "                        pnl += 1.5 * (entry_now - entry_last)\n",
    "                    entry_last = entry_now\n",
    "                    num_trades += 1\n",
    "                \n",
    "                else: # entropy > b2\n",
    "                    # enter downsized long\n",
    "                    long_position += 0.5\n",
    "                    if entry_last != None:\n",
    "                        pnl += 0.5 * (entry_now - entry_last)\n",
    "                    entry_last = entry_now\n",
    "                    num_trades += 1\n",
    "            \n",
    "            elif np.argmax(model_details[time_series]['preds_mean'][t]) == 0 and np.max(model_details[time_series]['preds_mean'][t]) > threshold:\n",
    "                entry_now = model_details[time_series]['test_mid'].iloc[t]\n",
    "                if beta1 < entropy < b2:\n",
    "                    # enter short\n",
    "                    short_position += 1\n",
    "                    if entry_last != None:\n",
    "                        pnl += entry_last - entry_now\n",
    "                    entry_last = entry_now\n",
    "                    num_trades += 1\n",
    "                    \n",
    "                elif entropy < beta1:\n",
    "                    # enter upsized short\n",
    "                    short_position += 1.5\n",
    "                    if entry_last != None:\n",
    "                        pnl += 1.5 * (entry_last - entry_now)\n",
    "                    entry_last = entry_now\n",
    "                    num_trades += 1\n",
    "                    \n",
    "                else: # entropy > b2\n",
    "                    # enter downsized short\n",
    "                    short_position += 0.5\n",
    "                    if entry_last != None:\n",
    "                        pnl += 0.5 * (entry_last - entry_now)\n",
    "                    entry_last = entry_now\n",
    "                    num_trades += 1\n",
    "                              \n",
    "        # close position\n",
    "        if long_position - short_position > 0:\n",
    "            # sell to close\n",
    "            entry_now = model_details[time_series]['test_mid'].iloc[-1]\n",
    "            pnl += (long_position - short_position) * (entry_last - entry_now)\n",
    "            num_trades += 1\n",
    "        elif long_position - short_position < 0:\n",
    "            # buy to close\n",
    "            entry_now = model_details[time_series]['test_mid'].iloc[-1]\n",
    "            pnl += (short_position - long_position) * (entry_now - entry_last)\n",
    "            num_trades += 1\n",
    "        \n",
    "        model_details[time_series]['bayesian_profits_({b1},{b2})'.format(b1=beta1,b2=b2)] = pnl\n",
    "        model_details[time_series]['bayesian_profits_normalized_({b1},{b2})'.format(b1=beta1,b2=b2)] = pnl/num_trades\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Now comes the question of how we should compare these strategies in terms of profit and risk. \n",
    "\n",
    "Since each strategy returns different transaction volumes, we use the ratio of profit to transaction volume to compare profitability. And while the [Sharpe ratio](https://en.wikipedia.org/wiki/Sharpe_ratio) is a popular measure of risk in a portfolio or strategy, it deems large positive and negative returns to be equally risky, so we follow BDLOB in using the Downward Deviation Ratio $ \\text{DDR} = \\frac{E(R_t)}{\\text{DD}_T} $ as our risk measure, where $E(R_t)$ is the average return per timestamp and $\\text{DD}_T=\\sqrt{\\frac{1}{T} \\sum_{t=1}^T \\text{min}(R_t,0)^2}$[<sub>[2]</sub>](#ref2). DDR, which is essentially the [Sortino ratio](https://en.wikipedia.org/wiki/Sortino_ratio) with a target rate of 0, has the desired property of penalizing negative returns and rewarding positive returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "labels = ['softmax_{}'.format(a) for a in alphas] + ['bayesian_{,}'.format(beta1,b2) for b2 in beta2]\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "# Normalized Profit Plots\n",
    "of_profits_normalized = [model_details['OF']['softmax_profits_normalized_{}'.format(a)] for a in alphas] +\n",
    "            [model_details['OF']['bayesian_profits_normalized_({,})'.format(beta1,b2)] for b2 in beta2]\n",
    "\n",
    "ofi_profits_normalized = [model_details['OFI']['softmax_profits_normalized_{}'.format(a)] for a in alphas] +\n",
    "            [model_details['OFI']['bayesian_profits_normalized_({,})'.format(beta1,b2)] for b2 in beta2]\n",
    "\n",
    "rects1 = ax1.bar(x - width/2, of_profits_normalized, width, label='OF')\n",
    "rects2 = ax1.bar(x + width/2, ofi_profits_normalized, width, label='OFI')\n",
    " \n",
    "ax1.set_ylabel('Normalized Profit')\n",
    "ax1.set_title('Normalized Profit by Strategy and Model')\n",
    "ax1.set_xticks(x, labels)\n",
    "ax1.legend()\n",
    "\n",
    "ax1.bar_label(rects1, padding=3)\n",
    "ax1.bar_label(rects2, padding=3)\n",
    "\n",
    "\n",
    "# DDR Plots\n",
    "of_ddr = [model_details['OF']['softmax_profits_normalized_{}'.format(a)] for a in alphas] +\n",
    "            [model_details['OF']['bayesian_profits_normalized_({,})'.format(beta1,b2)] for b2 in beta2]\n",
    "\n",
    "ofi_ddr = [model_details['OFI']['softmax_profits_normalized_{}'.format(a)] for a in alphas] +\n",
    "            [model_details['OFI']['bayesian_profits_normalized_({,})'.format(beta1,b2)] for b2 in beta2]\n",
    "\n",
    "rects1 = ax2.bar(x - width/2, of_ddr, width, label='OF')\n",
    "rects2 = ax2.bar(x + width/2, ofi_ddr, width, label='OFI')\n",
    "\n",
    "ax2.set_ylabel('DDR')\n",
    "ax2.set_title('DDR by Strategy and Model')\n",
    "ax2.set_xticks(x, labels)\n",
    "ax2.legend()\n",
    "\n",
    "ax2.bar_label(rects1, padding=3)\n",
    "ax2.bar_label(rects2, padding=3)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_details['OF']['test_mid'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Diagnostics\n",
    "To be in acccordance with the Box-Jenkins approach, we test the fitted models' residuals $\\left\\{\\hat{u}_i\\right\\}_{i=1}^{T}$ for any [autocorrelation](https://en.wikipedia.org/wiki/Autocorrelation)—that is, we check whether residuals are correlated to past residuals in the sequence. If true, there is statistical evidence that the model is underfitting, and we should increase the lag parameter of our sequential model and re-train. If false, we accept the model residuals to be [white noise](https://en.wikipedia.org/wiki/White_noise). \n",
    "\n",
    "We compute our residuals as the [cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy) of the classification problem at each timestamp, which we define by\n",
    "$$ \\hat{u}_i=-\\sum_{j=-1}^{1}y_i(j)\\log\\hat{y}_i(j) $$\n",
    "for $i\\in\\left\\{1,...,T\\right\\}$, where $y_i$ is the [one-hot encoded](https://en.wikipedia.org/wiki/One-hot#Machine_learning_and_statistics) 3-variable vector of the true 1-step movement, $\\hat{y}_i$ is our model's unrounded prediction of that encoding, and $T$ is the number of observations.\n",
    "\n",
    "Letting $\\hat{\\tau}_i$ be the sample autocorrelations of the residuals and $m$ to be a maximum lag to test, we use the Ljung-Box statistic\n",
    "$$ Q(m) = T(T+2)\\sum_{l=1}^{m}\\frac{\\hat{\\tau}_l^2}{T-l} $$\n",
    "as our test statistic for the null hypothesis $H_0: \\tau_1=...=\\tau_m=0$ versus the alternative $H_a: \\tau_i\\neq0$ for some $i\\in\\left\\{1,...,m\\right\\}$. For large $T$, the statistic is chi-squared distributed with $m$ degrees of freedom, and we reject the null in favor of the alternative if the test statistic is greater than the critical value of the corresponding chi-squared distribution at the 99% confidence level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OF is underfitting.\n",
      "OFI is underfitting.\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "with device('cpu:0'):\n",
    "    for time_series in model_details:\n",
    "        residuals = -np.sum(model_details[time_series]['train_y']*\n",
    "                           np.log(model_details[time_series]['model'].predict(model_details[time_series]['train_x'])),\n",
    "                           axis=1)\n",
    "        lb, p = acorr_ljungbox(residuals, lags=100, boxpierce=False, return_df=False)\n",
    "        if len(p[p < 0.01]) > 0:\n",
    "            print(time_series + \" is underfitting.\")\n",
    "        else:\n",
    "            print(time_series + \" is not underfitting.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we conclude that both models are underfitting the training data, so we should increase the lag parameter and redo the training and diagnostics until we no longer underfit. However, we don't pursue this route for a pair of reasons. For one, while there is no risk of overfitting the deep learning model as we increase the lag, doing so would increase computational time. With the current lag, iterating through the cross-validation and training takes many many hours already, and I'm antsy to finish this project. Secondly, the fact that the model is underfitting should come as no surprise, since this really tells us that our model is too simple for the data. Financial data, even such stationary processes as order flow and order flow imbalance, are incredibly complex, so it's hard to expect any interpretable model to well-fit the input data. In this truth lies one of the problems of deep learning in finance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortfalls of Deep Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep neural networks as a \"black box\"\n",
    "\n",
    "\"Offline\" version of learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <span id=\"ref1\">Zhang, Zihao, Stefan Zohren, and Stephen Roberts (2019). “DeepLOB: Deep Convolutional Neural Networks for Limit Order Books”. In: *IEEE Transactions On Signal Processing* 67.11, pp. 3001–3012.<br>\n",
    "2. <span id=\"ref2\">Zhang, Zihao, Stefan Zohren, and Stephen Roberts (2018). “BDLOB: Bayesian Deep Convolutional Neural Networks For Limit Order Books”. In: *arXiv preprint arXiv:1811.10041*.<br>\n",
    "3. <span id=\"ref3\">Cont, Rama, Arseniy Kukanov, and Sasha Stoikov (2014). “The Price Impact Of\n",
    "Order Book Events”. In: *Journal Of Financial Econometrics* 12.1, pp. 47–88.<br>\n",
    "4. <span id=\"ref4\">Kolm, Petter, Jeremy Turiel, and Nicholas Westray (2021). \"Deep Order Flow Imbalance: Extracting Alpha At Multiple Horizons From The Limit Order Book\". In: *Available at SSRN 3900141*.<br>\n",
    "5. <span id=\"ref5\">Lütkepohl, Helmut (2005). *New Introduction to Multiple Time Series Analysis*. Springer.<br> \n",
    "6. <span id=\"ref6\">Dixon, Matthew F., Igor Halperin, and Paul Bilokon (2020). *Machine Learning in Finance: From Theory to Practice*. Springer.<br>\n",
    "7. <span id=\"ref7\">Hochreiter, Sepp and Jürgen Schmidhuber (1997). “Long Short-Term Memory”. In: *Neural Computation* 9.8, pp. 1735–1780.<br>\n",
    "8. <span id=\"ref8\">Ntakaris, Adamantios, Martin Magris, Juho Kanniainen, Moncef Gabbouj, and Alexandros Iosifidis (2018). “Benchmark Dataset For Mid-Price Forecasting Of Limit Order Book Data With Machine Learning Methods”. In: *Journal of Forecasting* 37.8, pp. 852–866.<br>\n",
    "9. <span id=\"ref9\">Yang, Kiyoung, and Cyrus Shahabi (2005). \"On the Stationarity of Multivariate Time Series for Correlation-Based Data Analysis\". In: *Fifth IEEE International Conference on Data Mining*, pp. 1-4.<br>\n",
    "10. <span id=\"ref10\">Gal, Yarin (2016). *Uncertainty in Deep Learning*. Phd Thesis, University of Cambridge.<br>\n",
    "\n",
    "- Szegedy, Christian, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich (2015). “Going Deeper With Convolutions”. In: *Proceedings Of The IEEE Conference On Computer Vision And Pattern Recognition*, pp. 1–9.<br>\n",
    "\n",
    "- Tsay, Ruey S. (2010). *Analysis of Financial Time Series* (3rd ed.). Wiley.<br>\n",
    "- Johansen, Søren (1995). *Likelihood-Based Inference in Cointegrated Vector Autoregressive Models*. Oxford University Press.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b6c5c2265085fc676d0e3932a49fa51d673497428dc888174fbb13fc54974a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
