{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Deep Learning Approach to Limit Order Book Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Aric Cutuli<br>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening Remarks\n",
    "The enigma concerning the predictability of markets has always been the principal driver of my interest in finance, and it inpires my ongoing exploration of machine learning's applications within the analysis and forecasting of financial time series. Today, we compare the performance of a studied deep learning model for limit order book forecasting on two stationary representations of the limit order book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "In this notebook, we implement an artificial neural network originally employed by (Zhang et al) that combines convolutional neural networks (CNNs) and a long short-term memory (LSTM) neural network in order to classify future directions of an order book at a high frequency. Specifically, given Coinbase order book data for Bitcoin, we seek to predict whether the mid price increases, decreases, or does not change in the next observation of the time series. Unlike the works of (Zhang et al), which use raw non-stationary order book states as inputs to the network, our instantiation of the architecture is trained on order flow and order flow imbalance, which are stationary quantities derived from the limit order book. Hence, this discussion also draws heavy inspiration from a recent article (Kolm et al), which demonstrated that forecasting using order flow significantly outperforms the model trained using raw order book inputs. Today, we further this discussion by demonstrating the increased forecasting flexibility that order flow offers over order flow imbalance, and we observe the model's efficacy under a pair of trading strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order Books, Flow, and Imbalance\n",
    "Today's trading of financial instruments is often facilitated by a *limit order book*, also known as an *order book*. The order book collects bids and offers made by prospective buyers and sellers and determines which incoming orders get executed and which are added to the book. The *bid price* is the highest price buyers are prepared to buy at, and the *ask price* is the lowest price sellers are willing to sell at. The *mid price*, which our model will seek to predict moves in, is the midpoint of the bid price and the ask price.\n",
    "\n",
    "An order is defined by its side, quantity demanded, price to trade at, and time of submission. As one enters the system, the matching engine of the exchange seeks to match the order with existing orders in the book. Orders that match are executed and called *market orders*, and orders that do not match or only partially match are added to the book and called *limit orders*.\n",
    "\n",
    "<img src='Images/Limit-order-book-diagram-A-new-buy-limit-order-arrives-at-price-bt-increasing-the.png' style='width:425px;height:312px'/>\n",
    "\n",
    "Our model takes as inputs representations of the first ten levels of the order book. A level is denoted by its price and volume that is bid or asked. So, as we progress down levels on the bid side of the order book, the price decreases, and as we progress down levels of the ask side, the price increases. Each observation in our dataset will be a $40$-variable vector displaying the price and volume for each of the top ten bid and ask levels, giving us a truncated screenshot of the *state of the limit order book* at each timestep. \n",
    "\n",
    "$$ \\text{s}_t^{LOB} := (a_t^1, v_t^{1,a}, b_t^1, v_t^{1,b}, ..., a_t^{10}, v_t^{10,a}, b_t^{10}, v_t^{10,b})^T \\in \\mathbb{R}^{40} $$\n",
    "\n",
    "We define the *bid order flows* (bOF) and *ask order flows* (aOF) at a timestamp to be 10-variable vectors computed using two consecutive order book states, where each element is given by\n",
    "\n",
    "$$ \\text{bOF}_{t,i} :=   \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      v_t^{i,b}, & b_t^i > b_{t-1}^i \\\\\n",
    "      v_t^{i,b} - v_{t-1}^{i,b}, & b_t^i = b_{t-1}^i \\\\\n",
    "      -v_t^{i,b}, & b_t^i < b_{t-1}^i \\\\\n",
    "\\end{array} \n",
    "\\right. $$\n",
    "$$ \\text{aOF}_{t,i} :=   \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      -v_t^{i,a}, & a_t^i > a_{t-1}^i \\\\\n",
    "      v_t^{i,a} - v_{t-1}^{i,a}, & a_t^i = a_{t-1}^i \\\\\n",
    "      v_t^{i,a}, & a_t^i < a_{t-1}^i \\\\\n",
    "\\end{array} \n",
    "\\right. $$\n",
    "\n",
    "for $i = 1, ..., 10$. With this, we define *order flow* (OF)\n",
    "\n",
    "$$ \\text{OF}_t :=  (\\text{bOF}_{t,1}, \\text{aOF}_{t,1}, ..., \\text{bOF}_{t,10}, \\text{aOF}_{t,10})^T \\in \\mathbb{R}^{20} $$\n",
    "\n",
    "and *order flow imbalance* (OFI)\n",
    "\n",
    "$$ \\text{OFI}_t := \\text{bOF}_t - \\text{aOF}_t \\in \\mathbb{R}^{10}. $$\n",
    "\n",
    "While a sequence of limit order book states is a complex non-stationary process, the above formulas for order flow and order flow imbalance are able to transform consecutive order book states into a stationary process. This property allows for our eventual test test of the deep learning model to be reasonably similar to the training set and thus appropriate to predict off of using the model. It also allows for more ease in the learning of long-term dependencies by our LSTM layer. On a separate note, when trained on order flow, which keeps the bid and ask sides separate, the CNN layers of our model will be given the added flexibility of being able to combine bid and ask order flows asymmetrically, so we hypothesize that our forecasting model will perform better on order flow than on order flow imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the CNN-LSTM Model\n",
    "While the universal approximation theorem states that a feedforward neural network with a single hidden layer can approximate any continuous function on any compact set, different neural network architectures are capable of exploiting unique structures in data, a quality that is particularly useful for the learning of complex financial time series. These different architectures, which include CNNs and LSTMs among others, compress data and change their behavior over time in a way that supports their efficacy in difficult modeling situations, while feedforward neural networks can suffer from instability and less interpretability when seeking to learn complex relationships in data.\n",
    "\n",
    "Although artificial neural networks can be used individually, they are often complementary in their modeling capabilities and, when used together, can learn unique structures in data and improve a model's ability to execute a desired task. For instance, our CNN-LSTM model architecture, which we borrow from Kolm et al, consists of a CNN and an Inception Module that compresses and wraps the order book data in a manner that allows a LSTM Module to learn temporal dependencies from a smaller parameter space, leading to a more parsimonious model.\n",
    "\n",
    "*Convolutional neural networks* (CNNs) are feedforward neural networks that can exploit data locality in an input, so in the CNN-LSTM model, CNN layers reduce the dimension of the multivariate input by aggregating bid and ask sides and levels in the order book. The output of these convolutional layers serve as an input to the *Inception Module*, which wraps convolutions together to capture behavior over multiple timescales, acting as a sort of moving average indicator whose decay weights are learned via back-propagation. Then, the outputs of the Inception Module are concatenated and reshaped into an input to the *long short-term memory* (LSTM) neural network. LSTMs are a class of *recurrent neural networks* (RNNs) that are designed to handle temporal dependencies in sequential data and alleviate the vanishing gradient problem faced by generic RNNs. The LSTM unit consists of a memory cell and three gates that determine what information should be remembered by the memory cell. For an $ n $-dimensional input vector $\\text{x}_t$, the LSTM unit is defined by\n",
    "\n",
    "$$ \\text{f}_t = \\sigma (\\text{U}^f \\text{x}_t + \\text{W}^f \\text{h}_{t-1} + \\text{b}^f) $$\n",
    "$$ \\text{i}_t = \\sigma (\\text{U}^i \\text{x}_t + \\text{W}^i \\text{h}_{t-1} + \\text{b}^i) $$\n",
    "$$ \\text{o}_t = \\sigma (\\text{U}^o \\text{x}_t + \\text{W}^o \\text{h}_{t-1} + \\text{b}^o) $$\n",
    "$$ \\text{c}_t = \\text{f}_t \\circ \\text{c}_{t-1} + \\text{i}_t \\circ \\text{tanh} (\\text{U}^c \\text{x}_t + \\text{W}^c \\text{h}_{t-1} + \\text{b}^c) $$\n",
    "$$ \\text{h}_t = \\text{o}_t \\circ \\text{tanh} (\\text{c}_t) $$\n",
    "\n",
    "where $ m $ is the number of LSTM units in the module, $ \\sigma := (1+e^{-x})^{-1} $ is the sigmoid activation function, $ \\text{f}_t \\in \\mathbb{R}^m $ is the forget gate's activation vector, $ \\text{i}_t \\in \\mathbb{R}^m $ is the input gate's activation vector, $ \\text{o}_t \\in \\mathbb{R}^m $ is the output gate's activation vector, $ \\text{c}_t \\in \\mathbb{R}^m $ is the LSTM unit's hidden state vector, and $ \\text{h}_t \\in \\mathbb{R}^m $ is the unit's output vector. $ \\text{U} \\in \\mathbb{R}^{m \\times n} $, $ \\text{W} \\in \\mathbb{R}^{m \\times m} $, and $ \\text{b} \\in \\mathbb{R}^m $ are learned during training and represent the weight matrices in connection to the input vector, the weight matrices in connection to the previous output state, and the bias vectors, respectively. \n",
    "\n",
    "<img src='Images/lstm.png' style='width:500px;height:390px'/>\n",
    "\n",
    "Moreover, Zhang et al showcase the performance benefit of applying variational dropout to the model by serving as a stochastic regularizer to reduce overfitting. The dropout layer is inserted after the Inception Module, and we determine its rate with cross-validated grid-search. Lastly, since we formulate this forecasting problem as one of classification, we add an output layer with a softmax activation function, resulting in a final output whose elements represent the probability of observing each price movement in the next timestamp. We use the Adaptive Moment Estimation algorithm (ADAM) as our choice of optimization algorithm and set the parameters to those of the DeepLOB implementation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input, layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def CNN_LSTM(input_type, lag_order, dropout):\n",
    "    \n",
    "    # Convolutions across LOB levels\n",
    "    if input_type == 'OF':\n",
    "        input_tensor = Input(shape=(lag_order,20,1))\n",
    "        # Combine imbalance information across sides for each level of the order book\n",
    "        layer_x = layers.Conv2D(16, (1,2))(input_tensor)\n",
    "        layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "        # Combine imbalance information across time for each side and level of the order book\n",
    "        layer_x = layers.Conv2D(16, (4,1), padding='same')(input_tensor)\n",
    "        layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "        layer_x = layers.Conv2D(16, (4,1), padding='same')(layer_x)\n",
    "        layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "    elif input_type == 'OFI':\n",
    "        input_tensor = Input(shape=(lag_order,10,1))\n",
    "        # Combine imbalance information across time for each side and level of the order book\n",
    "        layer_x = layers.Conv2D(16, (4,1), padding='same')(input_tensor)\n",
    "        layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "        layer_x = layers.Conv2D(16, (4,1), padding='same')(layer_x)\n",
    "        layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "    else:\n",
    "        raise Exception(\"input_type should be 'OF' or 'OFI'\")\n",
    "\n",
    "    # Combine imbalance information across all levels of the book\n",
    "    layer_x = layers.Conv2D(16, (1,10))(layer_x)\n",
    "    layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "\n",
    "    # Inception Module\n",
    "    # Tower 1\n",
    "    tower_1 = layers.Conv2D(32, (1,1), padding='same')(layer_x)\n",
    "    tower_1 = layers.LeakyReLU(alpha=0.01)(tower_1)\n",
    "    tower_1 = layers.Conv2D(32, (3,1), padding='same')(tower_1)\n",
    "    tower_1 = layers.LeakyReLU(alpha=0.01)(tower_1)\n",
    "    # Tower 2\n",
    "    tower_2 = layers.Conv2D(32, (1,1), padding='same')(layer_x)\n",
    "    tower_2 = layers.LeakyReLU(alpha=0.01)(tower_2)\n",
    "    tower_2 = layers.Conv2D(32, (5,1), padding='same')(tower_2)\n",
    "    tower_2 = layers.LeakyReLU(alpha=0.01)(tower_2)  \n",
    "    # Tower 3\n",
    "    tower_3 = layers.MaxPooling2D((3,1), padding='same', strides=(1,1))(layer_x)\n",
    "    tower_3 = layers.Conv2D(32, (1,1), padding='same')(tower_3)\n",
    "    tower_3 = layers.LeakyReLU(alpha=0.01, )(tower_3)\n",
    "\n",
    "    # Concatenation and reshaping\n",
    "    layer_x = layers.concatenate([tower_1, tower_2, tower_3], axis=-1)\n",
    "    layer_x = layers.Reshape((lag_order,96))(layer_x)\n",
    "    \n",
    "    # Insert dropout layer\n",
    "    layer_x = layers.Dropout(dropout)(layer_x)\n",
    "    \n",
    "    # LSTM with 64 hidden units\n",
    "    layer_x = layers.LSTM(64)(layer_x)\n",
    "    \n",
    "    # Final output layer\n",
    "    output = layers.Dense(3, activation='softmax')(layer_x)\n",
    "    \n",
    "    model = Model(input_tensor, output)\n",
    "    \n",
    "    opt = Adam(learning_rate=0.01, epsilon=1)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_details = {\n",
    "    'OF': {\n",
    "        'model': None, 'function': CNN_LSTM, 'data': None, 'lag_order': 100, 'dropout': 0.2\n",
    "    },\n",
    "    'OFI': {\n",
    "        'model': None, 'function': CNN_LSTM, 'data': None, 'lag_order': 100, 'dropout': 0.2\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scrape our data using Coinbase's public API. Our method pulls live order book state information for Bitcoin traded on Coinbase. We essentially have the ability to choose however many observations we desire. To avoid processing the data pull for an obscene length of time, we unfortunately decide not to get as many observations as the datasets used in the aforementioned related papers, since the purpose of this notebook is not to submit a solution to the model risk management team, but instead to showcase what I've taught myself in the past few months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from cbpro import PublicClient\n",
    "# from time import time, strftime, gmtime\n",
    "\n",
    "# public_client = PublicClient()\n",
    "# lob_data = pd.DataFrame()\n",
    "# start = time()\n",
    "# while len(lob_data) < 100002:\n",
    "#     raw_data = pd.concat((pd.DataFrame.from_dict(public_client.get_product_order_book('BTC-USD',level=2)['asks'])[:10],\n",
    "#                      pd.DataFrame.from_dict(public_client.get_product_order_book('BTC-USD',level=2)['bids'])[:10]),axis=1)\n",
    "#     lob_data = pd.concat((lob_data, pd.concat((pd.DataFrame(raw_data.drop(2,axis=1).iloc[i]).T for i in range(10)), axis=1).apply(lambda x: pd.Series(x.dropna().values))))\n",
    "# end = time()\n",
    "# print((end-start)/len(lob_data))\n",
    "\n",
    "# lob_data.columns = ['PRICE_ASK_1','VOLUME_ASK_1','PRICE_BID_1','VOLUME_BID_1',\n",
    "#            'PRICE_ASK_2','VOLUME_ASK_2','PRICE_BID_2','VOLUME_BID_2',\n",
    "#            'PRICE_ASK_3','VOLUME_ASK_3','PRICE_BID_3','VOLUME_BID_3',\n",
    "#            'PRICE_ASK_4','VOLUME_ASK_4','PRICE_BID_4','VOLUME_BID_4',\n",
    "#            'PRICE_ASK_5','VOLUME_ASK_5','PRICE_BID_5','VOLUME_BID_5',\n",
    "#            'PRICE_ASK_6','VOLUME_ASK_6','PRICE_BID_6','VOLUME_BID_6',\n",
    "#            'PRICE_ASK_7','VOLUME_ASK_7','PRICE_BID_7','VOLUME_BID_7',\n",
    "#            'PRICE_ASK_8','VOLUME_ASK_8','PRICE_BID_8','VOLUME_BID_8',\n",
    "#            'PRICE_ASK_9','VOLUME_ASK_9','PRICE_BID_9','VOLUME_BID_9',\n",
    "#            'PRICE_ASK_10','VOLUME_ASK_10','PRICE_BID_10','VOLUME_BID_10']\n",
    "# lob_data.index = range(len(lob_data))\n",
    "# lob_data = lob_data.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it can vary between any two events in the dataset, the time interval between two observations is on average $xxxx$ seconds. So, in lieu of the granularity that Zhang et al and Ntakaris et al boast in their respective datasets we simply extract labels for relative changes in only the next event. Just as in those datasets, our labels describe the percentage change of the mid price between events. For percentage changes greater than $0.002$, we use label $1$, for percentages change between $-0.002$ and $0.002$, we use label $0$, and for percentage changes smaller than $-0.002$, we use label $-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRICE_ASK_0</th>\n",
       "      <th>VOLUME_ASK_0</th>\n",
       "      <th>PRICE_BID_0</th>\n",
       "      <th>VOLUME_BID_0</th>\n",
       "      <th>PRICE_ASK_1</th>\n",
       "      <th>VOLUME_ASK_1</th>\n",
       "      <th>PRICE_BID_1</th>\n",
       "      <th>VOLUME_BID_1</th>\n",
       "      <th>PRICE_ASK_2</th>\n",
       "      <th>VOLUME_ASK_2</th>\n",
       "      <th>...</th>\n",
       "      <th>VOLUME_BID_7</th>\n",
       "      <th>PRICE_ASK_8</th>\n",
       "      <th>VOLUME_ASK_8</th>\n",
       "      <th>PRICE_BID_8</th>\n",
       "      <th>VOLUME_BID_8</th>\n",
       "      <th>PRICE_ASK_9</th>\n",
       "      <th>VOLUME_ASK_9</th>\n",
       "      <th>PRICE_BID_9</th>\n",
       "      <th>VOLUME_BID_9</th>\n",
       "      <th>LABEL_1TICK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20690.00</td>\n",
       "      <td>0.084309</td>\n",
       "      <td>20689.99</td>\n",
       "      <td>0.011289</td>\n",
       "      <td>20690.02</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>20689.98</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>20691.38</td>\n",
       "      <td>0.981250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>20695.72</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>20685.97</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>20696.33</td>\n",
       "      <td>0.358238</td>\n",
       "      <td>20685.65</td>\n",
       "      <td>0.002158</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20690.00</td>\n",
       "      <td>0.084309</td>\n",
       "      <td>20689.99</td>\n",
       "      <td>0.011289</td>\n",
       "      <td>20690.02</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>20689.98</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>20691.38</td>\n",
       "      <td>0.981250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>20695.72</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>20685.97</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>20696.33</td>\n",
       "      <td>0.358238</td>\n",
       "      <td>20685.65</td>\n",
       "      <td>0.002158</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20690.00</td>\n",
       "      <td>0.084309</td>\n",
       "      <td>20689.99</td>\n",
       "      <td>0.011289</td>\n",
       "      <td>20690.02</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>20689.98</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>20691.38</td>\n",
       "      <td>0.981250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>20695.72</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>20685.97</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>20696.33</td>\n",
       "      <td>0.358238</td>\n",
       "      <td>20685.65</td>\n",
       "      <td>0.002158</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20690.00</td>\n",
       "      <td>0.084309</td>\n",
       "      <td>20689.65</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>20690.02</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>20689.64</td>\n",
       "      <td>0.006213</td>\n",
       "      <td>20691.38</td>\n",
       "      <td>0.981250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>20695.72</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>20685.57</td>\n",
       "      <td>2.043200</td>\n",
       "      <td>20696.33</td>\n",
       "      <td>0.358238</td>\n",
       "      <td>20685.32</td>\n",
       "      <td>0.018317</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20689.88</td>\n",
       "      <td>0.017278</td>\n",
       "      <td>20689.83</td>\n",
       "      <td>0.011799</td>\n",
       "      <td>20689.89</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>20686.69</td>\n",
       "      <td>0.003841</td>\n",
       "      <td>20689.90</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024724</td>\n",
       "      <td>20692.00</td>\n",
       "      <td>0.231488</td>\n",
       "      <td>20682.78</td>\n",
       "      <td>0.025542</td>\n",
       "      <td>20693.47</td>\n",
       "      <td>0.016210</td>\n",
       "      <td>20682.10</td>\n",
       "      <td>0.241633</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>21076.00</td>\n",
       "      <td>0.020025</td>\n",
       "      <td>21072.72</td>\n",
       "      <td>0.002722</td>\n",
       "      <td>21076.08</td>\n",
       "      <td>0.025441</td>\n",
       "      <td>21072.71</td>\n",
       "      <td>0.091130</td>\n",
       "      <td>21076.14</td>\n",
       "      <td>0.020467</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018979</td>\n",
       "      <td>21078.66</td>\n",
       "      <td>0.008612</td>\n",
       "      <td>21071.74</td>\n",
       "      <td>0.227235</td>\n",
       "      <td>21078.72</td>\n",
       "      <td>0.227227</td>\n",
       "      <td>21071.32</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>21071.31</td>\n",
       "      <td>0.005324</td>\n",
       "      <td>21068.78</td>\n",
       "      <td>0.094586</td>\n",
       "      <td>21071.35</td>\n",
       "      <td>0.013313</td>\n",
       "      <td>21068.65</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>21071.41</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.237270</td>\n",
       "      <td>21073.46</td>\n",
       "      <td>0.013086</td>\n",
       "      <td>21066.24</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>21073.48</td>\n",
       "      <td>0.021072</td>\n",
       "      <td>21065.65</td>\n",
       "      <td>0.355873</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>21071.31</td>\n",
       "      <td>0.005324</td>\n",
       "      <td>21068.78</td>\n",
       "      <td>0.094586</td>\n",
       "      <td>21071.35</td>\n",
       "      <td>0.013313</td>\n",
       "      <td>21068.65</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>21071.41</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.237270</td>\n",
       "      <td>21073.46</td>\n",
       "      <td>0.013086</td>\n",
       "      <td>21066.24</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>21073.48</td>\n",
       "      <td>0.021072</td>\n",
       "      <td>21065.65</td>\n",
       "      <td>0.355873</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>21064.51</td>\n",
       "      <td>0.111293</td>\n",
       "      <td>21064.50</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>21064.60</td>\n",
       "      <td>0.041741</td>\n",
       "      <td>21061.10</td>\n",
       "      <td>0.744450</td>\n",
       "      <td>21065.25</td>\n",
       "      <td>0.008726</td>\n",
       "      <td>...</td>\n",
       "      <td>0.355963</td>\n",
       "      <td>21066.80</td>\n",
       "      <td>0.009655</td>\n",
       "      <td>21058.89</td>\n",
       "      <td>0.004937</td>\n",
       "      <td>21067.00</td>\n",
       "      <td>0.227349</td>\n",
       "      <td>21057.91</td>\n",
       "      <td>0.098579</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>21064.19</td>\n",
       "      <td>0.094998</td>\n",
       "      <td>21061.10</td>\n",
       "      <td>0.744450</td>\n",
       "      <td>21064.25</td>\n",
       "      <td>0.012357</td>\n",
       "      <td>21059.36</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>21064.31</td>\n",
       "      <td>0.227376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195993</td>\n",
       "      <td>21065.25</td>\n",
       "      <td>0.009204</td>\n",
       "      <td>21056.68</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>21065.26</td>\n",
       "      <td>0.686197</td>\n",
       "      <td>21056.15</td>\n",
       "      <td>0.058778</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100001 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        PRICE_ASK_0  VOLUME_ASK_0  PRICE_BID_0  VOLUME_BID_0  PRICE_ASK_1  \\\n",
       "0          20690.00      0.084309     20689.99      0.011289     20690.02   \n",
       "1          20690.00      0.084309     20689.99      0.011289     20690.02   \n",
       "2          20690.00      0.084309     20689.99      0.011289     20690.02   \n",
       "3          20690.00      0.084309     20689.65      0.005000     20690.02   \n",
       "4          20689.88      0.017278     20689.83      0.011799     20689.89   \n",
       "...             ...           ...          ...           ...          ...   \n",
       "99996      21076.00      0.020025     21072.72      0.002722     21076.08   \n",
       "99997      21071.31      0.005324     21068.78      0.094586     21071.35   \n",
       "99998      21071.31      0.005324     21068.78      0.094586     21071.35   \n",
       "99999      21064.51      0.111293     21064.50      0.005000     21064.60   \n",
       "100000     21064.19      0.094998     21061.10      0.744450     21064.25   \n",
       "\n",
       "        VOLUME_ASK_1  PRICE_BID_1  VOLUME_BID_1  PRICE_ASK_2  VOLUME_ASK_2  \\\n",
       "0           0.000143     20689.98      0.000348     20691.38      0.981250   \n",
       "1           0.000143     20689.98      0.000348     20691.38      0.981250   \n",
       "2           0.000143     20689.98      0.000348     20691.38      0.981250   \n",
       "3           0.000143     20689.64      0.006213     20691.38      0.981250   \n",
       "4           0.000100     20686.69      0.003841     20689.90      0.000100   \n",
       "...              ...          ...           ...          ...           ...   \n",
       "99996       0.025441     21072.71      0.091130     21076.14      0.020467   \n",
       "99997       0.013313     21068.65      0.080000     21071.41      0.005000   \n",
       "99998       0.013313     21068.65      0.080000     21071.41      0.005000   \n",
       "99999       0.041741     21061.10      0.744450     21065.25      0.008726   \n",
       "100000      0.012357     21059.36      0.002087     21064.31      0.227376   \n",
       "\n",
       "        ...  VOLUME_BID_7  PRICE_ASK_8  VOLUME_ASK_8  PRICE_BID_8  \\\n",
       "0       ...      0.000300     20695.72      0.100000     20685.97   \n",
       "1       ...      0.000300     20695.72      0.100000     20685.97   \n",
       "2       ...      0.000300     20695.72      0.100000     20685.97   \n",
       "3       ...      0.150000     20695.72      0.100000     20685.57   \n",
       "4       ...      0.024724     20692.00      0.231488     20682.78   \n",
       "...     ...           ...          ...           ...          ...   \n",
       "99996   ...      0.018979     21078.66      0.008612     21071.74   \n",
       "99997   ...      0.237270     21073.46      0.013086     21066.24   \n",
       "99998   ...      0.237270     21073.46      0.013086     21066.24   \n",
       "99999   ...      0.355963     21066.80      0.009655     21058.89   \n",
       "100000  ...      0.195993     21065.25      0.009204     21056.68   \n",
       "\n",
       "        VOLUME_BID_8  PRICE_ASK_9  VOLUME_ASK_9  PRICE_BID_9  VOLUME_BID_9  \\\n",
       "0           0.003401     20696.33      0.358238     20685.65      0.002158   \n",
       "1           0.003401     20696.33      0.358238     20685.65      0.002158   \n",
       "2           0.003401     20696.33      0.358238     20685.65      0.002158   \n",
       "3           2.043200     20696.33      0.358238     20685.32      0.018317   \n",
       "4           0.025542     20693.47      0.016210     20682.10      0.241633   \n",
       "...              ...          ...           ...          ...           ...   \n",
       "99996       0.227235     21078.72      0.227227     21071.32      0.100000   \n",
       "99997       0.120000     21073.48      0.021072     21065.65      0.355873   \n",
       "99998       0.120000     21073.48      0.021072     21065.65      0.355873   \n",
       "99999       0.004937     21067.00      0.227349     21057.91      0.098579   \n",
       "100000      0.000969     21065.26      0.686197     21056.15      0.058778   \n",
       "\n",
       "        LABEL_1TICK  \n",
       "0               0.0  \n",
       "1               0.0  \n",
       "2               0.0  \n",
       "3               0.0  \n",
       "4               0.0  \n",
       "...             ...  \n",
       "99996          -1.0  \n",
       "99997           0.0  \n",
       "99998          -1.0  \n",
       "99999          -1.0  \n",
       "100000          0.0  \n",
       "\n",
       "[100001 rows x 41 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lob_data['LABEL_1TICK'] = np.zeros(len(lob_data))\n",
    "# for i in range(len(lob_data)-1):\n",
    "#     if (lob_data.loc[i+1,'PRICE_ASK_0'] + lob_data.loc[i+1,'PRICE_ASK_0']) > 1.00002*(lob_data.loc[i,'PRICE_ASK_0'] + lob_data.loc[i,'PRICE_ASK_0']):\n",
    "#         lob_data['LABEL_1TICK'][i] = 1\n",
    "#     elif (lob_data.loc[i+1,'PRICE_BID_0'] + lob_data.loc[i+1,'PRICE_BID_0']) < 0.99998*(lob_data.loc[i,'PRICE_BID_0'] + lob_data.loc[i,'PRICE_BID_0']):\n",
    "#         lob_data['LABEL_1TICK'][i] = -1\n",
    "# lob_data = lob_data.head(len(lob_data)-1)\n",
    "\n",
    "# lob_data.to_csv('BTC-USD LOB {}'.format(strftime('%d-%b-%Y', gmtime())), index=False)\n",
    "lob_data = pd.read_csv('BTC-USD LOB 24 Jun 2022')\n",
    "lob_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we now have our sequence of $40$-variable vectors of order book states, we can obtain the order flow data as well the order flow imbalance data for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bOF_0</th>\n",
       "      <th>aOF_0</th>\n",
       "      <th>bOF_1</th>\n",
       "      <th>aOF_1</th>\n",
       "      <th>bOF_2</th>\n",
       "      <th>aOF_2</th>\n",
       "      <th>bOF_3</th>\n",
       "      <th>aOF_3</th>\n",
       "      <th>bOF_4</th>\n",
       "      <th>aOF_4</th>\n",
       "      <th>...</th>\n",
       "      <th>aOF_5</th>\n",
       "      <th>bOF_6</th>\n",
       "      <th>aOF_6</th>\n",
       "      <th>bOF_7</th>\n",
       "      <th>aOF_7</th>\n",
       "      <th>bOF_8</th>\n",
       "      <th>aOF_8</th>\n",
       "      <th>bOF_9</th>\n",
       "      <th>aOF_9</th>\n",
       "      <th>LABEL_1TICK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.005000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.006213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.003067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.009612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.002158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.150000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.043200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.018317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.011799</td>\n",
       "      <td>0.017278</td>\n",
       "      <td>-0.003841</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-1.513803</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-0.018317</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.006624</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057827</td>\n",
       "      <td>-0.016070</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>-0.024724</td>\n",
       "      <td>0.981250</td>\n",
       "      <td>-0.025542</td>\n",
       "      <td>0.231488</td>\n",
       "      <td>-0.241633</td>\n",
       "      <td>0.016210</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>-0.094586</td>\n",
       "      <td>0.005324</td>\n",
       "      <td>-0.080000</td>\n",
       "      <td>0.013313</td>\n",
       "      <td>-0.227265</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>-0.339581</td>\n",
       "      <td>0.006926</td>\n",
       "      <td>-0.000495</td>\n",
       "      <td>0.016311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016860</td>\n",
       "      <td>-0.010967</td>\n",
       "      <td>0.238264</td>\n",
       "      <td>-0.237270</td>\n",
       "      <td>0.061199</td>\n",
       "      <td>-0.120000</td>\n",
       "      <td>0.013086</td>\n",
       "      <td>-0.355873</td>\n",
       "      <td>0.021072</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>-0.005000</td>\n",
       "      <td>0.111293</td>\n",
       "      <td>-0.744450</td>\n",
       "      <td>0.041741</td>\n",
       "      <td>-0.007445</td>\n",
       "      <td>0.008726</td>\n",
       "      <td>-1.085000</td>\n",
       "      <td>0.044998</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.227351</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052319</td>\n",
       "      <td>-0.006386</td>\n",
       "      <td>0.007704</td>\n",
       "      <td>-0.355963</td>\n",
       "      <td>0.637379</td>\n",
       "      <td>-0.004937</td>\n",
       "      <td>0.009655</td>\n",
       "      <td>-0.098579</td>\n",
       "      <td>0.227349</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>-0.744450</td>\n",
       "      <td>0.094998</td>\n",
       "      <td>-0.002087</td>\n",
       "      <td>0.012357</td>\n",
       "      <td>-0.227364</td>\n",
       "      <td>0.227376</td>\n",
       "      <td>-0.237347</td>\n",
       "      <td>0.016477</td>\n",
       "      <td>-0.000697</td>\n",
       "      <td>0.034887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018410</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.009204</td>\n",
       "      <td>-0.195993</td>\n",
       "      <td>0.009204</td>\n",
       "      <td>-0.000969</td>\n",
       "      <td>0.009204</td>\n",
       "      <td>-0.058778</td>\n",
       "      <td>0.686197</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           bOF_0     aOF_0     bOF_1     aOF_1     bOF_2     aOF_2     bOF_3  \\\n",
       "1       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3      -0.005000  0.000000 -0.006213  0.000000 -0.003067  0.000000 -0.009612   \n",
       "4       0.011799  0.017278 -0.003841  0.000100 -1.513803  0.000100 -0.018317   \n",
       "5       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "99996   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "99997  -0.094586  0.005324 -0.080000  0.013313 -0.227265  0.005000 -0.339581   \n",
       "99998   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "99999  -0.005000  0.111293 -0.744450  0.041741 -0.007445  0.008726 -1.085000   \n",
       "100000 -0.744450  0.094998 -0.002087  0.012357 -0.227364  0.227376 -0.237347   \n",
       "\n",
       "           aOF_3     bOF_4     aOF_4  ...     aOF_5     bOF_6     aOF_6  \\\n",
       "1       0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "2       0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "3       0.000000 -0.000300  0.000000  ...  0.000000 -0.002158  0.000000   \n",
       "4       0.000100 -0.100000  0.006624  ...  0.057827 -0.016070  0.000143   \n",
       "5       0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "99996   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "99997   0.006926 -0.000495  0.016311  ...  0.016860 -0.010967  0.238264   \n",
       "99998   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "99999   0.044998 -0.050000  0.227351  ...  0.052319 -0.006386  0.007704   \n",
       "100000  0.016477 -0.000697  0.034887  ...  0.018410 -0.750000  0.009204   \n",
       "\n",
       "           bOF_7     aOF_7     bOF_8     aOF_8     bOF_9     aOF_9  \\\n",
       "1       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3      -0.150000  0.000000 -2.043200  0.000000 -0.018317  0.000000   \n",
       "4      -0.024724  0.981250 -0.025542  0.231488 -0.241633  0.016210   \n",
       "5       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "...          ...       ...       ...       ...       ...       ...   \n",
       "99996   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "99997  -0.237270  0.061199 -0.120000  0.013086 -0.355873  0.021072   \n",
       "99998   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "99999  -0.355963  0.637379 -0.004937  0.009655 -0.098579  0.227349   \n",
       "100000 -0.195993  0.009204 -0.000969  0.009204 -0.058778  0.686197   \n",
       "\n",
       "        LABEL_1TICK  \n",
       "1               0.0  \n",
       "2               0.0  \n",
       "3               0.0  \n",
       "4               0.0  \n",
       "5               0.0  \n",
       "...             ...  \n",
       "99996          -1.0  \n",
       "99997           0.0  \n",
       "99998          -1.0  \n",
       "99999          -1.0  \n",
       "100000          0.0  \n",
       "\n",
       "[100000 rows x 21 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "of_data = pd.DataFrame()\n",
    "for i in range(10):\n",
    "    of_data['bOF_{}'.format(i)] = np.empty(len(lob_data))\n",
    "    of_data['aOF_{}'.format(i)] = np.empty(len(lob_data))\n",
    "    of_data['bOF_{}'.format(i)][0] = None\n",
    "    of_data['aOF_{}'.format(i)][0] = None\n",
    "    for j in range(1,len(lob_data)):\n",
    "            \n",
    "        # Bid Order Flow\n",
    "        if lob_data.loc[j,'PRICE_BID_{}'.format(i)] > lob_data.loc[j-1,'PRICE_BID_{}'.format(i)]:\n",
    "            of_data['bOF_{}'.format(i)][j] = lob_data.loc[j,'VOLUME_BID_{}'.format(i)]\n",
    "        elif lob_data.loc[j,'PRICE_BID_{}'.format(i)] < lob_data.loc[j-1,'PRICE_BID_{}'.format(i)]:\n",
    "            of_data['bOF_{}'.format(i)][j] = -1*lob_data.loc[j,'VOLUME_BID_{}'.format(i)]\n",
    "        else:\n",
    "            of_data['bOF_{}'.format(i)][j] = lob_data.loc[j,'VOLUME_BID_{}'.format(i)] - lob_data.loc[j-1,'VOLUME_BID_{}'.format(i)]\n",
    "            \n",
    "        # Ask Order Flow\n",
    "        if lob_data.loc[j,'PRICE_ASK_{}'.format(i)] > lob_data.loc[j-1,'PRICE_ASK_{}'.format(i)]:\n",
    "            of_data['aOF_{}'.format(i)][j] = -1*lob_data.loc[j,'VOLUME_ASK_{}'.format(i)]\n",
    "        elif lob_data.loc[j,'PRICE_ASK_{}'.format(i)] < lob_data.loc[j-1,'PRICE_ASK_{}'.format(i)]:\n",
    "            of_data['aOF_{}'.format(i)][j] = lob_data.loc[j,'VOLUME_ASK_{}'.format(i)]\n",
    "        else:\n",
    "            of_data['aOF_{}'.format(i)][j] = lob_data.loc[j,'VOLUME_ASK_{}'.format(i)] - lob_data.loc[j-1,'VOLUME_ASK_{}'.format(i)]\n",
    "            \n",
    "# Add output columns to of_data\n",
    "of_data = pd.concat([of_data,lob_data.iloc[:,-1:]],axis=1)\n",
    "\n",
    "# Drop first row\n",
    "of_data = of_data.iloc[1:,:]\n",
    "\n",
    "model_details['OF']['data'] = of_data\n",
    "of_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OFI_0</th>\n",
       "      <th>OFI_1</th>\n",
       "      <th>OFI_2</th>\n",
       "      <th>OFI_3</th>\n",
       "      <th>OFI_4</th>\n",
       "      <th>OFI_5</th>\n",
       "      <th>OFI_6</th>\n",
       "      <th>OFI_7</th>\n",
       "      <th>OFI_8</th>\n",
       "      <th>OFI_9</th>\n",
       "      <th>LABEL_1TICK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.005000</td>\n",
       "      <td>-0.006213</td>\n",
       "      <td>-0.003067</td>\n",
       "      <td>-0.009612</td>\n",
       "      <td>-0.000300</td>\n",
       "      <td>-0.003401</td>\n",
       "      <td>-0.002158</td>\n",
       "      <td>-0.150000</td>\n",
       "      <td>-2.043200</td>\n",
       "      <td>-0.018317</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.005478</td>\n",
       "      <td>-0.003941</td>\n",
       "      <td>-1.513903</td>\n",
       "      <td>-0.018417</td>\n",
       "      <td>-0.106624</td>\n",
       "      <td>-0.807827</td>\n",
       "      <td>-0.016213</td>\n",
       "      <td>-1.005974</td>\n",
       "      <td>-0.257030</td>\n",
       "      <td>-0.257842</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>-0.099911</td>\n",
       "      <td>-0.093313</td>\n",
       "      <td>-0.232265</td>\n",
       "      <td>-0.346507</td>\n",
       "      <td>-0.016806</td>\n",
       "      <td>-0.766860</td>\n",
       "      <td>-0.249231</td>\n",
       "      <td>-0.298469</td>\n",
       "      <td>-0.133086</td>\n",
       "      <td>-0.376945</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>-0.116293</td>\n",
       "      <td>-0.786191</td>\n",
       "      <td>-0.016171</td>\n",
       "      <td>-1.129998</td>\n",
       "      <td>-0.277351</td>\n",
       "      <td>-0.289645</td>\n",
       "      <td>-0.014090</td>\n",
       "      <td>-0.993342</td>\n",
       "      <td>-0.014593</td>\n",
       "      <td>-0.325928</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>-0.839448</td>\n",
       "      <td>-0.014444</td>\n",
       "      <td>-0.454740</td>\n",
       "      <td>-0.253823</td>\n",
       "      <td>-0.035584</td>\n",
       "      <td>-0.067699</td>\n",
       "      <td>-0.759204</td>\n",
       "      <td>-0.205197</td>\n",
       "      <td>-0.010173</td>\n",
       "      <td>-0.744974</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          OFI_0     OFI_1     OFI_2     OFI_3     OFI_4     OFI_5     OFI_6  \\\n",
       "0      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2     -0.005000 -0.006213 -0.003067 -0.009612 -0.000300 -0.003401 -0.002158   \n",
       "3     -0.005478 -0.003941 -1.513903 -0.018417 -0.106624 -0.807827 -0.016213   \n",
       "4      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "99995  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "99996 -0.099911 -0.093313 -0.232265 -0.346507 -0.016806 -0.766860 -0.249231   \n",
       "99997  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "99998 -0.116293 -0.786191 -0.016171 -1.129998 -0.277351 -0.289645 -0.014090   \n",
       "99999 -0.839448 -0.014444 -0.454740 -0.253823 -0.035584 -0.067699 -0.759204   \n",
       "\n",
       "          OFI_7     OFI_8     OFI_9  LABEL_1TICK  \n",
       "0      0.000000  0.000000  0.000000          0.0  \n",
       "1      0.000000  0.000000  0.000000          0.0  \n",
       "2     -0.150000 -2.043200 -0.018317          0.0  \n",
       "3     -1.005974 -0.257030 -0.257842          0.0  \n",
       "4      0.000000  0.000000  0.000000          0.0  \n",
       "...         ...       ...       ...          ...  \n",
       "99995  0.000000  0.000000  0.000000         -1.0  \n",
       "99996 -0.298469 -0.133086 -0.376945          0.0  \n",
       "99997  0.000000  0.000000  0.000000         -1.0  \n",
       "99998 -0.993342 -0.014593 -0.325928         -1.0  \n",
       "99999 -0.205197 -0.010173 -0.744974          0.0  \n",
       "\n",
       "[100000 rows x 11 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ofi_data = pd.DataFrame()\n",
    "for i in range(10):\n",
    "    ofi_data['OFI_{}'.format(i)] = np.empty(len(of_data))\n",
    "    for j in range(len(of_data)):\n",
    "        ofi_data['OFI_{}'.format(i)][j] = of_data.loc[j+1,'bOF_{}'.format(i)] - of_data.loc[j+1,'aOF_{}'.format(i)]\n",
    "\n",
    "ofi_data = pd.concat([ofi_data,of_data.iloc[:,-1:]],axis=1).apply(lambda x: pd.Series(x.dropna().values))\n",
    "\n",
    "model_details['OFI']['data'] = ofi_data\n",
    "ofi_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology and Experimentation\n",
    "Now that we have our data, we seek to train the CNN-LSTM to accomplish the forecasting task of classifying future mid-prices by their directional moves. In this section, we adopt the Box-Jenkins approach to time series modeling by pulling aside to recognize the time series as vector autoregressive (VAR) processes, determining the orders of the corresponding VAR models in order to appropriately tensorize the data, tuning necessary hyperparameters of the deep learning model with cross-validation, and evaluating the trained deep learning model.\n",
    "### Vector Autoregressive Processes\n",
    "The *vector autoregressive* (VAR) model is the multivariate extension of the *autoregressive* (AR) model. That is, it is a statistical representation of a collection of time-varying stochastic processes and thus often serves a role in the modeling of multivariate financial time series and other complex randomly-evolving processes that occur in the real world. The term *autoregressive* indicates that each realization of the process is a linear function of previous values in the sequence plus a stochastic error term that is uncorrelated with those of other periods. That is, for a $ K $-variate time series $ \\left\\{y_i\\right\\}_{i=1}^{t-1} $ in which we assume only $ p $ past values are necessary to forecast the next observation $ y_{t} $, we have the $ \\text{VAR}(p) $ representation\n",
    "\n",
    "$$ y_{t}= \\alpha + B_1 y_t + B_2 y_{t-1} + ... + B_p y_{t-p} + u_{t}, $$\n",
    "\n",
    "where $ \\alpha = (\\alpha_1, ..., \\alpha_K)^T $, $ B_i =  \\left[ \\begin{array}{ccc}\n",
    "\\beta_{11,i} & \\dots & \\beta_{1K,i} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\beta_{K1,i} & \\dots & \\beta_{KK,i} \\end{array} \\right] $, and $ \\left\\{u_i\\right\\}_{i=1}^{t} = \\left\\{(u_{1i}, ..., u_{Ki})^T \\right\\}_{i=1}^{t} \\subset \\mathbb{R}^K$ is independently identically distributed with mean zero. We do not assume that all $ B_i $ are nonzero, so $p$ really represents an upper bound on the order of the process.\n",
    "\n",
    "Now, in order to estimate the VAR order $p$ with consistency, it's important to assure two key statistical properties hold in our time series: stationarity and cointegration. \n",
    "### Stationarity and Cointegration\n",
    "A time series is considered *stationary* if its mean and correlation coefficients remain constant as time progresses. That is, a stationary time series is one that changes over time in a manner that is consistent. This is important because when a model is learned, what the model is really learning are the regression coefficients. So, when a model predicts on data in the time series, it is doing so using its knowledge of the relationship that was learned in previous data points. Because of this, we want to ensure that that relationship remains consistent in future data.\n",
    "\n",
    "For a collection of non-stationary time series, if a stationary linear combination of them exists, then the combined (multivariate) time series is said to be *cointegrated*. We consider a multivariate time series to be stationary if the number of cointegrated relationships is equal to the number of variables in the time series. \n",
    "\n",
    "Accordingly, we proceed by performing *Johansen's cointegration test*. We do not utilize the Augmented Dickey Fuller (ADF) test for each univariate time series in our data, since univariate stationarities of variables do not guarantee linear combinations of them are stationary. Nonetheless, we recall that order flow and order flow imbalance are univariately stationary. The null hypothesis for Johansen's cointegration test with trace is that the number of cointegrating relationships is less than the number of variables in our time series, while the alternative is that the number of cointegrating relationships equals the number of variables in our time series. By rejecting the null hypothesis at a specified significance level, we accept that the multivariate time series is stationary with that significance. We choose to perform the test at the $99\\%$ significance level.\n",
    "\n",
    "However, there is a key caveat to Johansen's cointegration test that cannot be overlooked in our analysis, which is the fact that computing the critical values for a multivariate time series of more than $12$ variables cannot be done. So, we cannot directly apply this test on our $20$-variable order flow time series since the result cannot deduce number the cointegrating relationships. However, this lack of robust analysis isn't a deal breaker at this stage, since successful models have been trained on order flow in many peer-reviewed papers. We continue by testing for stationary on our order flow imbalance time series since it is only a $10$-variable time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "jres = coint_johansen(ofi_data.iloc[:,:-1], det_order=0, k_ar_diff=1)\n",
    "jres.trace_stat > jres.trace_stat_crit_vals[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated trace statistics and are far greater than the critical values at the $99\\%$ significance level, so we can reject the null hypothesis at the $99\\%$ significance level. That is, we are at least $99\\%$ confident that the number of cointegrating relationships is equal to the number of variables in our multivariate time series, so we consider our order flow imbalance data to be stationary.\n",
    "### Order Selection for the VAR Model\n",
    "We turn now to fitting the order flow and order flow imbalance data as VAR models\n",
    "\n",
    "$$ y_{t}= \\alpha + B_1 y_t + B_2 y_{t-1} + ... + B_p y_{t-p} + u_{t} $$\n",
    "\n",
    "from which we extract the orders $p$ for use as the lag parameters in the deep learning model. Now, the articles from which we adopt the CNN-LSTM choose a generic rolling window of $100$ timestamps, but squared forecast errors are higher in higher order models than in lower order models, so we want to avoid choosing unnecessarily high VAR orders by statistically estimating $p$ such that $B_p \\neq 0$ and $B_i = 0$ for $i>p$.\n",
    "\n",
    "In practice, estimating the optimal lag is accomplished by iteratively fitting the model with an increasing estimate $m$ for $p$ and selecting the estimate $\\hat{p}$ that minimizes the Akaike information criteria (AIC) score\n",
    "$$ \\text{AIC}(m)=2\\ln|\\tilde{\\Sigma}_u(m)|+\\frac{2mK^2}{T} $$\n",
    "for the $K$-variate $\\text{VAR}(m)$ process, where $\\tilde{\\Sigma}_u(m)$ is the maximum likelihood estimator of the covariance matrix of $u_t$. For a more rigorous exploration of the VAR order selection process and of multivariate time series analysis as a whole, check out Helmut Lütkepohl's book on the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OF VAR order:  69\n",
      "OFI VAR order:  76\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.api import VAR\n",
    "for time_series in model_details:\n",
    "    model = VAR(model_details[time_series]['data'].iloc[:,:-1])\n",
    "    results = model.fit(maxlags=100, ic='aic')\n",
    "    var_order = results.k_ar\n",
    "    print(time_series + ' VAR order estimate: ', var_order)\n",
    "    model_details[time_series]['var_order'] = var_order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final point to mention here: when using a lookback period of $m$, our sequential learning model will not be able to learn dependencies over intervals longer than $m$, so while we trust the estimation of VAR order, it does not guarantee that the deep learning model does not underfit the data. In such a circumstance, it would be necessary to circle back and increase our choice of lag. More on this in a later section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepping the Data\n",
    "Now is time to prepare the data for cross-validation, training, and testing.\n",
    "#### Splitting the Time Series\n",
    "For each time series, we split the data by using the first $80\\%$ for training and the remaining $20\\%$ for out-of-sample testing. Since the ordering of our data matters, we make sure the test set is in the future of the training set to avoid look-ahead bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "train_weight = 0.8\n",
    "\n",
    "model_details['OF']['train'] = of_data.iloc[:int(len(of_data)*train_weight)]\n",
    "model_details['OF']['test'] = of_data.iloc[int(len(of_data)*train_weight):]\n",
    "\n",
    "model_details['OFI']['train'] = ofi_data.iloc[:int(len(ofi_data)*train_weight)]\n",
    "model_details['OFI']['test'] = ofi_data.iloc[int(len(ofi_data)*train_weight):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling\n",
    "Data normalization helps avoid potential fitting difficulties as a result of multiple features assuming different value ranges. Since we are dealing with time series data in which look-ahead bias must be mitigated, we scale the training data without knowledge of the test set and use those same characteristics of the training set to scale the test set. This is another reason why stationarity is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b2/s2446cr942bb0jd09xl930y80000gn/T/ipykernel_7953/432770347.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  time_series['train'][col] = time_series['train'].loc[:,col].apply(stdize_input)\n",
      "/var/folders/b2/s2446cr942bb0jd09xl930y80000gn/T/ipykernel_7953/432770347.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  time_series['test'][col] = time_series['test'].loc[:,col].apply(stdize_input)\n"
     ]
    }
   ],
   "source": [
    "# Standardize data\n",
    "# note that for a multivariate time series, you would need to scale \n",
    "# each variable by its own mean and standard deviation in the training set\n",
    "\n",
    "for time_series in model_details.values():\n",
    "    for col in time_series['train'].columns[:-1]:\n",
    "        mu = np.float(time_series['train'].loc[:,col].mean())\n",
    "        sigma = np.float(time_series['train'].loc[:,col].std())\n",
    "        stdize_input = lambda x: (x - mu) / sigma\n",
    "        time_series['train'][col] = time_series['train'].loc[:,col].apply(stdize_input)\n",
    "        time_series['test'][col] = time_series['test'].loc[:,col].apply(stdize_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform our sequential learning task, we must tensorize our data to create a time series that will serve as the inputs to the network.\n",
    "\n",
    "Let's define the following function for reshaping the data into a return prediction format for the 5 time horizons of the original set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ready data for training:\n",
    "# 1. lag: the order of the VAR process, or the sequence length or lookback window\n",
    "# 2. dimension: number of variables in the time series\n",
    "# 3. predictions=1: number of forecasting horizons\n",
    "def format_data(data, lag, dimension, predictions=1):\n",
    "    '''\n",
    "    lag: the order of the VAR process, or the sequence length or lookback window\n",
    "    dimension: number of variables in the time series\n",
    "    predictions=1: number of forecasting horizons\n",
    "    '''\n",
    "    data = data.values\n",
    "    shape = data.shape\n",
    "    X = np.zeros((shape[0]-lag, lag, dimension))\n",
    "    Y = np.zeros((shape[0]-lag, predictions))\n",
    "    for i in range(shape[0]-lag):\n",
    "        X[i] = data[i:i+lag, :dimension] # take the variables' columns as features\n",
    "        Y[i] = data[i+lag-1, -predictions:] # take the last column as labels\n",
    "    X = X.reshape(X.shape[0], lag, dimension, 1) # add the 4th dimension: 1 channel\n",
    "    \n",
    "    Y += 1\n",
    "    \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time_series in model_details.values():\n",
    "    time_series['train_x'], time_series['train_y'] = format_data(time_series['train'], time_series['lag_order'], len(time_series['train'].columns)-1)\n",
    "    time_series['train_y'] = time_series['train_y'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79924, 76, 10, 1)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_details['OFI']['train_x'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "<img src='Images/cross-val.png' style='width:625px;height:365px'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "any model that has been fitted under a Box–Jenkins approach needs to be assessed out-of-sample by time series cross-validation\n",
    "\n",
    "As advocated for in BDLOB, we utilize dropout variational inference as a stochastic regularizer in the deep neural network. We use grid search to detect an appropriate dropout rate. We insert this dropout layer after the Inception Module.\n",
    "\n",
    "We apply early stopping for all models to avoid overfitting, terminating training when validation loss has not decreased for five consecutive epochs.\n",
    "\n",
    "In the time series model, observations are strongly related to their recent histories. Hence, the ordering of our data matters, so we can't apply the typical K-fold cross-validation that is applied in cross-sectional models. In prediction models over time series data, no future observations can be used in the training set. Instead, a sliding window must be used to train and predict out-of-sample over multiple repetitions to allow for parameter tuning\n",
    "\n",
    "Do we fix the length of the sliding training window or do we allow it to \"telescope\" in time? There are pros and cons to choosing the latter; it includes more observations for training, but parameter confidence loses interpretability due to the loss of sample size control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=5, min_delta=3e-5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, TimeSeriesSplit, GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary containing a list of values to be iterated through\n",
    "# for each parameter of the model included in the search\n",
    "param_grid = {'dropout': np.linspace(0,.9,10)}\n",
    "tscv = TimeSeriesSplit(n_splits = 4)\n",
    "\n",
    "for time_series in model_details.values():\n",
    "    \n",
    "# In the kth split, TimeSeriesSplit returns first k folds\n",
    "# as training set and the (k+1)th fold as test set.\n",
    "    \n",
    "    \n",
    "# A grid search is performed for each of the models, and the parameter set which\n",
    "# performs best over all the cross-validation splits is saved in the `params` dictionary\n",
    "\n",
    "    model = KerasRegressor(build_fn=time_series['function'], epochs=max_epochs, \n",
    "                        batch_size=batch_size, verbose=2)\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, \n",
    "                    cv=tscv, n_jobs=1, verbose=1)\n",
    "    train_Y = to_categorical(train_Y) # y is the next event's mid price (k=1)\n",
    "    grid_result = grid.fit(time_series['train_x'], time_series['train_y'], callbacks=[es])\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params_ = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param_ in zip(means, stds, params_):\n",
    "        print(\"%f (%f) with %r\" % (mean, stdev, param_))\n",
    "\n",
    "    time_series['dropout'] = grid_result.best_params_['dropout']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-step Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 76, 10, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 76, 10, 16)   80          ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " leaky_re_lu_14 (LeakyReLU)     (None, 76, 10, 16)   0           ['conv2d_15[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 76, 10, 16)   1040        ['leaky_re_lu_14[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_15 (LeakyReLU)     (None, 76, 10, 16)   0           ['conv2d_16[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 76, 1, 16)    2576        ['leaky_re_lu_15[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_16 (LeakyReLU)     (None, 76, 1, 16)    0           ['conv2d_17[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 76, 1, 32)    544         ['leaky_re_lu_16[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 76, 1, 32)    544         ['leaky_re_lu_16[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_17 (LeakyReLU)     (None, 76, 1, 32)    0           ['conv2d_18[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_re_lu_19 (LeakyReLU)     (None, 76, 1, 32)    0           ['conv2d_20[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 76, 1, 16)   0           ['leaky_re_lu_16[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 76, 1, 32)    3104        ['leaky_re_lu_17[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 76, 1, 32)    5152        ['leaky_re_lu_19[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 76, 1, 32)    544         ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " leaky_re_lu_18 (LeakyReLU)     (None, 76, 1, 32)    0           ['conv2d_19[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_re_lu_20 (LeakyReLU)     (None, 76, 1, 32)    0           ['conv2d_21[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_re_lu_21 (LeakyReLU)     (None, 76, 1, 32)    0           ['conv2d_22[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 76, 1, 96)    0           ['leaky_re_lu_18[0][0]',         \n",
      "                                                                  'leaky_re_lu_20[0][0]',         \n",
      "                                                                  'leaky_re_lu_21[0][0]']         \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 76, 96)       0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 76, 96)       0           ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 64)           41216       ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 3)            195         ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 54,995\n",
      "Trainable params: 54,995\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_params['function'](dropout = model_params['dropout'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2498/2498 [==============================] - 199s 78ms/step - loss: 0.7692 - accuracy: 0.7287\n",
      "Epoch 2/100\n",
      "2498/2498 [==============================] - 189s 76ms/step - loss: 0.7617 - accuracy: 0.7344\n",
      "Epoch 3/100\n",
      "2498/2498 [==============================] - 189s 76ms/step - loss: 0.7560 - accuracy: 0.7344\n",
      "Epoch 4/100\n",
      "2498/2498 [==============================] - 189s 76ms/step - loss: 0.7242 - accuracy: 0.7344\n",
      "Epoch 5/100\n",
      "2498/2498 [==============================] - 190s 76ms/step - loss: 0.6617 - accuracy: 0.7333\n",
      "Epoch 6/100\n",
      "2498/2498 [==============================] - 190s 76ms/step - loss: 0.6451 - accuracy: 0.7332\n",
      "Epoch 7/100\n",
      "2498/2498 [==============================] - 186s 75ms/step - loss: 0.6368 - accuracy: 0.7339\n",
      "Epoch 8/100\n",
      "2498/2498 [==============================] - 176s 70ms/step - loss: 0.6293 - accuracy: 0.7348\n",
      "Epoch 9/100\n",
      "2498/2498 [==============================] - 176s 70ms/step - loss: 0.6230 - accuracy: 0.7350\n",
      "Epoch 10/100\n",
      "2498/2498 [==============================] - 190s 76ms/step - loss: 0.6168 - accuracy: 0.7367\n",
      "Epoch 11/100\n",
      "2498/2498 [==============================] - 210s 84ms/step - loss: 0.6104 - accuracy: 0.7364\n",
      "Epoch 12/100\n",
      "2498/2498 [==============================] - 191s 76ms/step - loss: 0.6041 - accuracy: 0.7364\n",
      "Epoch 13/100\n",
      "2498/2498 [==============================] - 177s 71ms/step - loss: 0.5985 - accuracy: 0.7385\n",
      "Epoch 14/100\n",
      "2498/2498 [==============================] - 176s 71ms/step - loss: 0.5935 - accuracy: 0.7394\n",
      "Epoch 15/100\n",
      "2498/2498 [==============================] - 181s 72ms/step - loss: 0.5875 - accuracy: 0.7411\n",
      "Epoch 16/100\n",
      "2498/2498 [==============================] - 177s 71ms/step - loss: 0.5831 - accuracy: 0.7422\n",
      "Epoch 17/100\n",
      "2498/2498 [==============================] - 179s 72ms/step - loss: 0.5778 - accuracy: 0.7446\n",
      "Epoch 18/100\n",
      "2498/2498 [==============================] - 179s 71ms/step - loss: 0.5731 - accuracy: 0.7449\n",
      "Epoch 19/100\n",
      "2498/2498 [==============================] - 175s 70ms/step - loss: 0.5683 - accuracy: 0.7471\n",
      "Epoch 20/100\n",
      "2498/2498 [==============================] - 180s 72ms/step - loss: 0.5637 - accuracy: 0.7479\n",
      "Epoch 21/100\n",
      "2498/2498 [==============================] - 177s 71ms/step - loss: 0.5590 - accuracy: 0.7490\n",
      "Epoch 22/100\n",
      "2498/2498 [==============================] - 178s 71ms/step - loss: 0.5562 - accuracy: 0.7504\n",
      "Epoch 23/100\n",
      "2498/2498 [==============================] - 178s 71ms/step - loss: 0.5544 - accuracy: 0.7501\n",
      "Epoch 24/100\n",
      "2498/2498 [==============================] - 177s 71ms/step - loss: 0.5520 - accuracy: 0.7509\n",
      "Epoch 25/100\n",
      "2498/2498 [==============================] - 179s 71ms/step - loss: 0.5505 - accuracy: 0.7510\n",
      "Epoch 26/100\n",
      "2498/2498 [==============================] - 177s 71ms/step - loss: 0.5494 - accuracy: 0.7505\n",
      "Epoch 27/100\n",
      "2498/2498 [==============================] - 179s 72ms/step - loss: 0.5474 - accuracy: 0.7524\n",
      "Epoch 28/100\n",
      "2498/2498 [==============================] - 179s 72ms/step - loss: 0.5457 - accuracy: 0.7525\n",
      "Epoch 29/100\n",
      "2498/2498 [==============================] - 177s 71ms/step - loss: 0.5444 - accuracy: 0.7554\n",
      "Epoch 30/100\n",
      "2498/2498 [==============================] - 179s 72ms/step - loss: 0.5436 - accuracy: 0.7556\n",
      "Epoch 31/100\n",
      "2498/2498 [==============================] - 175s 70ms/step - loss: 0.5420 - accuracy: 0.7556\n",
      "Epoch 32/100\n",
      "2498/2498 [==============================] - 176s 71ms/step - loss: 0.5412 - accuracy: 0.7548\n",
      "Epoch 33/100\n",
      "2498/2498 [==============================] - 175s 70ms/step - loss: 0.5406 - accuracy: 0.7546\n",
      "Epoch 34/100\n",
      "2498/2498 [==============================] - 177s 71ms/step - loss: 0.5394 - accuracy: 0.7563\n",
      "Epoch 35/100\n",
      "2498/2498 [==============================] - 184s 74ms/step - loss: 0.5390 - accuracy: 0.7552\n",
      "Epoch 36/100\n",
      "2498/2498 [==============================] - 184s 74ms/step - loss: 0.5379 - accuracy: 0.7557\n",
      "Epoch 37/100\n",
      "2498/2498 [==============================] - 182s 73ms/step - loss: 0.5365 - accuracy: 0.7567\n",
      "Epoch 38/100\n",
      "2498/2498 [==============================] - 173s 69ms/step - loss: 0.5350 - accuracy: 0.7581\n",
      "Epoch 39/100\n",
      "2498/2498 [==============================] - 175s 70ms/step - loss: 0.5351 - accuracy: 0.7565\n",
      "Epoch 40/100\n",
      "2498/2498 [==============================] - 178s 71ms/step - loss: 0.5343 - accuracy: 0.7570\n",
      "Epoch 41/100\n",
      "2498/2498 [==============================] - 177s 71ms/step - loss: 0.5336 - accuracy: 0.7563\n",
      "Epoch 42/100\n",
      "2498/2498 [==============================] - 176s 71ms/step - loss: 0.5332 - accuracy: 0.7571\n",
      "Epoch 43/100\n",
      "2498/2498 [==============================] - 175s 70ms/step - loss: 0.5321 - accuracy: 0.7585\n",
      "Epoch 44/100\n",
      "2498/2498 [==============================] - 178s 71ms/step - loss: 0.5314 - accuracy: 0.7593\n",
      "Epoch 45/100\n",
      "2498/2498 [==============================] - 176s 70ms/step - loss: 0.5306 - accuracy: 0.7584\n",
      "Epoch 46/100\n",
      "2498/2498 [==============================] - 175s 70ms/step - loss: 0.5300 - accuracy: 0.7592\n",
      "Epoch 47/100\n",
      "2498/2498 [==============================] - 177s 71ms/step - loss: 0.5302 - accuracy: 0.7585\n",
      "Epoch 48/100\n",
      "2498/2498 [==============================] - 176s 71ms/step - loss: 0.5294 - accuracy: 0.7596\n",
      "Epoch 49/100\n",
      "2498/2498 [==============================] - 178s 71ms/step - loss: 0.5280 - accuracy: 0.7595\n",
      "Epoch 50/100\n",
      "2498/2498 [==============================] - 179s 72ms/step - loss: 0.5280 - accuracy: 0.7605\n",
      "Epoch 51/100\n",
      "2498/2498 [==============================] - 178s 71ms/step - loss: 0.5277 - accuracy: 0.7593\n",
      "Epoch 52/100\n",
      "2498/2498 [==============================] - 177s 71ms/step - loss: 0.5272 - accuracy: 0.7602\n",
      "Epoch 53/100\n",
      "2498/2498 [==============================] - 172s 69ms/step - loss: 0.5274 - accuracy: 0.7595\n",
      "Epoch 54/100\n",
      "2498/2498 [==============================] - 175s 70ms/step - loss: 0.5259 - accuracy: 0.7613\n",
      "Epoch 55/100\n",
      "2498/2498 [==============================] - 173s 69ms/step - loss: 0.5258 - accuracy: 0.7600\n",
      "Epoch 56/100\n",
      "2498/2498 [==============================] - 172s 69ms/step - loss: 0.5250 - accuracy: 0.7605\n",
      "Epoch 57/100\n",
      "2498/2498 [==============================] - 174s 70ms/step - loss: 0.5247 - accuracy: 0.7611\n",
      "Epoch 58/100\n",
      "2498/2498 [==============================] - 171s 68ms/step - loss: 0.5242 - accuracy: 0.7611\n",
      "Epoch 59/100\n",
      "2498/2498 [==============================] - 172s 69ms/step - loss: 0.5239 - accuracy: 0.7607\n",
      "Epoch 60/100\n",
      "2498/2498 [==============================] - 170s 68ms/step - loss: 0.5234 - accuracy: 0.7607\n",
      "Epoch 61/100\n",
      "2498/2498 [==============================] - 172s 69ms/step - loss: 0.5223 - accuracy: 0.7619\n",
      "Epoch 62/100\n",
      "2498/2498 [==============================] - 173s 69ms/step - loss: 0.5225 - accuracy: 0.7625\n",
      "Epoch 63/100\n",
      "2498/2498 [==============================] - 173s 69ms/step - loss: 0.5219 - accuracy: 0.7621\n",
      "Epoch 64/100\n",
      "2498/2498 [==============================] - 172s 69ms/step - loss: 0.5216 - accuracy: 0.7619\n",
      "Epoch 65/100\n",
      "2498/2498 [==============================] - 174s 70ms/step - loss: 0.5209 - accuracy: 0.7619\n",
      "Epoch 66/100\n",
      "2498/2498 [==============================] - 173s 69ms/step - loss: 0.5208 - accuracy: 0.7634\n",
      "Epoch 67/100\n",
      "2498/2498 [==============================] - 172s 69ms/step - loss: 0.5203 - accuracy: 0.7619\n",
      "Epoch 68/100\n",
      "2498/2498 [==============================] - 175s 70ms/step - loss: 0.5198 - accuracy: 0.7625\n",
      "Epoch 69/100\n",
      "2498/2498 [==============================] - 173s 69ms/step - loss: 0.5202 - accuracy: 0.7625\n",
      "Epoch 70/100\n",
      "2498/2498 [==============================] - 172s 69ms/step - loss: 0.5189 - accuracy: 0.7641\n",
      "Epoch 71/100\n",
      "2498/2498 [==============================] - 172s 69ms/step - loss: 0.5189 - accuracy: 0.7631\n",
      "Epoch 72/100\n",
      "2498/2498 [==============================] - 174s 70ms/step - loss: 0.5180 - accuracy: 0.7642\n",
      "Epoch 73/100\n",
      "2498/2498 [==============================] - 175s 70ms/step - loss: 0.5180 - accuracy: 0.7635\n",
      "Epoch 74/100\n",
      "2498/2498 [==============================] - 174s 69ms/step - loss: 0.5176 - accuracy: 0.7644\n",
      "Epoch 75/100\n",
      "2498/2498 [==============================] - 177s 71ms/step - loss: 0.5170 - accuracy: 0.7647\n",
      "Epoch 76/100\n",
      "2498/2498 [==============================] - 175s 70ms/step - loss: 0.5165 - accuracy: 0.7648\n",
      "Epoch 77/100\n",
      "2498/2498 [==============================] - 171s 68ms/step - loss: 0.5170 - accuracy: 0.7642\n",
      "Epoch 78/100\n",
      "2498/2498 [==============================] - 173s 69ms/step - loss: 0.5163 - accuracy: 0.7644\n",
      "Epoch 79/100\n",
      "2498/2498 [==============================] - 173s 69ms/step - loss: 0.5159 - accuracy: 0.7652\n",
      "Epoch 80/100\n",
      "2498/2498 [==============================] - 173s 69ms/step - loss: 0.5150 - accuracy: 0.7663\n",
      "Epoch 81/100\n",
      "2498/2498 [==============================] - 172s 69ms/step - loss: 0.5153 - accuracy: 0.7657\n",
      "Epoch 82/100\n",
      "2498/2498 [==============================] - 173s 69ms/step - loss: 0.5146 - accuracy: 0.7648\n",
      "Epoch 83/100\n",
      "2498/2498 [==============================] - 175s 70ms/step - loss: 0.5143 - accuracy: 0.7654\n",
      "Epoch 84/100\n",
      "2498/2498 [==============================] - 172s 69ms/step - loss: 0.5143 - accuracy: 0.7652\n",
      "Epoch 85/100\n",
      "2498/2498 [==============================] - 173s 69ms/step - loss: 0.5143 - accuracy: 0.7657\n",
      "Epoch 86/100\n",
      "2498/2498 [==============================] - 172s 69ms/step - loss: 0.5133 - accuracy: 0.7674\n",
      "Epoch 87/100\n",
      "2498/2498 [==============================] - 173s 69ms/step - loss: 0.5131 - accuracy: 0.7658\n",
      "Epoch 88/100\n",
      "2498/2498 [==============================] - 172s 69ms/step - loss: 0.5129 - accuracy: 0.7668\n",
      "Epoch 89/100\n",
      "2498/2498 [==============================] - 173s 69ms/step - loss: 0.5127 - accuracy: 0.7661\n",
      "Epoch 90/100\n",
      "2498/2498 [==============================] - 174s 70ms/step - loss: 0.5122 - accuracy: 0.7669\n",
      "Epoch 91/100\n",
      "2498/2498 [==============================] - 174s 70ms/step - loss: 0.5118 - accuracy: 0.7668\n",
      "Epoch 92/100\n",
      "2498/2498 [==============================] - 175s 70ms/step - loss: 0.5115 - accuracy: 0.7673\n",
      "Epoch 93/100\n",
      "2498/2498 [==============================] - 174s 70ms/step - loss: 0.5110 - accuracy: 0.7675\n",
      "Epoch 94/100\n",
      "2498/2498 [==============================] - 175s 70ms/step - loss: 0.5107 - accuracy: 0.7672\n",
      "Epoch 95/100\n",
      "2498/2498 [==============================] - 176s 71ms/step - loss: 0.5108 - accuracy: 0.7680\n",
      "Epoch 96/100\n",
      "2498/2498 [==============================] - 175s 70ms/step - loss: 0.5102 - accuracy: 0.7684\n",
      "Epoch 97/100\n",
      "2498/2498 [==============================] - 177s 71ms/step - loss: 0.5099 - accuracy: 0.7671\n",
      "Epoch 98/100\n",
      "2498/2498 [==============================] - 175s 70ms/step - loss: 0.5096 - accuracy: 0.7682\n",
      "Epoch 99/100\n",
      "2498/2498 [==============================] - 175s 70ms/step - loss: 0.5092 - accuracy: 0.7683\n",
      "Epoch 100/100\n",
      "2498/2498 [==============================] - 174s 70ms/step - loss: 0.5093 - accuracy: 0.7684\n"
     ]
    }
   ],
   "source": [
    "# train_Y = to_categorical(train_Y) # y is the next event's mid price (k=1)\n",
    "# model.fit(train_X, train_Y, epochs=max_epochs, batch_size=batch_size, callbacks=[es])\n",
    "# model_params['model'] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_params['model'].save('CNN-LSTM-SAVED.hdf5', overwrite=True)  # creates a HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model_params['model'] = load_model('CNN-LSTM-SAVED.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metrics for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "623/623 [==============================] - 23s 34ms/step - loss: 0.7190 - accuracy: 0.6878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7190269827842712, 0.6878136992454529]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X, test_Y = get_lagged_features(ofi_test)\n",
    "test_Y = test_Y.astype(int)\n",
    "\n",
    "test_Y = to_categorical(test_Y)\n",
    "model_params['model'].evaluate(test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "623/623 [==============================] - 23s 35ms/step\n"
     ]
    }
   ],
   "source": [
    "model_params['pred_test'] = model_params['model'].predict(test_X, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.45513463, 0.31783095, 0.22703438],\n",
       "       [0.0062182 , 0.9849231 , 0.00885864],\n",
       "       [0.47333342, 0.33191097, 0.1947556 ],\n",
       "       ...,\n",
       "       [0.00974436, 0.9790339 , 0.0112218 ],\n",
       "       [0.38841417, 0.49786842, 0.11371739],\n",
       "       [0.00994822, 0.97791284, 0.0121389 ]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params['pred_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_Y = np.zeros(test_Y.shape)\n",
    "for i in range(len(test_Y)):\n",
    "    pred_Y[i,np.argmax(model_params['pred_test'][i])] = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down       0.38      0.43      0.41      3542\n",
      "  Stationary       0.80      0.86      0.83     12733\n",
      "          Up       0.55      0.33      0.41      3649\n",
      "\n",
      "   micro avg       0.69      0.69      0.69     19924\n",
      "   macro avg       0.58      0.54      0.55     19924\n",
      "weighted avg       0.68      0.69      0.68     19924\n",
      " samples avg       0.69      0.69      0.69     19924\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_Y, pred_Y, target_names=['Down', 'Stationary', 'Up']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "mat_con = confusion_matrix(test_Y.argmax(axis=1), pred_Y.argmax(axis=1))\n",
    "\n",
    "fig, px = plt.subplots(figsize=(5,5))\n",
    "px.matshow(mat_con, cmap=plt.cm.Blues, alpha=0.5)\n",
    "\n",
    "for m in range(mat_con.shape[0]):\n",
    "    for n in range(mat_con.shape[1]):\n",
    "        px.text(x=m, y=n, s=mat_con[m, n], va='center', ha='center', size='xx-large')\n",
    "\n",
    "plt.xlabel('Predictions', fontsize=16)\n",
    "plt.ylabel('True', fontsize=16)\n",
    "plt.xticks(np.arange(3), ['Down', 'Stationary', 'Up'])\n",
    "plt.yticks(np.arange(3), ['Down', 'Stationary', 'Up'], rotation='vertical', va='center')\n",
    "plt.title('Confusion Matrix', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Trading Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, is there a profitable trading strategy utilizing this model? How risky is it?\n",
    "\n",
    "To answer this, we compare two trading strategies using our test set as the trading period.\n",
    "\n",
    "We trade the midpoint of the limit order book, so let's create a vector of the true midpoints we trade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_midpoints = (dataframe.iloc[1+int(len(ofi_data)*train_weight):]['PRICE_ASK_0'] + dataframe.iloc[1+int(len(ofi_data)*train_weight):]['PRICE_BID_0']) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Trading Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this strategy, we choose a threshold probability $\\alpha$ and go long if $p_{+1,t}>\\alpha$ and go short if $p_{-1,t}>\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.7950000000128 5\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.8\n",
    "position = None\n",
    "entry_last = None\n",
    "pnl = 0\n",
    "num_trades = 0\n",
    "for i in range(len(model_params['pred_test'])):\n",
    "    if position != 'LONG' and np.argmax(model_params['pred_test'][i]) == 2 and np.max(model_params['pred_test'][i]) > threshold:\n",
    "        # Go long\n",
    "        position = 'LONG'\n",
    "        entry_now = test_midpoints.iloc[i]\n",
    "        if entry_last != None:\n",
    "            pnl += entry_now - entry_last\n",
    "        entry_last = entry_now\n",
    "        num_trades += 1\n",
    "    if position != 'SHORT' and np.argmax(model_params['pred_test'][i]) == 0 and np.max(model_params['pred_test'][i]) > threshold:\n",
    "        # Go long\n",
    "        position = 'SHORT'\n",
    "        entry_now = test_midpoints.iloc[i]\n",
    "        if entry_last != None:\n",
    "            pnl += entry_last - entry_now\n",
    "        entry_last = entry_now\n",
    "        num_trades += 1\n",
    "print(pnl, num_trades)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Trading Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This strategy uses variational ratios, predictive entropy, and mutual information to summarize classification uncertainty due to variational dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortfalls of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep neural networks as a \"black box\"\n",
    "\n",
    "\"Offline\" version of learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Zhang, Zihao, Stefan Zohren, and Stephen Roberts (2019). “DeepLOB: Deep Convolutional Neural Networks for Limit Order Books”. In: *IEEE Transactions On Signal Processing* 67.11, pp. 3001–3012.<br>\n",
    "- Cont, Rama, Arseniy Kukanov, and Sasha Stoikov (2014). “The Price Impact Of\n",
    "Order Book Events”. In: *Journal Of Financial Econometrics* 12.1, pp. 47–88.<br>\n",
    "- Kolm, Petter, Jeremy Turiel, and Nicholas Westray (2021). \"Deep Order Flow Imbalance: Extracting Alpha At Multiple Horizons From The Limit Order Book\". In: *Available at SSRN 3900141*.<br>\n",
    "- Ntakaris, Adamantios, Martin Magris, Juho Kanniainen, Moncef Gabbouj, and Alexandros Iosifidis (2018). “Benchmark Dataset For Mid-Price Forecasting Of Limit Order Book Data With Machine Learning Methods”. In: *Journal of Forecasting* 37.8, pp. 852–866.<br>\n",
    "- Szegedy, Christian, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich (2015). “Going Deeper With Convolutions”. In: *Proceedings Of The IEEE Conference On Computer Vision And Pattern Recognition*, pp. 1–9.<br>\n",
    "- Zhang, Zihao, Stefan Zohren, and Stephen Roberts (2018). “BDLOB: Bayesian Deep Convolutional Neural Networks For Limit Order Books”. In: *arXiv preprint arXiv:1811.10041*.<br>\n",
    "- Dixon, Matthew F., Igor Halperin, and Paul Bilokon (2020). *Machine Learning in Finance: From Theory to Practice*. Springer.<br>\n",
    "- Tsay, Ruey S. (2010). *Analysis of Financial Time Series* (3rd ed.). Wiley.<br>\n",
    "- Lütkepohl, Helmut (2005). *New Introduction to Multiple Time Series Analysis*. Springer.<br> \n",
    "- Yang, Kiyoung, and Cyrus Shahabi (2005). \"On the Stationarity of Multivariate Time Series for Correlation-Based Data Analysis\". In: *Fifth IEEE International Conference on Data Mining*, pp. 1-4.<br>\n",
    "- Johansen, Søren (1995). *Likelihood-Based Inference in Cointegrated Vector Autoregressive Models*. Oxford University Press.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b6c5c2265085fc676d0e3932a49fa51d673497428dc888174fbb13fc54974a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
