{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order Flow Imbalance and Advanced Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Aric Cutuli<br>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The enigma concerning the predictability of markets has always been the principal driver of my interest in finance, and it inpires my ongoing exploration of machine learning's applications within financial modeling. Today, we delve into recent literary discussions concerning the intersection of supervised learning and limit order book forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we implement an advanced neural network originally employed by (Zhang et al) that combines convolutional neural networks (CNNs) and a long-short term memory network (LSTM) in order to classify future directions of an order book at a high frequency. Unlike the works of (Zhang et al), which use raw nonstationary order book states as the input to the network, our instantiation of the architecture is trained on order flow, which are stationary quantities derived from the limit order book (Cont et al). Hence, this discussion also draws heavy inspiration from a recent article (Kolm et al) that uses order flow and supervised learning to formulate the order book forecasting problem as one of regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extend the discussions of the aforementioned papers, we build a trading signal based off of the trained network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limit Order Book and Order Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modern equity trading is conducted electronically and is often facilitated by a limit order book. The order book collects bids and offers made by prospective buyers and sellers. The bid price is the highest price buyers are prepared to buy at, and the ask price is the lowest price sellers are willing to sell at. An order is submitted with the inputs side, quantity, price, and time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Forecasting Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Producing a forecast is simply a matter of taking the conditional expectation of the data under the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction of microstructure data has generically been handled with PCA or LDA, but here we adopt CNNs and Inception Modules to accomplish this task.\n",
    "\n",
    "CNNs are used to automate the feature extraction process instead of feature engineering and PCA. (Zhang et al)\n",
    "\n",
    "CNNs attempt to reduce the network size by exploiting data locality (Dixon + Halperin)\n",
    "\n",
    "in financial modeling, we typically have different spatial structures, such as the limit order book depths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inception module increases width and depth of the convolutions, hence helps capture local interactions bewtween timestamps. (Szegedy et al) Infer decay rates of the autoregressive process. (Zhang et al)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cbpro import PublicClient\n",
    "# from time import time, strftime, gmtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7216747066254201\n"
     ]
    }
   ],
   "source": [
    "# public_client = PublicClient()\n",
    "# dataframe = pd.DataFrame()\n",
    "# start = time()\n",
    "# while len(dataframe) < 100002:\n",
    "#     raw_data = pd.concat((pd.DataFrame.from_dict(public_client.get_product_order_book('BTC-USD',level=2)['asks'])[:10],\n",
    "#                      pd.DataFrame.from_dict(public_client.get_product_order_book('BTC-USD',level=2)['bids'])[:10]),axis=1)\n",
    "#     dataframe = pd.concat((dataframe, pd.concat((pd.DataFrame(raw_data.drop(2,axis=1).iloc[i]).T for i in range(10)), axis=1).apply(lambda x: pd.Series(x.dropna().values))))\n",
    "# end = time()\n",
    "# print((end-start)/len(dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>...</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20690</td>\n",
       "      <td>0.08430901</td>\n",
       "      <td>20689.99</td>\n",
       "      <td>0.01128925</td>\n",
       "      <td>20690.02</td>\n",
       "      <td>0.00014259</td>\n",
       "      <td>20689.98</td>\n",
       "      <td>0.00034795</td>\n",
       "      <td>20691.38</td>\n",
       "      <td>0.98125</td>\n",
       "      <td>...</td>\n",
       "      <td>20686.68</td>\n",
       "      <td>0.00029957</td>\n",
       "      <td>20695.72</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20685.97</td>\n",
       "      <td>0.00340121</td>\n",
       "      <td>20696.33</td>\n",
       "      <td>0.3582382</td>\n",
       "      <td>20685.65</td>\n",
       "      <td>0.0021585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20690</td>\n",
       "      <td>0.08430901</td>\n",
       "      <td>20689.99</td>\n",
       "      <td>0.01128925</td>\n",
       "      <td>20690.02</td>\n",
       "      <td>0.00014259</td>\n",
       "      <td>20689.98</td>\n",
       "      <td>0.00034795</td>\n",
       "      <td>20691.38</td>\n",
       "      <td>0.98125</td>\n",
       "      <td>...</td>\n",
       "      <td>20686.68</td>\n",
       "      <td>0.00029957</td>\n",
       "      <td>20695.72</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20685.97</td>\n",
       "      <td>0.00340121</td>\n",
       "      <td>20696.33</td>\n",
       "      <td>0.3582382</td>\n",
       "      <td>20685.65</td>\n",
       "      <td>0.0021585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20690</td>\n",
       "      <td>0.08430901</td>\n",
       "      <td>20689.99</td>\n",
       "      <td>0.01128925</td>\n",
       "      <td>20690.02</td>\n",
       "      <td>0.00014259</td>\n",
       "      <td>20689.98</td>\n",
       "      <td>0.00034795</td>\n",
       "      <td>20691.38</td>\n",
       "      <td>0.98125</td>\n",
       "      <td>...</td>\n",
       "      <td>20686.68</td>\n",
       "      <td>0.00029957</td>\n",
       "      <td>20695.72</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20685.97</td>\n",
       "      <td>0.00340121</td>\n",
       "      <td>20696.33</td>\n",
       "      <td>0.3582382</td>\n",
       "      <td>20685.65</td>\n",
       "      <td>0.0021585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20690</td>\n",
       "      <td>0.08430901</td>\n",
       "      <td>20689.65</td>\n",
       "      <td>0.005</td>\n",
       "      <td>20690.02</td>\n",
       "      <td>0.00014259</td>\n",
       "      <td>20689.64</td>\n",
       "      <td>0.00621342</td>\n",
       "      <td>20691.38</td>\n",
       "      <td>0.98125</td>\n",
       "      <td>...</td>\n",
       "      <td>20685.58</td>\n",
       "      <td>0.15</td>\n",
       "      <td>20695.72</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20685.57</td>\n",
       "      <td>2.0432</td>\n",
       "      <td>20696.33</td>\n",
       "      <td>0.3582382</td>\n",
       "      <td>20685.32</td>\n",
       "      <td>0.01831666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20689.88</td>\n",
       "      <td>0.01727754</td>\n",
       "      <td>20689.83</td>\n",
       "      <td>0.0117994</td>\n",
       "      <td>20689.89</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>20686.69</td>\n",
       "      <td>0.00384091</td>\n",
       "      <td>20689.9</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>...</td>\n",
       "      <td>20682.96</td>\n",
       "      <td>0.0247245</td>\n",
       "      <td>20692</td>\n",
       "      <td>0.2314877</td>\n",
       "      <td>20682.78</td>\n",
       "      <td>0.02554214</td>\n",
       "      <td>20693.47</td>\n",
       "      <td>0.01620968</td>\n",
       "      <td>20682.1</td>\n",
       "      <td>0.24163268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21071.31</td>\n",
       "      <td>0.00532448</td>\n",
       "      <td>21068.78</td>\n",
       "      <td>0.09458639</td>\n",
       "      <td>21071.35</td>\n",
       "      <td>0.01331261</td>\n",
       "      <td>21068.65</td>\n",
       "      <td>0.08</td>\n",
       "      <td>21071.41</td>\n",
       "      <td>0.005</td>\n",
       "      <td>...</td>\n",
       "      <td>21066.4</td>\n",
       "      <td>0.23726989</td>\n",
       "      <td>21073.46</td>\n",
       "      <td>0.01308631</td>\n",
       "      <td>21066.24</td>\n",
       "      <td>0.12</td>\n",
       "      <td>21073.48</td>\n",
       "      <td>0.02107213</td>\n",
       "      <td>21065.65</td>\n",
       "      <td>0.35587299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21071.31</td>\n",
       "      <td>0.00532448</td>\n",
       "      <td>21068.78</td>\n",
       "      <td>0.09458639</td>\n",
       "      <td>21071.35</td>\n",
       "      <td>0.01331261</td>\n",
       "      <td>21068.65</td>\n",
       "      <td>0.08</td>\n",
       "      <td>21071.41</td>\n",
       "      <td>0.005</td>\n",
       "      <td>...</td>\n",
       "      <td>21066.4</td>\n",
       "      <td>0.23726989</td>\n",
       "      <td>21073.46</td>\n",
       "      <td>0.01308631</td>\n",
       "      <td>21066.24</td>\n",
       "      <td>0.12</td>\n",
       "      <td>21073.48</td>\n",
       "      <td>0.02107213</td>\n",
       "      <td>21065.65</td>\n",
       "      <td>0.35587299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21064.51</td>\n",
       "      <td>0.11129266</td>\n",
       "      <td>21064.5</td>\n",
       "      <td>0.005</td>\n",
       "      <td>21064.6</td>\n",
       "      <td>0.04174136</td>\n",
       "      <td>21061.1</td>\n",
       "      <td>0.74445</td>\n",
       "      <td>21065.25</td>\n",
       "      <td>0.00872623</td>\n",
       "      <td>...</td>\n",
       "      <td>21059.31</td>\n",
       "      <td>0.35596314</td>\n",
       "      <td>21066.8</td>\n",
       "      <td>0.00965513</td>\n",
       "      <td>21058.89</td>\n",
       "      <td>0.00493738</td>\n",
       "      <td>21067</td>\n",
       "      <td>0.22734925</td>\n",
       "      <td>21057.91</td>\n",
       "      <td>0.09857907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21064.19</td>\n",
       "      <td>0.09499795</td>\n",
       "      <td>21061.1</td>\n",
       "      <td>0.74445</td>\n",
       "      <td>21064.25</td>\n",
       "      <td>0.01235664</td>\n",
       "      <td>21059.36</td>\n",
       "      <td>0.00208697</td>\n",
       "      <td>21064.31</td>\n",
       "      <td>0.22737597</td>\n",
       "      <td>...</td>\n",
       "      <td>21057.09</td>\n",
       "      <td>0.19599322</td>\n",
       "      <td>21065.25</td>\n",
       "      <td>0.00920369</td>\n",
       "      <td>21056.68</td>\n",
       "      <td>0.00096918</td>\n",
       "      <td>21065.26</td>\n",
       "      <td>0.68619654</td>\n",
       "      <td>21056.15</td>\n",
       "      <td>0.05877771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21061.25</td>\n",
       "      <td>0.05336</td>\n",
       "      <td>21061.24</td>\n",
       "      <td>0.005</td>\n",
       "      <td>21062.41</td>\n",
       "      <td>0.025</td>\n",
       "      <td>21061.12</td>\n",
       "      <td>0.005</td>\n",
       "      <td>21062.56</td>\n",
       "      <td>0.01647658</td>\n",
       "      <td>...</td>\n",
       "      <td>21057.51</td>\n",
       "      <td>0.04928953</td>\n",
       "      <td>21064.43</td>\n",
       "      <td>0.00920369</td>\n",
       "      <td>21057.11</td>\n",
       "      <td>0.05877771</td>\n",
       "      <td>21064.44</td>\n",
       "      <td>0.00920369</td>\n",
       "      <td>21057.1</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100002 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0           1         0           1         0           1  \\\n",
       "0      20690  0.08430901  20689.99  0.01128925  20690.02  0.00014259   \n",
       "0      20690  0.08430901  20689.99  0.01128925  20690.02  0.00014259   \n",
       "0      20690  0.08430901  20689.99  0.01128925  20690.02  0.00014259   \n",
       "0      20690  0.08430901  20689.65       0.005  20690.02  0.00014259   \n",
       "0   20689.88  0.01727754  20689.83   0.0117994  20689.89      0.0001   \n",
       "..       ...         ...       ...         ...       ...         ...   \n",
       "0   21071.31  0.00532448  21068.78  0.09458639  21071.35  0.01331261   \n",
       "0   21071.31  0.00532448  21068.78  0.09458639  21071.35  0.01331261   \n",
       "0   21064.51  0.11129266   21064.5       0.005   21064.6  0.04174136   \n",
       "0   21064.19  0.09499795   21061.1     0.74445  21064.25  0.01235664   \n",
       "0   21061.25     0.05336  21061.24       0.005  21062.41       0.025   \n",
       "\n",
       "           0           1         0           1  ...         0           1  \\\n",
       "0   20689.98  0.00034795  20691.38     0.98125  ...  20686.68  0.00029957   \n",
       "0   20689.98  0.00034795  20691.38     0.98125  ...  20686.68  0.00029957   \n",
       "0   20689.98  0.00034795  20691.38     0.98125  ...  20686.68  0.00029957   \n",
       "0   20689.64  0.00621342  20691.38     0.98125  ...  20685.58        0.15   \n",
       "0   20686.69  0.00384091   20689.9      0.0001  ...  20682.96   0.0247245   \n",
       "..       ...         ...       ...         ...  ...       ...         ...   \n",
       "0   21068.65        0.08  21071.41       0.005  ...   21066.4  0.23726989   \n",
       "0   21068.65        0.08  21071.41       0.005  ...   21066.4  0.23726989   \n",
       "0    21061.1     0.74445  21065.25  0.00872623  ...  21059.31  0.35596314   \n",
       "0   21059.36  0.00208697  21064.31  0.22737597  ...  21057.09  0.19599322   \n",
       "0   21061.12       0.005  21062.56  0.01647658  ...  21057.51  0.04928953   \n",
       "\n",
       "           0           1         0           1         0           1  \\\n",
       "0   20695.72         0.1  20685.97  0.00340121  20696.33   0.3582382   \n",
       "0   20695.72         0.1  20685.97  0.00340121  20696.33   0.3582382   \n",
       "0   20695.72         0.1  20685.97  0.00340121  20696.33   0.3582382   \n",
       "0   20695.72         0.1  20685.57      2.0432  20696.33   0.3582382   \n",
       "0      20692   0.2314877  20682.78  0.02554214  20693.47  0.01620968   \n",
       "..       ...         ...       ...         ...       ...         ...   \n",
       "0   21073.46  0.01308631  21066.24        0.12  21073.48  0.02107213   \n",
       "0   21073.46  0.01308631  21066.24        0.12  21073.48  0.02107213   \n",
       "0    21066.8  0.00965513  21058.89  0.00493738     21067  0.22734925   \n",
       "0   21065.25  0.00920369  21056.68  0.00096918  21065.26  0.68619654   \n",
       "0   21064.43  0.00920369  21057.11  0.05877771  21064.44  0.00920369   \n",
       "\n",
       "           0           1  \n",
       "0   20685.65   0.0021585  \n",
       "0   20685.65   0.0021585  \n",
       "0   20685.65   0.0021585  \n",
       "0   20685.32  0.01831666  \n",
       "0    20682.1  0.24163268  \n",
       "..       ...         ...  \n",
       "0   21065.65  0.35587299  \n",
       "0   21065.65  0.35587299  \n",
       "0   21057.91  0.09857907  \n",
       "0   21056.15  0.05877771  \n",
       "0    21057.1        0.75  \n",
       "\n",
       "[100002 rows x 40 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRICE_ASK_0</th>\n",
       "      <th>VOLUME_ASK_0</th>\n",
       "      <th>PRICE_BID_0</th>\n",
       "      <th>VOLUME_BID_0</th>\n",
       "      <th>PRICE_ASK_1</th>\n",
       "      <th>VOLUME_ASK_1</th>\n",
       "      <th>PRICE_BID_1</th>\n",
       "      <th>VOLUME_BID_1</th>\n",
       "      <th>PRICE_ASK_2</th>\n",
       "      <th>VOLUME_ASK_2</th>\n",
       "      <th>...</th>\n",
       "      <th>PRICE_BID_7</th>\n",
       "      <th>VOLUME_BID_7</th>\n",
       "      <th>PRICE_ASK_8</th>\n",
       "      <th>VOLUME_ASK_8</th>\n",
       "      <th>PRICE_BID_8</th>\n",
       "      <th>VOLUME_BID_8</th>\n",
       "      <th>PRICE_ASK_9</th>\n",
       "      <th>VOLUME_ASK_9</th>\n",
       "      <th>PRICE_BID_9</th>\n",
       "      <th>VOLUME_BID_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20690.00</td>\n",
       "      <td>0.084309</td>\n",
       "      <td>20689.99</td>\n",
       "      <td>0.011289</td>\n",
       "      <td>20690.02</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>20689.98</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>20691.38</td>\n",
       "      <td>0.981250</td>\n",
       "      <td>...</td>\n",
       "      <td>20686.68</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>20695.72</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>20685.97</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>20696.33</td>\n",
       "      <td>0.358238</td>\n",
       "      <td>20685.65</td>\n",
       "      <td>0.002158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20690.00</td>\n",
       "      <td>0.084309</td>\n",
       "      <td>20689.99</td>\n",
       "      <td>0.011289</td>\n",
       "      <td>20690.02</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>20689.98</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>20691.38</td>\n",
       "      <td>0.981250</td>\n",
       "      <td>...</td>\n",
       "      <td>20686.68</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>20695.72</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>20685.97</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>20696.33</td>\n",
       "      <td>0.358238</td>\n",
       "      <td>20685.65</td>\n",
       "      <td>0.002158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20690.00</td>\n",
       "      <td>0.084309</td>\n",
       "      <td>20689.99</td>\n",
       "      <td>0.011289</td>\n",
       "      <td>20690.02</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>20689.98</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>20691.38</td>\n",
       "      <td>0.981250</td>\n",
       "      <td>...</td>\n",
       "      <td>20686.68</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>20695.72</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>20685.97</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>20696.33</td>\n",
       "      <td>0.358238</td>\n",
       "      <td>20685.65</td>\n",
       "      <td>0.002158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20690.00</td>\n",
       "      <td>0.084309</td>\n",
       "      <td>20689.65</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>20690.02</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>20689.64</td>\n",
       "      <td>0.006213</td>\n",
       "      <td>20691.38</td>\n",
       "      <td>0.981250</td>\n",
       "      <td>...</td>\n",
       "      <td>20685.58</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>20695.72</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>20685.57</td>\n",
       "      <td>2.043200</td>\n",
       "      <td>20696.33</td>\n",
       "      <td>0.358238</td>\n",
       "      <td>20685.32</td>\n",
       "      <td>0.018317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20689.88</td>\n",
       "      <td>0.017278</td>\n",
       "      <td>20689.83</td>\n",
       "      <td>0.011799</td>\n",
       "      <td>20689.89</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>20686.69</td>\n",
       "      <td>0.003841</td>\n",
       "      <td>20689.90</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>...</td>\n",
       "      <td>20682.96</td>\n",
       "      <td>0.024724</td>\n",
       "      <td>20692.00</td>\n",
       "      <td>0.231488</td>\n",
       "      <td>20682.78</td>\n",
       "      <td>0.025542</td>\n",
       "      <td>20693.47</td>\n",
       "      <td>0.016210</td>\n",
       "      <td>20682.10</td>\n",
       "      <td>0.241633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>21071.31</td>\n",
       "      <td>0.005324</td>\n",
       "      <td>21068.78</td>\n",
       "      <td>0.094586</td>\n",
       "      <td>21071.35</td>\n",
       "      <td>0.013313</td>\n",
       "      <td>21068.65</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>21071.41</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>...</td>\n",
       "      <td>21066.40</td>\n",
       "      <td>0.237270</td>\n",
       "      <td>21073.46</td>\n",
       "      <td>0.013086</td>\n",
       "      <td>21066.24</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>21073.48</td>\n",
       "      <td>0.021072</td>\n",
       "      <td>21065.65</td>\n",
       "      <td>0.355873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>21071.31</td>\n",
       "      <td>0.005324</td>\n",
       "      <td>21068.78</td>\n",
       "      <td>0.094586</td>\n",
       "      <td>21071.35</td>\n",
       "      <td>0.013313</td>\n",
       "      <td>21068.65</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>21071.41</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>...</td>\n",
       "      <td>21066.40</td>\n",
       "      <td>0.237270</td>\n",
       "      <td>21073.46</td>\n",
       "      <td>0.013086</td>\n",
       "      <td>21066.24</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>21073.48</td>\n",
       "      <td>0.021072</td>\n",
       "      <td>21065.65</td>\n",
       "      <td>0.355873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>21064.51</td>\n",
       "      <td>0.111293</td>\n",
       "      <td>21064.50</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>21064.60</td>\n",
       "      <td>0.041741</td>\n",
       "      <td>21061.10</td>\n",
       "      <td>0.744450</td>\n",
       "      <td>21065.25</td>\n",
       "      <td>0.008726</td>\n",
       "      <td>...</td>\n",
       "      <td>21059.31</td>\n",
       "      <td>0.355963</td>\n",
       "      <td>21066.80</td>\n",
       "      <td>0.009655</td>\n",
       "      <td>21058.89</td>\n",
       "      <td>0.004937</td>\n",
       "      <td>21067.00</td>\n",
       "      <td>0.227349</td>\n",
       "      <td>21057.91</td>\n",
       "      <td>0.098579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>21064.19</td>\n",
       "      <td>0.094998</td>\n",
       "      <td>21061.10</td>\n",
       "      <td>0.744450</td>\n",
       "      <td>21064.25</td>\n",
       "      <td>0.012357</td>\n",
       "      <td>21059.36</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>21064.31</td>\n",
       "      <td>0.227376</td>\n",
       "      <td>...</td>\n",
       "      <td>21057.09</td>\n",
       "      <td>0.195993</td>\n",
       "      <td>21065.25</td>\n",
       "      <td>0.009204</td>\n",
       "      <td>21056.68</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>21065.26</td>\n",
       "      <td>0.686197</td>\n",
       "      <td>21056.15</td>\n",
       "      <td>0.058778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100001</th>\n",
       "      <td>21061.25</td>\n",
       "      <td>0.053360</td>\n",
       "      <td>21061.24</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>21062.41</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>21061.12</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>21062.56</td>\n",
       "      <td>0.016477</td>\n",
       "      <td>...</td>\n",
       "      <td>21057.51</td>\n",
       "      <td>0.049290</td>\n",
       "      <td>21064.43</td>\n",
       "      <td>0.009204</td>\n",
       "      <td>21057.11</td>\n",
       "      <td>0.058778</td>\n",
       "      <td>21064.44</td>\n",
       "      <td>0.009204</td>\n",
       "      <td>21057.10</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100002 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        PRICE_ASK_0  VOLUME_ASK_0  PRICE_BID_0  VOLUME_BID_0  PRICE_ASK_1  \\\n",
       "0          20690.00      0.084309     20689.99      0.011289     20690.02   \n",
       "1          20690.00      0.084309     20689.99      0.011289     20690.02   \n",
       "2          20690.00      0.084309     20689.99      0.011289     20690.02   \n",
       "3          20690.00      0.084309     20689.65      0.005000     20690.02   \n",
       "4          20689.88      0.017278     20689.83      0.011799     20689.89   \n",
       "...             ...           ...          ...           ...          ...   \n",
       "99997      21071.31      0.005324     21068.78      0.094586     21071.35   \n",
       "99998      21071.31      0.005324     21068.78      0.094586     21071.35   \n",
       "99999      21064.51      0.111293     21064.50      0.005000     21064.60   \n",
       "100000     21064.19      0.094998     21061.10      0.744450     21064.25   \n",
       "100001     21061.25      0.053360     21061.24      0.005000     21062.41   \n",
       "\n",
       "        VOLUME_ASK_1  PRICE_BID_1  VOLUME_BID_1  PRICE_ASK_2  VOLUME_ASK_2  \\\n",
       "0           0.000143     20689.98      0.000348     20691.38      0.981250   \n",
       "1           0.000143     20689.98      0.000348     20691.38      0.981250   \n",
       "2           0.000143     20689.98      0.000348     20691.38      0.981250   \n",
       "3           0.000143     20689.64      0.006213     20691.38      0.981250   \n",
       "4           0.000100     20686.69      0.003841     20689.90      0.000100   \n",
       "...              ...          ...           ...          ...           ...   \n",
       "99997       0.013313     21068.65      0.080000     21071.41      0.005000   \n",
       "99998       0.013313     21068.65      0.080000     21071.41      0.005000   \n",
       "99999       0.041741     21061.10      0.744450     21065.25      0.008726   \n",
       "100000      0.012357     21059.36      0.002087     21064.31      0.227376   \n",
       "100001      0.025000     21061.12      0.005000     21062.56      0.016477   \n",
       "\n",
       "        ...  PRICE_BID_7  VOLUME_BID_7  PRICE_ASK_8  VOLUME_ASK_8  \\\n",
       "0       ...     20686.68      0.000300     20695.72      0.100000   \n",
       "1       ...     20686.68      0.000300     20695.72      0.100000   \n",
       "2       ...     20686.68      0.000300     20695.72      0.100000   \n",
       "3       ...     20685.58      0.150000     20695.72      0.100000   \n",
       "4       ...     20682.96      0.024724     20692.00      0.231488   \n",
       "...     ...          ...           ...          ...           ...   \n",
       "99997   ...     21066.40      0.237270     21073.46      0.013086   \n",
       "99998   ...     21066.40      0.237270     21073.46      0.013086   \n",
       "99999   ...     21059.31      0.355963     21066.80      0.009655   \n",
       "100000  ...     21057.09      0.195993     21065.25      0.009204   \n",
       "100001  ...     21057.51      0.049290     21064.43      0.009204   \n",
       "\n",
       "        PRICE_BID_8  VOLUME_BID_8  PRICE_ASK_9  VOLUME_ASK_9  PRICE_BID_9  \\\n",
       "0          20685.97      0.003401     20696.33      0.358238     20685.65   \n",
       "1          20685.97      0.003401     20696.33      0.358238     20685.65   \n",
       "2          20685.97      0.003401     20696.33      0.358238     20685.65   \n",
       "3          20685.57      2.043200     20696.33      0.358238     20685.32   \n",
       "4          20682.78      0.025542     20693.47      0.016210     20682.10   \n",
       "...             ...           ...          ...           ...          ...   \n",
       "99997      21066.24      0.120000     21073.48      0.021072     21065.65   \n",
       "99998      21066.24      0.120000     21073.48      0.021072     21065.65   \n",
       "99999      21058.89      0.004937     21067.00      0.227349     21057.91   \n",
       "100000     21056.68      0.000969     21065.26      0.686197     21056.15   \n",
       "100001     21057.11      0.058778     21064.44      0.009204     21057.10   \n",
       "\n",
       "        VOLUME_BID_9  \n",
       "0           0.002158  \n",
       "1           0.002158  \n",
       "2           0.002158  \n",
       "3           0.018317  \n",
       "4           0.241633  \n",
       "...              ...  \n",
       "99997       0.355873  \n",
       "99998       0.355873  \n",
       "99999       0.098579  \n",
       "100000      0.058778  \n",
       "100001      0.750000  \n",
       "\n",
       "[100002 rows x 40 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.columns = ['PRICE_ASK_0','VOLUME_ASK_0','PRICE_BID_0','VOLUME_BID_0',\n",
    "           'PRICE_ASK_1','VOLUME_ASK_1','PRICE_BID_1','VOLUME_BID_1',\n",
    "           'PRICE_ASK_2','VOLUME_ASK_2','PRICE_BID_2','VOLUME_BID_2',\n",
    "           'PRICE_ASK_3','VOLUME_ASK_3','PRICE_BID_3','VOLUME_BID_3',\n",
    "           'PRICE_ASK_4','VOLUME_ASK_4','PRICE_BID_4','VOLUME_BID_4',\n",
    "           'PRICE_ASK_5','VOLUME_ASK_5','PRICE_BID_5','VOLUME_BID_5',\n",
    "           'PRICE_ASK_6','VOLUME_ASK_6','PRICE_BID_6','VOLUME_BID_6',\n",
    "           'PRICE_ASK_7','VOLUME_ASK_7','PRICE_BID_7','VOLUME_BID_7',\n",
    "           'PRICE_ASK_8','VOLUME_ASK_8','PRICE_BID_8','VOLUME_BID_8',\n",
    "           'PRICE_ASK_9','VOLUME_ASK_9','PRICE_BID_9','VOLUME_BID_9']\n",
    "dataframe.index = range(len(dataframe))\n",
    "dataframe = dataframe.astype(float)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the spirit of the FI-2010 dataset, we extract labels based on short-term\n",
    "and long-term, event-based, relative changes for the next 1, 2, 3, 5, and 10\n",
    "midpoints in the order book dataset. However, in lieu of comparable granularity in our dataset, we simply label relative changes for nly the next event.\n",
    "\n",
    "Our labels describe the percentage change of the mid-price\n",
    "\n",
    "The extracted labels are based\n",
    "on a threshold for the percentage change of 0.002. For percentage changes equal\n",
    "to or greater than 0.002, we use label 1. For percentage change that varies from\n",
    "-0.00199 to 0.00199, we use label 0, and, for percentage change smaller or equal\n",
    "to -0.002, we use label -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['LABEL_1TICK'] = np.zeros(len(dataframe))\n",
    "for i in range(len(dataframe)-1):\n",
    "    if (dataframe.loc[i+1,'PRICE_ASK_0'] + dataframe.loc[i+1,'PRICE_ASK_0']) > 1.00002*(dataframe.loc[i,'PRICE_ASK_0'] + dataframe.loc[i,'PRICE_ASK_0']):\n",
    "        dataframe['LABEL_1TICK'][i] = 1\n",
    "    elif (dataframe.loc[i+1,'PRICE_BID_0'] + dataframe.loc[i+1,'PRICE_BID_0']) < 0.99998*(dataframe.loc[i,'PRICE_BID_0'] + dataframe.loc[i,'PRICE_BID_0']):\n",
    "        dataframe['LABEL_1TICK'][i] = -1\n",
    "dataframe = dataframe.head(len(dataframe)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRICE_ASK_0</th>\n",
       "      <th>VOLUME_ASK_0</th>\n",
       "      <th>PRICE_BID_0</th>\n",
       "      <th>VOLUME_BID_0</th>\n",
       "      <th>PRICE_ASK_1</th>\n",
       "      <th>VOLUME_ASK_1</th>\n",
       "      <th>PRICE_BID_1</th>\n",
       "      <th>VOLUME_BID_1</th>\n",
       "      <th>PRICE_ASK_2</th>\n",
       "      <th>VOLUME_ASK_2</th>\n",
       "      <th>...</th>\n",
       "      <th>VOLUME_BID_7</th>\n",
       "      <th>PRICE_ASK_8</th>\n",
       "      <th>VOLUME_ASK_8</th>\n",
       "      <th>PRICE_BID_8</th>\n",
       "      <th>VOLUME_BID_8</th>\n",
       "      <th>PRICE_ASK_9</th>\n",
       "      <th>VOLUME_ASK_9</th>\n",
       "      <th>PRICE_BID_9</th>\n",
       "      <th>VOLUME_BID_9</th>\n",
       "      <th>LABEL_1TICK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20690.00</td>\n",
       "      <td>0.084309</td>\n",
       "      <td>20689.99</td>\n",
       "      <td>0.011289</td>\n",
       "      <td>20690.02</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>20689.98</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>20691.38</td>\n",
       "      <td>0.981250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>20695.72</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>20685.97</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>20696.33</td>\n",
       "      <td>0.358238</td>\n",
       "      <td>20685.65</td>\n",
       "      <td>0.002158</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20690.00</td>\n",
       "      <td>0.084309</td>\n",
       "      <td>20689.99</td>\n",
       "      <td>0.011289</td>\n",
       "      <td>20690.02</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>20689.98</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>20691.38</td>\n",
       "      <td>0.981250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>20695.72</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>20685.97</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>20696.33</td>\n",
       "      <td>0.358238</td>\n",
       "      <td>20685.65</td>\n",
       "      <td>0.002158</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20690.00</td>\n",
       "      <td>0.084309</td>\n",
       "      <td>20689.99</td>\n",
       "      <td>0.011289</td>\n",
       "      <td>20690.02</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>20689.98</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>20691.38</td>\n",
       "      <td>0.981250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>20695.72</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>20685.97</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>20696.33</td>\n",
       "      <td>0.358238</td>\n",
       "      <td>20685.65</td>\n",
       "      <td>0.002158</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20690.00</td>\n",
       "      <td>0.084309</td>\n",
       "      <td>20689.65</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>20690.02</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>20689.64</td>\n",
       "      <td>0.006213</td>\n",
       "      <td>20691.38</td>\n",
       "      <td>0.981250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>20695.72</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>20685.57</td>\n",
       "      <td>2.043200</td>\n",
       "      <td>20696.33</td>\n",
       "      <td>0.358238</td>\n",
       "      <td>20685.32</td>\n",
       "      <td>0.018317</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20689.88</td>\n",
       "      <td>0.017278</td>\n",
       "      <td>20689.83</td>\n",
       "      <td>0.011799</td>\n",
       "      <td>20689.89</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>20686.69</td>\n",
       "      <td>0.003841</td>\n",
       "      <td>20689.90</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024724</td>\n",
       "      <td>20692.00</td>\n",
       "      <td>0.231488</td>\n",
       "      <td>20682.78</td>\n",
       "      <td>0.025542</td>\n",
       "      <td>20693.47</td>\n",
       "      <td>0.016210</td>\n",
       "      <td>20682.10</td>\n",
       "      <td>0.241633</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>21076.00</td>\n",
       "      <td>0.020025</td>\n",
       "      <td>21072.72</td>\n",
       "      <td>0.002722</td>\n",
       "      <td>21076.08</td>\n",
       "      <td>0.025441</td>\n",
       "      <td>21072.71</td>\n",
       "      <td>0.091130</td>\n",
       "      <td>21076.14</td>\n",
       "      <td>0.020467</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018979</td>\n",
       "      <td>21078.66</td>\n",
       "      <td>0.008612</td>\n",
       "      <td>21071.74</td>\n",
       "      <td>0.227235</td>\n",
       "      <td>21078.72</td>\n",
       "      <td>0.227227</td>\n",
       "      <td>21071.32</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>21071.31</td>\n",
       "      <td>0.005324</td>\n",
       "      <td>21068.78</td>\n",
       "      <td>0.094586</td>\n",
       "      <td>21071.35</td>\n",
       "      <td>0.013313</td>\n",
       "      <td>21068.65</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>21071.41</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.237270</td>\n",
       "      <td>21073.46</td>\n",
       "      <td>0.013086</td>\n",
       "      <td>21066.24</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>21073.48</td>\n",
       "      <td>0.021072</td>\n",
       "      <td>21065.65</td>\n",
       "      <td>0.355873</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>21071.31</td>\n",
       "      <td>0.005324</td>\n",
       "      <td>21068.78</td>\n",
       "      <td>0.094586</td>\n",
       "      <td>21071.35</td>\n",
       "      <td>0.013313</td>\n",
       "      <td>21068.65</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>21071.41</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.237270</td>\n",
       "      <td>21073.46</td>\n",
       "      <td>0.013086</td>\n",
       "      <td>21066.24</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>21073.48</td>\n",
       "      <td>0.021072</td>\n",
       "      <td>21065.65</td>\n",
       "      <td>0.355873</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>21064.51</td>\n",
       "      <td>0.111293</td>\n",
       "      <td>21064.50</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>21064.60</td>\n",
       "      <td>0.041741</td>\n",
       "      <td>21061.10</td>\n",
       "      <td>0.744450</td>\n",
       "      <td>21065.25</td>\n",
       "      <td>0.008726</td>\n",
       "      <td>...</td>\n",
       "      <td>0.355963</td>\n",
       "      <td>21066.80</td>\n",
       "      <td>0.009655</td>\n",
       "      <td>21058.89</td>\n",
       "      <td>0.004937</td>\n",
       "      <td>21067.00</td>\n",
       "      <td>0.227349</td>\n",
       "      <td>21057.91</td>\n",
       "      <td>0.098579</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>21064.19</td>\n",
       "      <td>0.094998</td>\n",
       "      <td>21061.10</td>\n",
       "      <td>0.744450</td>\n",
       "      <td>21064.25</td>\n",
       "      <td>0.012357</td>\n",
       "      <td>21059.36</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>21064.31</td>\n",
       "      <td>0.227376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195993</td>\n",
       "      <td>21065.25</td>\n",
       "      <td>0.009204</td>\n",
       "      <td>21056.68</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>21065.26</td>\n",
       "      <td>0.686197</td>\n",
       "      <td>21056.15</td>\n",
       "      <td>0.058778</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100001 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        PRICE_ASK_0  VOLUME_ASK_0  PRICE_BID_0  VOLUME_BID_0  PRICE_ASK_1  \\\n",
       "0          20690.00      0.084309     20689.99      0.011289     20690.02   \n",
       "1          20690.00      0.084309     20689.99      0.011289     20690.02   \n",
       "2          20690.00      0.084309     20689.99      0.011289     20690.02   \n",
       "3          20690.00      0.084309     20689.65      0.005000     20690.02   \n",
       "4          20689.88      0.017278     20689.83      0.011799     20689.89   \n",
       "...             ...           ...          ...           ...          ...   \n",
       "99996      21076.00      0.020025     21072.72      0.002722     21076.08   \n",
       "99997      21071.31      0.005324     21068.78      0.094586     21071.35   \n",
       "99998      21071.31      0.005324     21068.78      0.094586     21071.35   \n",
       "99999      21064.51      0.111293     21064.50      0.005000     21064.60   \n",
       "100000     21064.19      0.094998     21061.10      0.744450     21064.25   \n",
       "\n",
       "        VOLUME_ASK_1  PRICE_BID_1  VOLUME_BID_1  PRICE_ASK_2  VOLUME_ASK_2  \\\n",
       "0           0.000143     20689.98      0.000348     20691.38      0.981250   \n",
       "1           0.000143     20689.98      0.000348     20691.38      0.981250   \n",
       "2           0.000143     20689.98      0.000348     20691.38      0.981250   \n",
       "3           0.000143     20689.64      0.006213     20691.38      0.981250   \n",
       "4           0.000100     20686.69      0.003841     20689.90      0.000100   \n",
       "...              ...          ...           ...          ...           ...   \n",
       "99996       0.025441     21072.71      0.091130     21076.14      0.020467   \n",
       "99997       0.013313     21068.65      0.080000     21071.41      0.005000   \n",
       "99998       0.013313     21068.65      0.080000     21071.41      0.005000   \n",
       "99999       0.041741     21061.10      0.744450     21065.25      0.008726   \n",
       "100000      0.012357     21059.36      0.002087     21064.31      0.227376   \n",
       "\n",
       "        ...  VOLUME_BID_7  PRICE_ASK_8  VOLUME_ASK_8  PRICE_BID_8  \\\n",
       "0       ...      0.000300     20695.72      0.100000     20685.97   \n",
       "1       ...      0.000300     20695.72      0.100000     20685.97   \n",
       "2       ...      0.000300     20695.72      0.100000     20685.97   \n",
       "3       ...      0.150000     20695.72      0.100000     20685.57   \n",
       "4       ...      0.024724     20692.00      0.231488     20682.78   \n",
       "...     ...           ...          ...           ...          ...   \n",
       "99996   ...      0.018979     21078.66      0.008612     21071.74   \n",
       "99997   ...      0.237270     21073.46      0.013086     21066.24   \n",
       "99998   ...      0.237270     21073.46      0.013086     21066.24   \n",
       "99999   ...      0.355963     21066.80      0.009655     21058.89   \n",
       "100000  ...      0.195993     21065.25      0.009204     21056.68   \n",
       "\n",
       "        VOLUME_BID_8  PRICE_ASK_9  VOLUME_ASK_9  PRICE_BID_9  VOLUME_BID_9  \\\n",
       "0           0.003401     20696.33      0.358238     20685.65      0.002158   \n",
       "1           0.003401     20696.33      0.358238     20685.65      0.002158   \n",
       "2           0.003401     20696.33      0.358238     20685.65      0.002158   \n",
       "3           2.043200     20696.33      0.358238     20685.32      0.018317   \n",
       "4           0.025542     20693.47      0.016210     20682.10      0.241633   \n",
       "...              ...          ...           ...          ...           ...   \n",
       "99996       0.227235     21078.72      0.227227     21071.32      0.100000   \n",
       "99997       0.120000     21073.48      0.021072     21065.65      0.355873   \n",
       "99998       0.120000     21073.48      0.021072     21065.65      0.355873   \n",
       "99999       0.004937     21067.00      0.227349     21057.91      0.098579   \n",
       "100000      0.000969     21065.26      0.686197     21056.15      0.058778   \n",
       "\n",
       "        LABEL_1TICK  \n",
       "0               0.0  \n",
       "1               0.0  \n",
       "2               0.0  \n",
       "3               0.0  \n",
       "4               0.0  \n",
       "...             ...  \n",
       "99996          -1.0  \n",
       "99997           0.0  \n",
       "99998          -1.0  \n",
       "99999          -1.0  \n",
       "100000          0.0  \n",
       "\n",
       "[100001 rows x 41 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe.to_csv('BTC-USD Data {}'.format(strftime('%d %b %Y', gmtime())), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('BTC-USD Data 24 Jun 2022')\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our cryptocurrency, we let t ∈ {1, . . . , T } denote an enumeration of all order book updates. Given two consecutive order book states for the cryptocurrency at t − 1 and t, we define the bid order flows and ask order flows at time t as vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "of_data = pd.DataFrame()\n",
    "for i in range(10):\n",
    "    of_data['bOF_{}'.format(i)] = np.empty(len(dataframe))\n",
    "    of_data['aOF_{}'.format(i)] = np.empty(len(dataframe))\n",
    "    of_data['bOF_{}'.format(i)][0] = None\n",
    "    of_data['aOF_{}'.format(i)][0] = None\n",
    "    for j in range(1,len(dataframe)):\n",
    "            \n",
    "        # Bid Order Flow\n",
    "        if dataframe.loc[j,'PRICE_BID_{}'.format(i)] > dataframe.loc[j-1,'PRICE_BID_{}'.format(i)]:\n",
    "            of_data['bOF_{}'.format(i)][j] = dataframe.loc[j,'VOLUME_BID_{}'.format(i)]\n",
    "        elif dataframe.loc[j,'PRICE_BID_{}'.format(i)] < dataframe.loc[j-1,'PRICE_BID_{}'.format(i)]:\n",
    "            of_data['bOF_{}'.format(i)][j] = -1*dataframe.loc[j,'VOLUME_BID_{}'.format(i)]\n",
    "        else:\n",
    "            of_data['bOF_{}'.format(i)][j] = dataframe.loc[j,'VOLUME_BID_{}'.format(i)] - dataframe.loc[j-1,'VOLUME_BID_{}'.format(i)]\n",
    "            \n",
    "        # Ask Order Flow\n",
    "        if dataframe.loc[j,'PRICE_ASK_{}'.format(i)] > dataframe.loc[j-1,'PRICE_ASK_{}'.format(i)]:\n",
    "            of_data['aOF_{}'.format(i)][j] = -1*dataframe.loc[j,'VOLUME_ASK_{}'.format(i)]\n",
    "        elif dataframe.loc[j,'PRICE_ASK_{}'.format(i)] < dataframe.loc[j-1,'PRICE_ASK_{}'.format(i)]:\n",
    "            of_data['aOF_{}'.format(i)][j] = dataframe.loc[j,'VOLUME_ASK_{}'.format(i)]\n",
    "        else:\n",
    "            of_data['aOF_{}'.format(i)][j] = dataframe.loc[j,'VOLUME_ASK_{}'.format(i)] - dataframe.loc[j-1,'VOLUME_ASK_{}'.format(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add output columns to of_data\n",
    "of_data = pd.concat([of_data,dataframe.iloc[:,-1:]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop first row\n",
    "of_data = of_data.iloc[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bOF_0</th>\n",
       "      <th>aOF_0</th>\n",
       "      <th>bOF_1</th>\n",
       "      <th>aOF_1</th>\n",
       "      <th>bOF_2</th>\n",
       "      <th>aOF_2</th>\n",
       "      <th>bOF_3</th>\n",
       "      <th>aOF_3</th>\n",
       "      <th>bOF_4</th>\n",
       "      <th>aOF_4</th>\n",
       "      <th>...</th>\n",
       "      <th>aOF_5</th>\n",
       "      <th>bOF_6</th>\n",
       "      <th>aOF_6</th>\n",
       "      <th>bOF_7</th>\n",
       "      <th>aOF_7</th>\n",
       "      <th>bOF_8</th>\n",
       "      <th>aOF_8</th>\n",
       "      <th>bOF_9</th>\n",
       "      <th>aOF_9</th>\n",
       "      <th>LABEL_1TICK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.005000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.006213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.003067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.009612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.002158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.150000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.043200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.018317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.011799</td>\n",
       "      <td>0.017278</td>\n",
       "      <td>-0.003841</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-1.513803</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-0.018317</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.006624</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057827</td>\n",
       "      <td>-0.016070</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>-0.024724</td>\n",
       "      <td>0.981250</td>\n",
       "      <td>-0.025542</td>\n",
       "      <td>0.231488</td>\n",
       "      <td>-0.241633</td>\n",
       "      <td>0.016210</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>-0.094586</td>\n",
       "      <td>0.005324</td>\n",
       "      <td>-0.080000</td>\n",
       "      <td>0.013313</td>\n",
       "      <td>-0.227265</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>-0.339581</td>\n",
       "      <td>0.006926</td>\n",
       "      <td>-0.000495</td>\n",
       "      <td>0.016311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016860</td>\n",
       "      <td>-0.010967</td>\n",
       "      <td>0.238264</td>\n",
       "      <td>-0.237270</td>\n",
       "      <td>0.061199</td>\n",
       "      <td>-0.120000</td>\n",
       "      <td>0.013086</td>\n",
       "      <td>-0.355873</td>\n",
       "      <td>0.021072</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>-0.005000</td>\n",
       "      <td>0.111293</td>\n",
       "      <td>-0.744450</td>\n",
       "      <td>0.041741</td>\n",
       "      <td>-0.007445</td>\n",
       "      <td>0.008726</td>\n",
       "      <td>-1.085000</td>\n",
       "      <td>0.044998</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.227351</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052319</td>\n",
       "      <td>-0.006386</td>\n",
       "      <td>0.007704</td>\n",
       "      <td>-0.355963</td>\n",
       "      <td>0.637379</td>\n",
       "      <td>-0.004937</td>\n",
       "      <td>0.009655</td>\n",
       "      <td>-0.098579</td>\n",
       "      <td>0.227349</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>-0.744450</td>\n",
       "      <td>0.094998</td>\n",
       "      <td>-0.002087</td>\n",
       "      <td>0.012357</td>\n",
       "      <td>-0.227364</td>\n",
       "      <td>0.227376</td>\n",
       "      <td>-0.237347</td>\n",
       "      <td>0.016477</td>\n",
       "      <td>-0.000697</td>\n",
       "      <td>0.034887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018410</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.009204</td>\n",
       "      <td>-0.195993</td>\n",
       "      <td>0.009204</td>\n",
       "      <td>-0.000969</td>\n",
       "      <td>0.009204</td>\n",
       "      <td>-0.058778</td>\n",
       "      <td>0.686197</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           bOF_0     aOF_0     bOF_1     aOF_1     bOF_2     aOF_2     bOF_3  \\\n",
       "1       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3      -0.005000  0.000000 -0.006213  0.000000 -0.003067  0.000000 -0.009612   \n",
       "4       0.011799  0.017278 -0.003841  0.000100 -1.513803  0.000100 -0.018317   \n",
       "5       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "99996   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "99997  -0.094586  0.005324 -0.080000  0.013313 -0.227265  0.005000 -0.339581   \n",
       "99998   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "99999  -0.005000  0.111293 -0.744450  0.041741 -0.007445  0.008726 -1.085000   \n",
       "100000 -0.744450  0.094998 -0.002087  0.012357 -0.227364  0.227376 -0.237347   \n",
       "\n",
       "           aOF_3     bOF_4     aOF_4  ...     aOF_5     bOF_6     aOF_6  \\\n",
       "1       0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "2       0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "3       0.000000 -0.000300  0.000000  ...  0.000000 -0.002158  0.000000   \n",
       "4       0.000100 -0.100000  0.006624  ...  0.057827 -0.016070  0.000143   \n",
       "5       0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "99996   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "99997   0.006926 -0.000495  0.016311  ...  0.016860 -0.010967  0.238264   \n",
       "99998   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "99999   0.044998 -0.050000  0.227351  ...  0.052319 -0.006386  0.007704   \n",
       "100000  0.016477 -0.000697  0.034887  ...  0.018410 -0.750000  0.009204   \n",
       "\n",
       "           bOF_7     aOF_7     bOF_8     aOF_8     bOF_9     aOF_9  \\\n",
       "1       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3      -0.150000  0.000000 -2.043200  0.000000 -0.018317  0.000000   \n",
       "4      -0.024724  0.981250 -0.025542  0.231488 -0.241633  0.016210   \n",
       "5       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "...          ...       ...       ...       ...       ...       ...   \n",
       "99996   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "99997  -0.237270  0.061199 -0.120000  0.013086 -0.355873  0.021072   \n",
       "99998   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "99999  -0.355963  0.637379 -0.004937  0.009655 -0.098579  0.227349   \n",
       "100000 -0.195993  0.009204 -0.000969  0.009204 -0.058778  0.686197   \n",
       "\n",
       "        LABEL_1TICK  \n",
       "1               0.0  \n",
       "2               0.0  \n",
       "3               0.0  \n",
       "4               0.0  \n",
       "5               0.0  \n",
       "...             ...  \n",
       "99996          -1.0  \n",
       "99997           0.0  \n",
       "99998          -1.0  \n",
       "99999          -1.0  \n",
       "100000          0.0  \n",
       "\n",
       "[100000 rows x 21 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "of_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Staionary time series is one such that its statistical parameters such as mean and variance do not change over time. If the linear combination or spread of two time series is stationary, then the two variables are said to be cointegrated. That is, for two time series, if the spread stays around the mean, then the variables are said to be cointegrated. Correlated time series, on the other hand, refer to variables whose movements are typically in the same direction, but their spread in nonstationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the importance of stationarity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We firstly determine the stationarity for our multivariate time series by performing the Johansen's cointegration test. (Johansen) We do not utilize the Augmented Dickey Fuller (ADF) test for each univariate time series in our data, since we are not interested in the stationarity of each variable in a multivariate time series. As mentioned previously, however, we understand that order flow and volume are univariately stationary (Cont et al)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null hypothesis for Johansen's cointegration test with trace is that the number of cointegrating relationships is less than the number of variables in our time series, while the alternative is that the number of cointegrating relationships equals the number of variables in our time series. By rejecting the null hypothesis at a specified significance level, we accept that the multivariate time series is stationary with that significance. We choose to perform the test at the 99% significance level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there is a key caveat in our specific usage of Johansen's cointegration test that cannot be overlooked, which is the fact that computing critical values for a multivariate time series of more than 12 variables cannot be done with this test, so we cannot directly apply this test on our 20-variable time series since the result can hence not be used to deduce the cointegrating relationships in the time series. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can, however, test for stationary on our order book data's order flow imbalance, which differences the bid and ask order flows for each respective level in the order book. This differencing leaves us with a 10-variable dataset that we can deduce the stationarity of and continue with our modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "ofi_data = pd.DataFrame()\n",
    "for i in range(10):\n",
    "    ofi_data['OFI_{}'.format(i)] = np.empty(len(of_data))\n",
    "    for j in range(len(of_data)):\n",
    "        ofi_data['OFI_{}'.format(i)][j] = of_data.loc[j+1,'bOF_{}'.format(i)] - of_data.loc[j+1,'aOF_{}'.format(i)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "ofi_data = pd.concat([ofi_data,of_data.iloc[:,-1:]],axis=1).apply(lambda x: pd.Series(x.dropna().values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OFI_0</th>\n",
       "      <th>OFI_1</th>\n",
       "      <th>OFI_2</th>\n",
       "      <th>OFI_3</th>\n",
       "      <th>OFI_4</th>\n",
       "      <th>OFI_5</th>\n",
       "      <th>OFI_6</th>\n",
       "      <th>OFI_7</th>\n",
       "      <th>OFI_8</th>\n",
       "      <th>OFI_9</th>\n",
       "      <th>LABEL_1TICK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.005000</td>\n",
       "      <td>-0.006213</td>\n",
       "      <td>-0.003067</td>\n",
       "      <td>-0.009612</td>\n",
       "      <td>-0.000300</td>\n",
       "      <td>-0.003401</td>\n",
       "      <td>-0.002158</td>\n",
       "      <td>-0.150000</td>\n",
       "      <td>-2.043200</td>\n",
       "      <td>-0.018317</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.005478</td>\n",
       "      <td>-0.003941</td>\n",
       "      <td>-1.513903</td>\n",
       "      <td>-0.018417</td>\n",
       "      <td>-0.106624</td>\n",
       "      <td>-0.807827</td>\n",
       "      <td>-0.016213</td>\n",
       "      <td>-1.005974</td>\n",
       "      <td>-0.257030</td>\n",
       "      <td>-0.257842</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>-0.099911</td>\n",
       "      <td>-0.093313</td>\n",
       "      <td>-0.232265</td>\n",
       "      <td>-0.346507</td>\n",
       "      <td>-0.016806</td>\n",
       "      <td>-0.766860</td>\n",
       "      <td>-0.249231</td>\n",
       "      <td>-0.298469</td>\n",
       "      <td>-0.133086</td>\n",
       "      <td>-0.376945</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>-0.116293</td>\n",
       "      <td>-0.786191</td>\n",
       "      <td>-0.016171</td>\n",
       "      <td>-1.129998</td>\n",
       "      <td>-0.277351</td>\n",
       "      <td>-0.289645</td>\n",
       "      <td>-0.014090</td>\n",
       "      <td>-0.993342</td>\n",
       "      <td>-0.014593</td>\n",
       "      <td>-0.325928</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>-0.839448</td>\n",
       "      <td>-0.014444</td>\n",
       "      <td>-0.454740</td>\n",
       "      <td>-0.253823</td>\n",
       "      <td>-0.035584</td>\n",
       "      <td>-0.067699</td>\n",
       "      <td>-0.759204</td>\n",
       "      <td>-0.205197</td>\n",
       "      <td>-0.010173</td>\n",
       "      <td>-0.744974</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          OFI_0     OFI_1     OFI_2     OFI_3     OFI_4     OFI_5     OFI_6  \\\n",
       "0      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2     -0.005000 -0.006213 -0.003067 -0.009612 -0.000300 -0.003401 -0.002158   \n",
       "3     -0.005478 -0.003941 -1.513903 -0.018417 -0.106624 -0.807827 -0.016213   \n",
       "4      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "99995  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "99996 -0.099911 -0.093313 -0.232265 -0.346507 -0.016806 -0.766860 -0.249231   \n",
       "99997  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "99998 -0.116293 -0.786191 -0.016171 -1.129998 -0.277351 -0.289645 -0.014090   \n",
       "99999 -0.839448 -0.014444 -0.454740 -0.253823 -0.035584 -0.067699 -0.759204   \n",
       "\n",
       "          OFI_7     OFI_8     OFI_9  LABEL_1TICK  \n",
       "0      0.000000  0.000000  0.000000          0.0  \n",
       "1      0.000000  0.000000  0.000000          0.0  \n",
       "2     -0.150000 -2.043200 -0.018317          0.0  \n",
       "3     -1.005974 -0.257030 -0.257842          0.0  \n",
       "4      0.000000  0.000000  0.000000          0.0  \n",
       "...         ...       ...       ...          ...  \n",
       "99995  0.000000  0.000000  0.000000         -1.0  \n",
       "99996 -0.298469 -0.133086 -0.376945          0.0  \n",
       "99997  0.000000  0.000000  0.000000         -1.0  \n",
       "99998 -0.993342 -0.014593 -0.325928         -1.0  \n",
       "99999 -0.205197 -0.010173 -0.744974          0.0  \n",
       "\n",
       "[100000 rows x 11 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ofi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jres = coint_johansen(ofi_data.iloc[:,:-1], det_order=0, k_ar_diff=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jres.trace_stat > jres.trace_stat_crit_vals[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated trace statistics and are far greater than the critical values at the 99% significance level, so we can reject the null hypothesis at the 99% significance level. That is, we are at least 99% confident that the number of cointegrating relationships is equal to the number of variables in our multivariate time series, so we consider the multivariate time series data to be stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark on Autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classically, the order of the autoregressive process is what we use to control the lag parameter of the deep learning model. For a univariate model, this can be determined from the time series data with a partial autocorrelation test. However, for a multivariate model such as ours, the estimation process is different since we must account for the cross-correlations between the 20 variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the articles from which we adopt the deep learning model offer a lag parameter of 100 timestamps. However, the mean squared error of a 1-step ahead prediction increases with respect to the order of the autoregressive process, so we choose to estimate the order statistically and select it as our window length. We can do so on the basis that if a process has order *p*, then it has order *p+1*.\n",
    "\n",
    "The `VAR` class of the `statsmodels` package assumes that the passed time series are stationary, so we can use it for our estimation of the order of our process.\n",
    "\n",
    "Choice of lag order can be a difficult problem. Standard analysis employs likelihood test or information criteria-based order selection.\n",
    "\n",
    "Pros and cons of increasing/decreasing lag parameter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    }
   ],
   "source": [
    "model = VAR(ofi_data.iloc[:,:-1])\n",
    "results = model.fit(maxlags=100, ic='aic')\n",
    "lag_order = results.k_ar\n",
    "print(lag_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the training and test set by using the first 80% of the time series and the remaining 20% for the test set. Note that the test set must be in the future of the training set to avoid look-ahead bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "train_weight = 0.8\n",
    "ofi_train, ofi_test = np.array([None]), np.array([None])\n",
    "ofi_train = ofi_data.iloc[:int(len(ofi_data)*train_weight)]\n",
    "ofi_test = ofi_data.iloc[int(len(ofi_data)*train_weight):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization of the data is important to avoid potential scaling difficulties in the fitting of the model. When there is more than one feature (covariate), scaling avoids one feature dominating over another due to disparate scales.\n",
    "\n",
    "To avoid introducing a look-ahead bias into the prediction, we must re-scale the training data without knowledge of the test set. Hence, we will simply standardize the training set using the mean and standard deviation of the training set and not the whole time series. Additionally, to avoid introducing a systematic bias into test set, we use the identical normalization for the test set - the mean and standard deviation of the training set are used to normalize the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-84-7f8ae83af7c1>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ofi_train[col] = ofi_train.loc[:,col].apply(stdize_input)\n",
      "<ipython-input-84-7f8ae83af7c1>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ofi_test[col] = ofi_test.loc[:,col].apply(stdize_input)\n"
     ]
    }
   ],
   "source": [
    "# Standardize data\n",
    "# note that for a multivariate time series, you would need to scale \n",
    "# each variable by its own mean and standard deviation in the training set\n",
    "for col in ofi_train.columns[:-1]:\n",
    "    mu = np.float(ofi_train.loc[:,col].mean())\n",
    "    sigma = np.float(ofi_train.loc[:,col].std())\n",
    "    stdize_input = lambda x: (x - mu) / sigma\n",
    "    ofi_train[col] = ofi_train.loc[:,col].apply(stdize_input)\n",
    "    ofi_test[col] = ofi_test.loc[:,col].apply(stdize_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform our sequential learning task, we must tensorize our data to create a time series that will serve as the inputs to the network.\n",
    "\n",
    "Let's define the following function for reshaping the data into a return prediction format for the 5 time horizons of the original set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ready data for training:\n",
    "# 1. sample_size=lag_order: the most recent lag_order updates, number of lags, i.e. sequence length\n",
    "# 2. feature_num=10: 10 features per time stamp, number of variables in the series\n",
    "# 3. target_num=1: relative change for the next event, number of forecasting horizons\n",
    "def get_lagged_features(data, sample_size=lag_order, feature_num=10, target_num=1):\n",
    "    data = data.values\n",
    "    shape = data.shape\n",
    "    X = np.zeros((shape[0]-sample_size, sample_size, feature_num))\n",
    "    Y = np.zeros(shape=(shape[0]-sample_size, target_num))\n",
    "    for i in range(shape[0]-sample_size):\n",
    "        X[i] = data[i:i+sample_size,:feature_num] # take the first 20 columns as features\n",
    "        Y[i] = data[i+sample_size-1,-target_num:] # take the last column as labels\n",
    "    X = X.reshape(X.shape[0], sample_size, feature_num, 1) # add the 4th dimension: 1 channel\n",
    "    \n",
    "    # \"Benchmark dataset for mid-price forecasting of limit order book data with machine learning\"\n",
    "    # label 1: change greater or equal to 0.002%\n",
    "    # label 0: change within -0.00199% to 0.00199%\n",
    "    # label -1: change smaller or equal to -0.002%\n",
    "    # Y+=1 relabels as 0,1,2\n",
    "    Y += 1\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y = get_lagged_features(ofi_train)\n",
    "train_Y = train_Y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79924, 76, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79924, 1)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "import keras.initializers\n",
    "from tensorflow.keras.layers import Dropout\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_LSTM(dropout = 0.2, seed = 0):\n",
    "    # the size of a single input is (lag_order,10). Note that we statistically generated a lag order of 15.\n",
    "    input_tensor = Input(shape=(lag_order,10,1))\n",
    "\n",
    "    # convolutional filter is (1,2) with stride of (1,2)\n",
    "#     layer_x = layers.Conv2D(16, (1,2), strides=(1,2), kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed))(input_tensor)\n",
    "#     layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "    layer_x = layers.Conv2D(16, (4,1), padding='same', kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed))(input_tensor)\n",
    "    layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "    layer_x = layers.Conv2D(16, (4,1), padding='same', kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed))(layer_x)\n",
    "    layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "\n",
    "    layer_x = layers.Conv2D(16, (1,10), kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed))(layer_x)\n",
    "    layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "#     layer_x = layers.Conv2D(16, (4,1), padding='same', kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed))(layer_x)\n",
    "#     layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "#     layer_x = layers.Conv2D(16, (4,1), padding='same', kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed))(layer_x)\n",
    "#     layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "\n",
    "    # Inception Module\n",
    "    tower_1 = layers.Conv2D(32, (1,1), padding='same', kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed))(layer_x)\n",
    "    tower_1 = layers.LeakyReLU(alpha=0.01)(tower_1)\n",
    "    tower_1 = layers.Conv2D(32, (3,1), padding='same', kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed))(tower_1)\n",
    "    tower_1 = layers.LeakyReLU(alpha=0.01)(tower_1)\n",
    "\n",
    "    tower_2 = layers.Conv2D(32, (1,1), padding='same', kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed))(layer_x)\n",
    "    tower_2 = layers.LeakyReLU(alpha=0.01)(tower_2)\n",
    "    tower_2 = layers.Conv2D(32, (5,1), padding='same', kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed))(tower_2)\n",
    "    tower_2 = layers.LeakyReLU(alpha=0.01)(tower_2)  \n",
    "\n",
    "    tower_3 = layers.MaxPooling2D((3,1), padding='same', strides=(1,1))(layer_x)\n",
    "    tower_3 = layers.Conv2D(32, (1,1), padding='same', kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed))(tower_3)\n",
    "    tower_3 = layers.LeakyReLU(alpha=0.01, )(tower_3)\n",
    "\n",
    "    layer_x = layers.concatenate([tower_1, tower_2, tower_3], axis=-1)\n",
    "\n",
    "    # concatenate features of tower_1, tower_2, tower_3\n",
    "    layer_x = layers.Reshape((lag_order,96))(layer_x)\n",
    "    \n",
    "    # insert dropout layer\n",
    "    layer_x = Dropout(dropout)(layer_x)\n",
    "    \n",
    "    # 64 LSTM units\n",
    "    layer_x = LSTM(64, kernel_initializer=keras.initializers.glorot_uniform(0), bias_initializer=keras.initializers.glorot_uniform(0), recurrent_initializer=keras.initializers.orthogonal(0))(layer_x)\n",
    "    # The last output layer uses a softmax activation function\n",
    "    output = layers.Dense(3, activation='softmax', kernel_initializer=keras.initializers.glorot_uniform(0), bias_initializer=keras.initializers.glorot_uniform(0))(layer_x)\n",
    "    model = Model(input_tensor, output)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.01, epsilon=1) # learning rate and epsilon are the same as paper DeepLOB\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'model': None, 'function': CNN_LSTM, 'dropout': 0.2, 'label': 'CNN_LSTM'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "any model that has been fitted under a Box–Jenkins approach needs to be assessed out-of-sample by time series cross-validation\n",
    "\n",
    "As advocated for in BDLOB, we utilize dropout variational inference as a stochastic regularizer in the deep neural network. We use grid search to detect an appropriate dropout rate. We insert this dropout layer after the Inception Module.\n",
    "\n",
    "We apply early stopping for all models to avoid overfitting, terminating training when validation loss has not decreased for five consecutive epochs.\n",
    "\n",
    "In the time series model, observations are strongly related to their recent histories. Hence, the ordering of our data matters, so we can't apply the typical K-fold cross-validation that is applied in cross-sectional models. In prediction models over time series data, no future observations can be used in the training set. Instead, a sliding window must be used to train and predict out-of-sample over multiple repetitions to allow for parameter tuning\n",
    "\n",
    "Do we fix the length of the sliding training window or do we allow it to \"telescope\" in time? There are pros and cons to choosing the latter; it includes more observations for training, but parameter confidence loses interpretability due to the loss of sample size control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=5, min_delta=3e-5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, TimeSeriesSplit, GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    \n",
    "# # A dictionary containing a list of values to be iterated through\n",
    "# # for each parameter of the model included in the search\n",
    "# param_grid = {'dropout': dropout}\n",
    "    \n",
    "# # In the kth split, TimeSeriesSplit returns first k folds\n",
    "# # as training set and the (k+1)th fold as test set.\n",
    "# tscv = TimeSeriesSplit(n_splits = 5)\n",
    "    \n",
    "# # A grid search is performed for each of the models, and the parameter set which\n",
    "# # performs best over all the cross-validation splits is saved in the `params` dictionary\n",
    "\n",
    "# model = KerasRegressor(build_fn=model_params['function'], epochs=max_epochs, \n",
    "#                         batch_size=batch_size, verbose=2)\n",
    "# grid = GridSearchCV(estimator=model, param_grid=param_grid, \n",
    "#                     cv=tscv, n_jobs=1, verbose=1)\n",
    "# train_Y = to_categorical(train_Y) # y is the next event's mid price (k=1)\n",
    "# grid_result = grid.fit(train_X, train_Y, callbacks=[es])\n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "# means = grid_result.cv_results_['mean_test_score']\n",
    "# stds = grid_result.cv_results_['std_test_score']\n",
    "# params_ = grid_result.cv_results_['params']\n",
    "# for mean, stdev, param_ in zip(means, stds, params_):\n",
    "#     print(\"%f (%f) with %r\" % (mean, stdev, param_))\n",
    "\n",
    "#     model_params['dropout'] = grid_result.best_params_['dropout']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-step Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 76, 10, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 76, 10, 16)   80          ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " leaky_re_lu_14 (LeakyReLU)     (None, 76, 10, 16)   0           ['conv2d_15[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 76, 10, 16)   1040        ['leaky_re_lu_14[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_15 (LeakyReLU)     (None, 76, 10, 16)   0           ['conv2d_16[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 76, 1, 16)    2576        ['leaky_re_lu_15[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_16 (LeakyReLU)     (None, 76, 1, 16)    0           ['conv2d_17[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 76, 1, 32)    544         ['leaky_re_lu_16[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 76, 1, 32)    544         ['leaky_re_lu_16[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_17 (LeakyReLU)     (None, 76, 1, 32)    0           ['conv2d_18[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_re_lu_19 (LeakyReLU)     (None, 76, 1, 32)    0           ['conv2d_20[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 76, 1, 16)   0           ['leaky_re_lu_16[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 76, 1, 32)    3104        ['leaky_re_lu_17[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 76, 1, 32)    5152        ['leaky_re_lu_19[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 76, 1, 32)    544         ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " leaky_re_lu_18 (LeakyReLU)     (None, 76, 1, 32)    0           ['conv2d_19[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_re_lu_20 (LeakyReLU)     (None, 76, 1, 32)    0           ['conv2d_21[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_re_lu_21 (LeakyReLU)     (None, 76, 1, 32)    0           ['conv2d_22[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 76, 1, 96)    0           ['leaky_re_lu_18[0][0]',         \n",
      "                                                                  'leaky_re_lu_20[0][0]',         \n",
      "                                                                  'leaky_re_lu_21[0][0]']         \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 76, 96)       0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 76, 96)       0           ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 64)           41216       ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 3)            195         ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 54,995\n",
      "Trainable params: 54,995\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_params['function'](dropout = model_params['dropout'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2498/2498 [==============================] - 199s 78ms/step - loss: 0.7692 - accuracy: 0.7287\n",
      "Epoch 2/100\n",
      "2498/2498 [==============================] - 189s 76ms/step - loss: 0.7617 - accuracy: 0.7344\n",
      "Epoch 3/100\n",
      "2498/2498 [==============================] - 189s 76ms/step - loss: 0.7560 - accuracy: 0.7344\n",
      "Epoch 4/100\n",
      "2498/2498 [==============================] - 189s 76ms/step - loss: 0.7242 - accuracy: 0.7344\n",
      "Epoch 5/100\n",
      "2498/2498 [==============================] - 190s 76ms/step - loss: 0.6617 - accuracy: 0.7333\n",
      "Epoch 6/100\n",
      "2498/2498 [==============================] - 190s 76ms/step - loss: 0.6451 - accuracy: 0.7332\n",
      "Epoch 7/100\n",
      "2498/2498 [==============================] - 186s 75ms/step - loss: 0.6368 - accuracy: 0.7339\n",
      "Epoch 8/100\n",
      "2498/2498 [==============================] - 176s 70ms/step - loss: 0.6293 - accuracy: 0.7348\n",
      "Epoch 9/100\n",
      "2498/2498 [==============================] - 176s 70ms/step - loss: 0.6230 - accuracy: 0.7350\n",
      "Epoch 10/100\n",
      "2498/2498 [==============================] - 190s 76ms/step - loss: 0.6168 - accuracy: 0.7367\n",
      "Epoch 11/100\n",
      "2498/2498 [==============================] - 210s 84ms/step - loss: 0.6104 - accuracy: 0.7364\n",
      "Epoch 12/100\n",
      "2498/2498 [==============================] - 191s 76ms/step - loss: 0.6041 - accuracy: 0.7364\n",
      "Epoch 13/100\n",
      "2498/2498 [==============================] - 177s 71ms/step - loss: 0.5985 - accuracy: 0.7385\n",
      "Epoch 14/100\n",
      "2498/2498 [==============================] - 176s 71ms/step - loss: 0.5935 - accuracy: 0.7394\n",
      "Epoch 15/100\n",
      "2498/2498 [==============================] - 181s 72ms/step - loss: 0.5875 - accuracy: 0.7411\n",
      "Epoch 16/100\n",
      "2498/2498 [==============================] - 177s 71ms/step - loss: 0.5831 - accuracy: 0.7422\n",
      "Epoch 17/100\n",
      "2498/2498 [==============================] - 179s 72ms/step - loss: 0.5778 - accuracy: 0.7446\n",
      "Epoch 18/100\n",
      "2498/2498 [==============================] - 179s 71ms/step - loss: 0.5731 - accuracy: 0.7449\n",
      "Epoch 19/100\n",
      "2498/2498 [==============================] - 175s 70ms/step - loss: 0.5683 - accuracy: 0.7471\n",
      "Epoch 20/100\n",
      "2498/2498 [==============================] - 180s 72ms/step - loss: 0.5637 - accuracy: 0.7479\n",
      "Epoch 21/100\n",
      "2498/2498 [==============================] - 177s 71ms/step - loss: 0.5590 - accuracy: 0.7490\n",
      "Epoch 22/100\n",
      "2498/2498 [==============================] - 178s 71ms/step - loss: 0.5562 - accuracy: 0.7504\n",
      "Epoch 23/100\n",
      "2498/2498 [==============================] - 178s 71ms/step - loss: 0.5544 - accuracy: 0.7501\n",
      "Epoch 24/100\n",
      "2498/2498 [==============================] - 177s 71ms/step - loss: 0.5520 - accuracy: 0.7509\n",
      "Epoch 25/100\n",
      "2498/2498 [==============================] - 179s 71ms/step - loss: 0.5505 - accuracy: 0.7510\n",
      "Epoch 26/100\n",
      "2498/2498 [==============================] - 177s 71ms/step - loss: 0.5494 - accuracy: 0.7505\n",
      "Epoch 27/100\n",
      "2498/2498 [==============================] - 179s 72ms/step - loss: 0.5474 - accuracy: 0.7524\n",
      "Epoch 28/100\n",
      "2498/2498 [==============================] - 179s 72ms/step - loss: 0.5457 - accuracy: 0.7525\n",
      "Epoch 29/100\n",
      "2498/2498 [==============================] - 177s 71ms/step - loss: 0.5444 - accuracy: 0.7554\n",
      "Epoch 30/100\n",
      "2498/2498 [==============================] - 179s 72ms/step - loss: 0.5436 - accuracy: 0.7556\n",
      "Epoch 31/100\n",
      "2498/2498 [==============================] - 175s 70ms/step - loss: 0.5420 - accuracy: 0.7556\n",
      "Epoch 32/100\n",
      "2498/2498 [==============================] - 176s 71ms/step - loss: 0.5412 - accuracy: 0.7548\n",
      "Epoch 33/100\n",
      "2498/2498 [==============================] - 175s 70ms/step - loss: 0.5406 - accuracy: 0.7546\n",
      "Epoch 34/100\n",
      "2498/2498 [==============================] - 177s 71ms/step - loss: 0.5394 - accuracy: 0.7563\n",
      "Epoch 35/100\n",
      "2498/2498 [==============================] - 184s 74ms/step - loss: 0.5390 - accuracy: 0.7552\n",
      "Epoch 36/100\n",
      "2498/2498 [==============================] - 184s 74ms/step - loss: 0.5379 - accuracy: 0.7557\n",
      "Epoch 37/100\n",
      "2498/2498 [==============================] - 182s 73ms/step - loss: 0.5365 - accuracy: 0.7567\n",
      "Epoch 38/100\n",
      "2498/2498 [==============================] - 173s 69ms/step - loss: 0.5350 - accuracy: 0.7581\n",
      "Epoch 39/100\n",
      "2498/2498 [==============================] - 175s 70ms/step - loss: 0.5351 - accuracy: 0.7565\n",
      "Epoch 40/100\n",
      "2498/2498 [==============================] - 178s 71ms/step - loss: 0.5343 - accuracy: 0.7570\n",
      "Epoch 41/100\n",
      "2498/2498 [==============================] - 177s 71ms/step - loss: 0.5336 - accuracy: 0.7563\n",
      "Epoch 42/100\n",
      "2498/2498 [==============================] - 176s 71ms/step - loss: 0.5332 - accuracy: 0.7571\n",
      "Epoch 43/100\n",
      "2498/2498 [==============================] - 175s 70ms/step - loss: 0.5321 - accuracy: 0.7585\n",
      "Epoch 44/100\n",
      "2498/2498 [==============================] - 178s 71ms/step - loss: 0.5314 - accuracy: 0.7593\n",
      "Epoch 45/100\n",
      "2498/2498 [==============================] - 176s 70ms/step - loss: 0.5306 - accuracy: 0.7584\n",
      "Epoch 46/100\n",
      "2498/2498 [==============================] - 175s 70ms/step - loss: 0.5300 - accuracy: 0.7592\n",
      "Epoch 47/100\n",
      "2498/2498 [==============================] - 177s 71ms/step - loss: 0.5302 - accuracy: 0.7585\n",
      "Epoch 48/100\n",
      "2498/2498 [==============================] - 176s 71ms/step - loss: 0.5294 - accuracy: 0.7596\n",
      "Epoch 49/100\n",
      "2498/2498 [==============================] - 178s 71ms/step - loss: 0.5280 - accuracy: 0.7595\n",
      "Epoch 50/100\n",
      "2498/2498 [==============================] - 179s 72ms/step - loss: 0.5280 - accuracy: 0.7605\n",
      "Epoch 51/100\n",
      "2498/2498 [==============================] - 178s 71ms/step - loss: 0.5277 - accuracy: 0.7593\n",
      "Epoch 52/100\n",
      "2498/2498 [==============================] - 177s 71ms/step - loss: 0.5272 - accuracy: 0.7602\n",
      "Epoch 53/100\n",
      "2498/2498 [==============================] - 172s 69ms/step - loss: 0.5274 - accuracy: 0.7595\n",
      "Epoch 54/100\n",
      "2498/2498 [==============================] - 175s 70ms/step - loss: 0.5259 - accuracy: 0.7613\n",
      "Epoch 55/100\n",
      "2498/2498 [==============================] - 173s 69ms/step - loss: 0.5258 - accuracy: 0.7600\n",
      "Epoch 56/100\n",
      "2498/2498 [==============================] - 172s 69ms/step - loss: 0.5250 - accuracy: 0.7605\n",
      "Epoch 57/100\n",
      "2498/2498 [==============================] - 174s 70ms/step - loss: 0.5247 - accuracy: 0.7611\n",
      "Epoch 58/100\n",
      "2498/2498 [==============================] - 171s 68ms/step - loss: 0.5242 - accuracy: 0.7611\n",
      "Epoch 59/100\n",
      "2498/2498 [==============================] - 172s 69ms/step - loss: 0.5239 - accuracy: 0.7607\n",
      "Epoch 60/100\n",
      "2498/2498 [==============================] - 170s 68ms/step - loss: 0.5234 - accuracy: 0.7607\n",
      "Epoch 61/100\n",
      "2498/2498 [==============================] - 172s 69ms/step - loss: 0.5223 - accuracy: 0.7619\n",
      "Epoch 62/100\n",
      "2498/2498 [==============================] - 173s 69ms/step - loss: 0.5225 - accuracy: 0.7625\n",
      "Epoch 63/100\n",
      "2498/2498 [==============================] - 173s 69ms/step - loss: 0.5219 - accuracy: 0.7621\n",
      "Epoch 64/100\n",
      "2498/2498 [==============================] - 172s 69ms/step - loss: 0.5216 - accuracy: 0.7619\n",
      "Epoch 65/100\n",
      "2498/2498 [==============================] - 174s 70ms/step - loss: 0.5209 - accuracy: 0.7619\n",
      "Epoch 66/100\n",
      "2498/2498 [==============================] - 173s 69ms/step - loss: 0.5208 - accuracy: 0.7634\n",
      "Epoch 67/100\n",
      "2498/2498 [==============================] - 172s 69ms/step - loss: 0.5203 - accuracy: 0.7619\n",
      "Epoch 68/100\n",
      "2498/2498 [==============================] - 175s 70ms/step - loss: 0.5198 - accuracy: 0.7625\n",
      "Epoch 69/100\n",
      "2498/2498 [==============================] - 173s 69ms/step - loss: 0.5202 - accuracy: 0.7625\n",
      "Epoch 70/100\n",
      "2498/2498 [==============================] - 172s 69ms/step - loss: 0.5189 - accuracy: 0.7641\n",
      "Epoch 71/100\n",
      "2498/2498 [==============================] - 172s 69ms/step - loss: 0.5189 - accuracy: 0.7631\n",
      "Epoch 72/100\n",
      "2498/2498 [==============================] - 174s 70ms/step - loss: 0.5180 - accuracy: 0.7642\n",
      "Epoch 73/100\n",
      "2498/2498 [==============================] - 175s 70ms/step - loss: 0.5180 - accuracy: 0.7635\n",
      "Epoch 74/100\n",
      "2498/2498 [==============================] - 174s 69ms/step - loss: 0.5176 - accuracy: 0.7644\n",
      "Epoch 75/100\n",
      "2498/2498 [==============================] - 177s 71ms/step - loss: 0.5170 - accuracy: 0.7647\n",
      "Epoch 76/100\n",
      "2498/2498 [==============================] - 175s 70ms/step - loss: 0.5165 - accuracy: 0.7648\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2498/2498 [==============================] - 171s 68ms/step - loss: 0.5170 - accuracy: 0.7642\n",
      "Epoch 78/100\n",
      "2498/2498 [==============================] - 173s 69ms/step - loss: 0.5163 - accuracy: 0.7644\n",
      "Epoch 79/100\n",
      "2498/2498 [==============================] - 173s 69ms/step - loss: 0.5159 - accuracy: 0.7652\n",
      "Epoch 80/100\n",
      "2498/2498 [==============================] - 173s 69ms/step - loss: 0.5150 - accuracy: 0.7663\n",
      "Epoch 81/100\n",
      "2498/2498 [==============================] - 172s 69ms/step - loss: 0.5153 - accuracy: 0.7657\n",
      "Epoch 82/100\n",
      "2498/2498 [==============================] - 173s 69ms/step - loss: 0.5146 - accuracy: 0.7648\n",
      "Epoch 83/100\n",
      "2498/2498 [==============================] - 175s 70ms/step - loss: 0.5143 - accuracy: 0.7654\n",
      "Epoch 84/100\n",
      "2498/2498 [==============================] - 172s 69ms/step - loss: 0.5143 - accuracy: 0.7652\n",
      "Epoch 85/100\n",
      "2498/2498 [==============================] - 173s 69ms/step - loss: 0.5143 - accuracy: 0.7657\n",
      "Epoch 86/100\n",
      "2498/2498 [==============================] - 172s 69ms/step - loss: 0.5133 - accuracy: 0.7674\n",
      "Epoch 87/100\n",
      "2498/2498 [==============================] - 173s 69ms/step - loss: 0.5131 - accuracy: 0.7658\n",
      "Epoch 88/100\n",
      "2498/2498 [==============================] - 172s 69ms/step - loss: 0.5129 - accuracy: 0.7668\n",
      "Epoch 89/100\n",
      "2498/2498 [==============================] - 173s 69ms/step - loss: 0.5127 - accuracy: 0.7661\n",
      "Epoch 90/100\n",
      "2498/2498 [==============================] - 174s 70ms/step - loss: 0.5122 - accuracy: 0.7669\n",
      "Epoch 91/100\n",
      "2498/2498 [==============================] - 174s 70ms/step - loss: 0.5118 - accuracy: 0.7668\n",
      "Epoch 92/100\n",
      "2498/2498 [==============================] - 175s 70ms/step - loss: 0.5115 - accuracy: 0.7673\n",
      "Epoch 93/100\n",
      "2498/2498 [==============================] - 174s 70ms/step - loss: 0.5110 - accuracy: 0.7675\n",
      "Epoch 94/100\n",
      "2498/2498 [==============================] - 175s 70ms/step - loss: 0.5107 - accuracy: 0.7672\n",
      "Epoch 95/100\n",
      "2498/2498 [==============================] - 176s 71ms/step - loss: 0.5108 - accuracy: 0.7680\n",
      "Epoch 96/100\n",
      "2498/2498 [==============================] - 175s 70ms/step - loss: 0.5102 - accuracy: 0.7684\n",
      "Epoch 97/100\n",
      "2498/2498 [==============================] - 177s 71ms/step - loss: 0.5099 - accuracy: 0.7671\n",
      "Epoch 98/100\n",
      "2498/2498 [==============================] - 175s 70ms/step - loss: 0.5096 - accuracy: 0.7682\n",
      "Epoch 99/100\n",
      "2498/2498 [==============================] - 175s 70ms/step - loss: 0.5092 - accuracy: 0.7683\n",
      "Epoch 100/100\n",
      "2498/2498 [==============================] - 174s 70ms/step - loss: 0.5093 - accuracy: 0.7684\n"
     ]
    }
   ],
   "source": [
    "train_Y = to_categorical(train_Y) # y is the next event's mid price (k=1)\n",
    "model.fit(train_X, train_Y, epochs=max_epochs, batch_size=batch_size, callbacks=[es])\n",
    "model_params['model'] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "623/623 [==============================] - 10s 16ms/step - loss: 0.7190 - accuracy: 0.6878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7190269827842712, 0.6878136992454529]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X, test_Y = get_lagged_features(ofi_test)\n",
    "test_Y = test_Y.astype(int)\n",
    "\n",
    "test_Y = to_categorical(test_Y)\n",
    "model.evaluate(test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model10.classification_report(true_y10, pred_y10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params['model'].save('CNN-LSTM-SAVED.hdf5', overwrite=True)  # creates a HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model_params['model'] = load_model('CNN-LSTM-SAVED.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want confusion matrix of **precision**, **recall**, **F1-Score**, and **AUC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_params['model']\n",
    "    \n",
    "model_params['pred_train'] = model.predict(train_X, verbose=1)\n",
    "model_params['CE_train'] = log_loss(train_y5, model_params['pred_train'])\n",
    "    \n",
    "model_params['pred_test'] = model.predict(test_X, verbose=1) \n",
    "model_params['CE_test'] = log_loss(test_y5, model_params['pred_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_y5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params['pred_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params['pred_test'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_y5, model_params['pred_test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pts = 10**4 # maximum number of  points to plot per series\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "x_vals = np.arange(len(train_y5))\n",
    "\n",
    "y_vals = model_params['pred_train']\n",
    "label = model_params['label'] + ' (train CE: %.2e)' % model_params['CE_train']\n",
    "plt.plot(x_vals, y_vals, c='blue', label=label, lw=1)\n",
    "\n",
    "plt.plot(x_vals, train_y5, c=\"black\", label=\"Observed\", lw=1)\n",
    "plt.xlim(x_vals.min(), x_vals.max())\n",
    "plt.xlabel('Time (ticks)', fontsize=14)\n",
    "plt.ylabel('$\\hat{Y}$', rotation=0, fontsize=14)\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "plt.title('Observed vs Model Outputs (Training)', fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "x_vals = np.arange(len(train_y5))\n",
    "\n",
    "y_vals = model_params['pred_train'] - train_y5\n",
    "label = model_params['label'] + ' (train CE: %.2e)' % model_params['CE_train']\n",
    "plt.plot(x_vals, y_vals, c='blue', label=label, lw=1)\n",
    "\n",
    "plt.axhline(0, linewidth=0.8)\n",
    "plt.xlim(x_vals.min(), x_vals.max())\n",
    "plt.xlabel('Time (ticks)', fontsize=14)\n",
    "plt.ylabel('$\\hat{Y}-Y$', fontsize=14)\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "plt.title('Observed vs Model Error (Training)', fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "x_vals = len(train_y5) + np.arange(len(test_y5))\n",
    "\n",
    "y_vals = model_params['pred_test']\n",
    "label = model_params['label'] + ' (test CE: %.2e)' % model_params['CE_test']\n",
    "plt.plot(x_vals, y_vals, c='blue', label=label, lw=1)\n",
    "\n",
    "plt.plot(x_vals, test_y5, c=\"black\", label=\"Observed\", lw=1)\n",
    "plt.xlim(x_vals.min(), x_vals.max())\n",
    "plt.xlabel('Time (ticks)', fontsize=14)\n",
    "plt.ylabel('$\\hat{Y}$', rotation=0, fontsize=14)\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "plt.title('Observed vs Model Outputs (Testing)', fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "x_vals = len(train_y5) + np.arange(len(test_y5))\n",
    "\n",
    "y_vals = model_params['pred_test'] - test_y5\n",
    "label = model_params['label'] + ' (test CE: %.2e)' % model_params['CE_test']\n",
    "plt.plot(x_vals, y_vals, c='blue', label=label, lw=1)\n",
    "\n",
    "plt.axhline(0, linewidth=0.8)\n",
    "plt.xlim(x_vals.min(), x_vals.max())\n",
    "plt.xlabel('Time (ticks)', fontsize=14)\n",
    "plt.ylabel('$\\hat{Y}-Y$', fontsize=14)\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "plt.title('Observed vs Model Error (Testing)', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Trading Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what happens if we go long every time the model predicts a positive return and likewise go short if the model predicts a negative return.\n",
    "\n",
    "if our model were to predict a positive return for the next second, we would immediately buy at the close and be filled at the closing price. We would then close out our position after the second elapsed, again, getting filled at the next close to produce a return.\n",
    "\n",
    "This is essentially the \"Normal trading strategy\" that is described in (Zhang et al)\n",
    "\n",
    "the Sharpe\n",
    "Ratio is based on variance of returns that treats both positive and negative returns to be “risky”. In\n",
    "practice, few traders would consider large positive returns as risky so we use the Downward Deviation\n",
    "Ratio (DDR) as a measure of risk\n",
    "\n",
    "The DDR penalizes negative returns and rewards large positive returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xticks = collect(minimum(testData.timestampfloor):Hour(4):maximum(testData.timestampfloor))\n",
    "# xtickslabels = Dates.format.(xticks, dateformat\"HH:MM\")\n",
    "\n",
    "# plot(testData.timestampfloor, cumsum(sign.(testData.CloseClosePred) .* testData.CloseCloseReturn), \n",
    "#     label=:none, title = \"Cummulative Return\", fmt=:png, xticks = (xticks, xtickslabels))\n",
    "pnl = 0\n",
    "while end of test_of_data not hit:\n",
    "    if model.predict(test_of_data[index]) > threshold:\n",
    "        go long\n",
    "    elif model.predict(test_of_data[index]) < -1*threshold:\n",
    "        go short\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortfalls of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep neural networks as a \"black box\"\n",
    "\n",
    "\"Offline\" version of learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Zhang, Zihao, Stefan Zohren, and Stephen Roberts (2019). “DeepLOB: Deep Convolutional Neural Networks for Limit Order Books”. In: *IEEE Transactions On Signal Processing* 67.11, pp. 3001–3012.<br>\n",
    "[2] Cont, Rama, Arseniy Kukanov, and Sasha Stoikov (2014). “The Price Impact Of\n",
    "Order Book Events”. In: *Journal Of Financial Econometrics* 12.1, pp. 47–88.<br>\n",
    "[3] Kolm, Petter, Jeremy Turiel, and Nicholas Westray (2021). \"Deep Order Flow Imbalance: Extracting Alpha At Multiple Horizons From The Limit Order Book\". In: *Available at SSRN 3900141*.<br>\n",
    "[] Ntakaris, Adamantios, Martin Magris, Juho Kanniainen, Moncef Gabbouj, and Alexandros Iosifidis (2018). “Benchmark Dataset For Mid-Price Forecasting Of Limit Order Book Data With Machine Learning Methods”. In: *Journal of Forecasting* 37.8, pp. 852–866.<br>\n",
    "[] Szegedy, Christian, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich (2015). “Going Deeper With Convolutions”. In: *Proceedings Of The IEEE Conference On Computer Vision And Pattern Recognition*, pp. 1–9.<br>\n",
    "[] Zhang, Zihao, Stefan Zohren, and Stephen Roberts (2018). “BDLOB: Bayesian Deep Convolutional Neural Networks For Limit Order Books”. In: *arXiv preprint arXiv:1811.10041*.<br>\n",
    "[] Dixon, Matthew F., Igor Halperin, and Paul Bilokon (2020). *Machine Learning in Finance: From Theory to Practice*. Springer.<br>\n",
    "[] Tsay, Ruey S. (2010). *Analysis of Financial Time Series* (3rd ed.). Wiley.<br>\n",
    "[] Lütkepohl, Helmut (2005). *New Introduction to Multiple Time Series Analysis*. Springer.<br> \n",
    "[] Yang, Kiyoung, and Cyrus Shahabi (2005). \"On the Stationarity of Multivariate Time Series for Correlation-Based Data Analysis\". In: *Fifth IEEE International Conference on Data Mining*, pp. 1-4.<br>\n",
    "[] Johansen, Søren (1995). *Likelihood-Based Inference in Cointegrated Vector Autoregressive Models*. Oxford University Press.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
